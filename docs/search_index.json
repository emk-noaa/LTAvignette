[["index.html", "vignette 1 Status", " vignette 1 Status This status report outlines the code we are developing to process WinCruz data and prepare it for density estimation analysis using distance and related packages in R. Recent changes Week of Jan. 30 Added land area removal feature to polygon processing functions (see process_polygon()). Fixed prep_data_for_ds() function so that the minority species in multi-species schools is correctly renamed as “Other” for detection function fitting (after Bradford et al. 2021; this feature can optionally be turned off in the above function, as well as in lta()). Week of Jan. 23 Began bootstrap tests of LTA code developed to date. Polished cruz_explorer() functionality with respect to truncation distance selection. Completed g(0) estimation functions (not yet added to this vignette). Week of Jan. 16 Development of density/abundance estimation routines, including bootstrapping (not yet added to this vignette). Began development of g(0) estimation functions, after Barlow et al. (2015). Week of Jan. 9 Planning for data analysis and parameter estimation stages of the project. Development of detection function fitting routines (not yet added to this vignette.) Week of Jan. 2 Refinement of cruise summarization functions (see Chapter 6). Addition of interactive map (see LTabundR::cruz_map() documentation). Addition of interactive data explorer Shiny app (see LTabundR::cruz_explorer() documentation, with examples in LTabundR-dev/test_code/CNP/data_processing.R). Week of Dec. 26 Completion of comparison and reconciliation between ABUND and LTabundR, involving various minor debugs. See details here. Completion of package and function documentation (to date). Week of Dec. 21 Refined code for subgroup-level school size estimation. This feature is now stable. Next item. Week of Dec. 13 Development of features for handling subgroup-level analysis of false killer whale sightings. (more soon). A wide variety of minor improvements as part of a major push to compare LTabundR results to those from ABUND7 and ABUND9. This included an effort to articulate the differences between LTabundR and ABUND routines (detailed in a new appendix), and to quantify the correspondence (or lack thereof) between the two programs. This involved the development of a function, abund9_compare(), which is not featured in this vignette but can be used to compare LTabundR outputs with those from ABUND9. We are also documenting comparisons on this document. Made beaufort_range as a cohort-specific setting, instead of a survey-wide setting. Prior to week of Dec. 8 Added a new survey-wide setting, out_handling, that determines how data occurring outside of geographic strata will be handled. Currently the two options are \"remove\" (the default), which removes data occurring outside of the strata (speeds things up, reduces memory needs), and \"stratum\", which assigns all exterior events to an \"out\" stratum. Modified segmentize() function so that a list of segments is not returned to the user; that feature was created enormous objects (&gt; 1 GB for the 1986 - 2020 CNP DAS data) and is not useful enough to retain as a standing slot. A list like that can be easily produced using a dplyr pipe (group_by(seg_id) %&gt;% group_split()) when the need arises (e.g., in map_effort() – this function has also been updated to accommodate this new change). Fixed minutiae regarding group size estimation: sightings with missing data are given a school size estimate of 1, as done in ABUND; geometric mean functions have been fixed. Added progress indicator to process_sightings(). New format_distance() function creates tables that match EFFORT.csv and SIGHTINGS.csv. (See here.) New process_surveys() function provides a wrapper for all data processing functions in a single command. This will be the main function that researchers use to process their data. All other functions are called within this function, and will rarely be called directly by researchers except in special analyses. This is explained at the top of the page on Data Processing. This function includes an option for saving the result locally as a RData file, which allows the user to run this function once and save the result for later sessions. New helper function, read_polygon(), reads a DAT file of polygon coordinates and converts it to a dataframe compatible with LTabundR functions. Can handle files with multiple polygons. Skeletonized new summary functions (see the Summarize page). Ship name is now added to all dataframes. New process_sightings() function, with code for calibrating group sizes according to user-provided coefficient tables (mirroring ABUND9 with slight mods, e.g., school size threshold for observers not in the Coeff.DAT file.). (Details: the function process_sightings() relies upon a utility function, group_size(), which in turn relies upon group_size_calibrate().) Added cohort-specific setting, school_size_calibrate, that allows researchers to specify whether a cohort should or should not have school sizes calibrated. Example use case: school_size_calibrate = TRUE for a cohort of dolphin species, but make it FALSE for a cohort of large baleen whales. Basic function for mapping sightings (QA/QC): map_sightings(). Added detail/polish to vignette pages on Settings, Processing Data, and the Appendix on Segmentizing. I believe these are ready for close review. Added detailed comments to code for all functions to date. "],["install.html", " 2 Install LTabundR Option 1: Install from GitHub Option 2: Install locally", " 2 Install LTabundR Option 1: Install from GitHub Since the package is currently private, this is more complicated than it will be once the package is released. The first step is creating an R environment variable containing your GitHub personal access token. Complete these instructions only once: # Open your .Renviron file library(usethis) usethis::edit_r_environ() # Add your GitHub personal access token as a variable in `.Renviron` GITHUB_TOKEN = &#39;your_token_goes_here_as_a_character_string&#39; # Save and close your .Renviron file # Reload .Renviron readRenviron(&#39;~/.Renviron&#39;) Then run this code every time you want to reinstall / update the package: library(devtools) github_token &lt;- Sys.getenv(&quot;GITHUB_TOKEN&quot;) devtools::install_github(repo = &#39;amandalbradford/LTabundR-dev&#39;, subdir=&#39;LTabundR&#39;, auth_token=github_token, force=TRUE) library(LTabundR) Option 2: Install locally For now you can also install or update the package locally, using this code: library(devtools) # Remove the package, if you have an earlier version of it on your machine if(&#39;LTabundR&#39; %in% installed.packages()){remove.packages(&#39;LTabundR&#39;)} # Specify the path to the package&#39;s project folder. path_to_package &lt;- &#39;../LTabundR-dev/LTabundR&#39; # Update function documentation document(path_to_package) # Install package and any dependencies you need install(path_to_package) library(LTabundR) "],["settings.html", " 3 Settings Survey strata Study area polygon Survey-wide settings Cohort-specific settings Example code", " 3 Settings To customize the way data are processed and included in your analysis, use the load_settings() function. This function emulates and expands upon the settings file, ABUND.INP, that was used to run ABUND7/9 in FORTRAN. This function allows you to use ‘factory defaults’ if you don’t wish to specify anything special such as strata or study area polygons: settings &lt;- load_settings() If you do not want to use all the defaults, you can give load_settings() some custom inputs. The function accepts four arguments: strata: dataframe(s) of coordinates study_area: a single dataframe of coordinates survey: settings that will apply universally to the analysis cohorts: settings that are specific to groups of species. By providing cohort-specific settings, the code for a single analysis becomes simpler and more easily reproduced, since the code only needs to be run once without modification. The output of load_settings() is a named list with a slot for each of these arguments: settings %&gt;% names [1] &quot;strata&quot; &quot;study_area&quot; &quot;survey&quot; &quot;cohorts&quot; Survey strata Stratum polygons can be provided as a named list of data.frame objects. Each data.frame must have Lat and Lon as the first two columns, providing coordinates in decimal degrees in which South and West coordinates are negative. Other columns are allowed, but the first two need to be Lon and Lat. The name of the slot holding the data.frame will be used as a reference name for the stratum. If strata is NULL, abundance will not be estimated; only density within the searched area (i.e., the total segment length x effective strip width). While users are welcome to upload polygons of their own, the package comes with “stock” polygons for strata that are commonly used in the main NMFS study regions: the Central North Pacific (CNP, including Hawaii) … data(strata_cnp) names(strata_cnp) [1] &quot;HI_EEZ&quot; &quot;WHICEAS&quot; &quot;OtherCNP&quot; …the California Current System (CCS) … data(strata_ccs) names(strata_ccs) [1] &quot;CCS&quot; &quot;Southern_CA&quot; &quot;Central_CA&quot; &quot;Nothern_CA&quot; &quot;OR_WA&quot; … and the Eastern Tropical Pacific (ETP): data(strata_etp) names(strata_etp) [1] &quot;MOPS_AreaCoreM&quot; &quot;MOPS_AreaIn&quot; &quot;MOPS_AreaIn1&quot; [4] &quot;MOPS_AreaIn2&quot; &quot;MOPS_AREAINS&quot; &quot;MOPS_AREAMID&quot; [7] &quot;MOPS_AreaMid1&quot; &quot;MOPS_AreaMid2&quot; &quot;MOPS_AreaMOPS&quot; [10] &quot;MOPS_AREANORS&quot; &quot;MOPS_AreaOuterM&quot; &quot;MOPS_AreaSou&quot; [13] &quot;MOPS_AREASOUS&quot; &quot;MOPS_AreaSpin&quot; &quot;MOPS_AreaSpinS&quot; [16] &quot;MOPS_AREAWES&quot; &quot;PODS_93STRAT1&quot; &quot;PODS_93STRAT2&quot; [19] &quot;PODS_Area92&quot; &quot;PODS_AREA92RS&quot; &quot;PODS_Area92s&quot; [22] &quot;PODS_AREA93&quot; &quot;PODS_AREA93A&quot; &quot;PODS_AREA93AR&quot; [25] &quot;PODS_AREA93AS&quot; &quot;PODS_AREA93BR&quot; &quot;PODS_AREA93M&quot; [28] &quot;PODS_AREA93MS&quot; &quot;PODS_AREA93R&quot; &quot;PODS_AREA93R1&quot; [31] &quot;PODS_AREA93R2&quot; &quot;PODS_AREA93RS&quot; &quot;PODS_AREA93S&quot; [34] &quot;PODS_AREANCOR&quot; &quot;PODS_GOCpoly&quot; &quot;Pre1986_Area79ES1&quot; [37] &quot;Pre1986_Area79ES1s&quot; &quot;Pre1986_Area79ES2&quot; &quot;Pre1986_Area79ES2s&quot; [40] &quot;Pre1986_Area79NE1&quot; &quot;Pre1986_Area79NE1s&quot; &quot;Pre1986_Area79NE2&quot; [43] &quot;Pre1986_Area79NE2s&quot; &quot;Pre1986_Area79NE3&quot; &quot;Pre1986_Area79NE3s&quot; [46] &quot;Pre1986_AreaCal&quot; &quot;Pre1986_AreaCals&quot; &quot;Pre1986_AreaMid&quot; [49] &quot;Pre1986_AreaMidS&quot; &quot;Pre1986_AreaNorth&quot; &quot;Pre1986_AreaNorthS&quot; [52] &quot;Pre1986_AreaSouth&quot; &quot;Pre1986_AreaSouthS&quot; &quot;STAR_Area98a&quot; [55] &quot;STAR_Area98b&quot; &quot;STAR_AreaCore&quot; &quot;STAR_AreaCore2&quot; [58] &quot;STAR_AreaCoreS&quot; &quot;STAR_AreaNCoast&quot; &quot;STAR_AreaNCstS&quot; [61] &quot;STAR_AreaOuter&quot; &quot;STAR_AreaOuter00&quot; &quot;STAR_AreaSCoast&quot; [64] &quot;STAR_AreaSCstS&quot; &quot;STAR_AreaSPn&quot; &quot;STAR_AreaSPs&quot; [67] &quot;STAR_AreaSTAR&quot; &quot;STAR_AreaSTAR2&quot; &quot;STAR_AreaSTARlite&quot; [70] &quot;STAR_Dcaparea&quot; The package includes functions for visualizing and selecting from these strata. See the Strata Gallery appendix. Study area polygon The study_area argument accepts a single data.frame, formatted the same as those for the strata argument, or a numeric value indicating the area of your study area in square km. If study_area is NULL, abundance will not be estimated; only density. Study area polygons can be provided in the same format as strata: a two-column csv (column names Lon and Lat with decimal-degree coordinates). A stock study area polygon is available for the CNP: data(study_cnp) study_cnp Lon Lat 1 -131 40.00 2 -126 32.05 3 -120 25.00 4 -120 -5.00 5 -185 -5.00 6 -185 40.00 7 -131 40.00 Survey-wide settings Survey-wide settings apply universally to all species in the analysis. Defaults settings$survey $out_handling [1] &quot;remove&quot; &quot;stratum&quot; $max_row_interval [1] 360 $segment_method [1] &quot;day&quot; $segment_target_km [1] 150 $segment_max_interval [1] 48 $segment_remainder_handling [1] &quot;append&quot; &quot;segment&quot; $ship_list NULL $species_codes NULL $group_size_coefficients NULL $smear_angles [1] FALSE $random_seed [1] 833171 $verbose [1] TRUE Defaults for the survey argument list are built up efficiently using the function load_survey_settings() (see example code at bottom). Details The survey_settings input accepts a list with any of the following named slots. out_handling: the first slot allows you to specify how data occurring outside of geo-strata should be handled. If this is set to \"remove\", those rows will be filtered out of the data early in the process. This reduces memory usage, speeds up processing, and gives you geographic control of how effort and sightings will be summarize. If this is set to \"stratum\", those data will be assigned to a fake geo-stratum, named \"out\". Effort in the \"out\" stratum will not be segmentized, but \"out\" sightings will be processed and retained in the final datasets. This setting might be useful if you want to use \"out\" data for survey summaries and/or detection function estimation. The default is \"remove\", since that saves the most time and memory. segment_method: This and the next few slots are devoted to controlling how effort will be “segmentized”, or chopped into discrete sections for the purposes of estimating the variabce of the abundance estimate. The two method options are \"day\" – all effort within the same Cruise-StudyArea-Stratum-Year-Effort scenario will be binned into segments by calendar date – and \"equallength\" – effort within each unique effort scenario (Cruise-StudyArea-etc.) will be divided into segments of approximately equal length. See the Appendix on segmentizing for details. segment_target_km: if segmenting by \"equallength\", this field allows you to specify what that target length is, in km. segment_max_km_gap: the segmentizing function works by calculating the distance between each row of DAS data. If that distance exceeds the length specified here (e.g., 5 km), the function will assume that there was a break in effort. segment_max_interval: if segmentizing by \"equallength\", this setting allows you to specify the time gaps in effort that are allowed to be contained within a single segment. For example, if your goal is a few large segments of equal length (e.g., 150-km segments, for bootstrap estimation of density variance), you are probably willing for discrete periods of effort to be concatenated into a single segment, even if the gaps between effort are as large as 1 or 2 days, in which case you would set segment_max_interval to 24 or 48 (hours), respectively. However, if your goal is many smaller segments (e.g., 5-km segments, for habitat modeling), you want to ensure that effort is contiguous so that segment locations can be accurately related to environmental variables, in which case you would set segment_max_interval to be very small (e.g., .2 hours, or 12 minutes). Setting this interval to a small number, such as 0.2, also allows the segmentizing function overlook momentary breaks in effort, such as when an unofficial observer logs a sighting. segment_remainder_handling: if segmentizing by \"equallength\", periods of effectively-contiguous effort (as specified by segment_max_interval) are unlikely to be perfectly divisible by your segment_target_km; there is going to be a remainder. You can handle this remainder in three ways: (1) \"disperse\" allows the function to adjust segment_target_km so that there is in fact no remainder, effectively dispersing the remainder evenly across all segments within that period of contiguous effort; (2) \"append\" asks the function to append the remainder to a randomly selected segment, such that most segments are the target length with the exception of one longer one; or (3) \"segment\" asks the function to simply place the remainder in its own segment, placed randomly within the period of contiguous effort. This setting also has a second layer of versatility, because it can accept a one- or two-element character vector. If a two-element vector is provided (e.g., c(\"append\",\"segment\")), the first element will be used in the event that the remainder is less than or equal to half your segment_target_km; if the remainder is more than half that target length, the second element will be used. This feature allows for replication of the segmentizing methods in Becker et al. (2010). The remaining slots in survey_settings pertain to various datasets and settings used in data processing: ship_list: A data.frame containing a list of ship names. If not provided the default version, which was current as of the release of ABUND9 in 2020, will be used (data(ships)). Supplied data.frames must match the column naming structure of data(ships). species_codes: A data.frame containing species codes. If not provided the default version, which was current as of the release of ABUND9 in 2020, will be used (data(species_codes)). Supplied data.frames must match the column naming structure of data(species_codes). group_size_coefficients: A data.frame of calibration factors. Find details in the subsection on processing sightings and estimating school size. smear_angles: If TRUE (the default is FALSE), bearing angles to a group of animals will be “smeared” by adding a uniformly distributed random number between -5 and +5 degrees. This has not been used in any recent analyses because observers have not been rounding angles as much as they used to. It was suggested by Buckland as a method for dealing with rounding which is especially influential when rounding to zero places many sightings at zero perpendicular distance. verbose: If TRUE (the default), status updates will be printed to the R console. Cohort-specific settings Cohort-specific settings apply only to a group of species. Since you can add as many cohorts to a settings object as you need, this allows you to stage your entire analysis and run your code once without modifying code or creating multiple versions of your code for each analysis of each cohort. Defaults The default is to use a single cohort for all species: settings$cohorts %&gt;% names [1] &quot;default&quot; Default values for the default cohort: settings$cohorts$default $id [1] &quot;default&quot; $species NULL $probable_species [1] FALSE $sighting_method [1] 0 $cue_range [1] 0 1 2 3 4 5 6 7 $school_size_range [1] 0 10000 $school_size_calibrate [1] TRUE $calibration_floor [1] 0 $use_low_if_na [1] FALSE $io_sightings [1] 0 $geometric_mean_group [1] TRUE $truncation_km [1] 5.5 $beaufort_range [1] 0 1 2 3 4 5 6 $abeam_sightings [1] FALSE $strata_overlap_handling [1] &quot;smallest&quot; &quot;largest&quot; &quot;each&quot; $distance_types [1] &quot;S&quot; &quot;F&quot; &quot;N&quot; $distance_modes [1] &quot;P&quot; &quot;C&quot; $distance_on_off [1] TRUE Defaults for the cohorts argument list is built up efficiently using the function load_cohort_settings() (see example code at bottom). Details The cohort_settings input accepts a list of any length. Each slot in that list can contain settings for a different cohort. Each cohort list can have any of the following named slots: id: An informal identifier for this cohort, to help you keep track of which cohort is which. For example, settings for a cohort of large whales species could be named \"big whales\"; settings for small delphinids and phocoenids could be named \"small_odontocetes\"; settings for beaked whales could be named \"beakers\". species: A character vector of species codes to include in this cohort. If NULL (the default), all species will be included. probable_species: If TRUE (default is FALSE), the “probable” species identifications will be used in place of the “unidentified” categories. sighting_method: A coded integer which determines which sightings will be included based on how they were first seen. Allowable codes are 0=any method, 1=with 25X only, 2=neither with 25x binoculars nor from the helicopter (i.e., naked eyes and 7x binoculars only). These codes match those used in ABUND7/9. cue_range: Numeric vector of acceptable “observation cues” for sightings used in estimates of abundance. (0=this detail is missing in the data, 1=associated birds, 2=splashes, 3=body of the marine mammal, 4=associated vessel, 5=?, 6=blow / spout, 7=associated helicopter). These codes match those used in ABUND7/9. school_size_range: Minimum and maximum group sizes to be included in estimates of abundance. This is the overall group size, not the number of the given species that are present in a group. school_size_calibrate: A logical (TRUE or FALSE) specifying whether or not to carry out school size adjustments according to the calibration table provided in survey$group_size_coefficients (if that table is provided). This setting allows you to toggle the survey-wide setting for certain cohorts. For example, perhaps you want to carry out calibration for a cohort of dolphin species, but not for a cohort of large whales whose group sizes tend to be smaller and easier to estimate accurately. calibration_floor: A numeric indicating the minimum school size estimate for which school size calibration will be attempted. This pertains only to observers who do no have an entry in the group_size_coefficients table provided in load_survey_settings() (that table has a calibration floor for each observer). The default is 0, meaning that calibration will be attempted for all school size estimates, regarding of the raw estimate. use_low_if_na: If this setting is TRUE, an observer does not make a best estimate of group size, mean group size will be calculated from “low” estimates. This will be done only if no observer has a “best” estimate. io_sightings: A coded integer which specifies how sightings by the independent observer will be handled. Allowable codes, which are inherited from those used in ABUND7/9, are \"_1\"=include independent observer sightings wih all other sightings, \"0\"=ignore sightings by independent observer, \"1\"=use only sightings made by regular observer team WHEN an independent observer was present, \"2\"=include only sightings made by the independent observer. IO sightings are typically used only for making g(0) estimates, otherwise IO sightings are usually ignored (code = \"0\"). geometric_mean_group: This logical variable specifies whether to use a weighted geometric mean when calculating mean group size. Barlow, Gerrodette, and Perryman (1998) found that this gave slightly better performance than a straight mean group size. Default is TRUE, but it will only be done if group_size_coefficients is not NULL. truncation_km: Specifies the maximum perpendicular distance for groups that are to be included for abundance estimation. Also determines the bins used for grouped perpendicular distances. beaufort_range: Vector of Beaufort sea states (integers) that are acceptable in estimating the detection function and density. Beaufort data with a decimal place will be rounded to the nearest integer to evaluate for inclusion. abeam_sightings: = If TRUE, sightings that occur aft of beam are included in estimating the detection function and densities. Default is FALSE: all abeam sightings will be ignored. strata_overlap_handling: In the event that survey strata overlap, this setting tells R how to handle it. The options are \"smallest\" (the default), in which effort can belong to only a single stratum, and the smallest of overlapping strata will be used (e.g., an insular polygon nested within a larger EEZ polygon); \"largest\", in which effort can belong to only a single stratum and the largest of overlapping strata will be used (we are not sure what use case this would serve, but we offer it as an option for niche analyses); and \"each\", in which each effort is allowed to belong to two or more strata at once and all analyses will be conducted for each overlapping polygon separately (e.g., this may be appropriate for nested strata in which you want to estimate density in the entirety of the larger stratum in addition to estimating density for the nested stratum). distance_types: A character vector of the effort types that will be included in detection function estimation and density estimation, and therefore considered in effort segmentizing. Accepted values are \"S\" (systematic/standard effort), \"F\" (fine-scale effort), and \"N\" (non-systematic/non-standard effort, in which systematic protocols are being used but effort is not occurring along design-based transect routes). The default values are c(\"S\",\"F\",\"N\"). distance_modes: The effort modes that will be included in detection function estimation and density estimation, and therefore considered in effort segmentizing. Accepted values are \"P\" (passing) and \"C\" (closing), and the default values are c(\"P\",\"C\"). distance_on_off: The value(s) of OnEffort (On Effort is TRUE, Off Effort is FALSE) that will be included in detection function estimation and density estimation, and therefore considered in effort segmentizing. Default is TRUE only. (We don’t expect FALSE or c(TRUE,FALSE) to be used much, if at all, but we make this option available). Example code Use settings defaults No strata or group size calibration. settings &lt;- load_settings() Use settings defaults, but with strata # Load strata dataframes data(strata_cnp) data(study_cnp) settings &lt;- load_settings(strata = strata_cnp, study_area = study_cnp) Customize survey, but not cohorts When a cohort is not specified, the default values will be used. # Load strata dataframes data(strata_cnp) data(study_cnp) # Load group size coefficients data(group_size_coefficients) # Load ships data(ships) # Load species codes data(species_codes) # Survey settings survey &lt;- load_survey_settings(out_handling = &#39;stratum&#39;, max_row_interval = 700, segment_method = &#39;equallength&#39;, segment_target_km = 30, segment_max_interval = 48, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), ship_list = ships, species_codes = species_codes, group_size_coefficients = group_size_coefficients, smear_angles = FALSE, verbose = TRUE) # Load settings settings &lt;- load_settings(strata = strata_cnp, study_area = study_cnp, survey) Fully custom: strata, study area, survey &amp; cohorts These are the settings we will use in the remainder of the tutorial: # Load strata data(strata_cnp) data(study_cnp) # Load group size coefficients data(group_size_coefficients) # Load ships data(ships) # Load species codes data(species_codes) # Survey settings survey &lt;- load_survey_settings(out_handling = &#39;stratum&#39;, max_row_interval = 700, segment_method = &#39;equallength&#39;, segment_target_km = 30, segment_max_interval = 48, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), ship_list = ships, species_codes = species_codes, group_size_coefficients = group_size_coefficients, smear_angles = FALSE, verbose = TRUE) # Cohort 1 (default) cohort1 &lt;- load_cohort_settings() # Cohort 2 cohort2 &lt;- load_cohort_settings(id=&#39;fkw_insular&#39;, species=33, use_low_if_na = TRUE, truncation_km = 5, strata_overlap_handling = &#39;each&#39;) # Load settings settings &lt;- load_settings(strata = strata_cnp, study_area = study_cnp, survey = survey, cohorts=list(cohort1, cohort2)) "],["processing.html", " 4 Data processing Behind the scenes Review", " 4 Data processing You can process your survey data using a single function, process_surveys(), which takes two arguments: the filepath(s) to your DAS survey data, and your settings object. For example: demo &lt;- process_surveys(das_file = &#39;data/surveys/HICEASwinter2020.das&#39;, settings = settings) That single command will convert your raw DAS data to a “cruz” object, a list of polished datasets that are prepared to be passed to subsequent analyses. That function is a wrapper for several discrete stages of data formatting/processing. Behind the scenes, each of those stages is carried out using a specific LTabundR function. The remainder of this page is a detailed step-by-step explanation of the data processing that occurs when you call process_surveys(). Behind the scenes Bring in cruise data Specify the path to your .DAS data file(s): das_file &lt;- &#39;data/surveys/HICEASwinter2020.das&#39; Read in and process this .DAS file using the functions in Sam’s swfscDAS package. To do so quickly, we built a wrapper function that makes this quick and easy: das &lt;- load_das(das_file, perform_checks = TRUE, print_glimpse = TRUE) Cruise numbers: 2001 &lt;NA&gt; 48 0 Rows: 22,486 Columns: 40 $ Event &lt;chr&gt; &quot;*&quot;, &quot;*&quot;, &quot;*&quot;, &quot;*&quot;, &quot;*&quot;, &quot;*&quot;, &quot;*&quot;, &quot;*&quot;, &quot;*&quot;, &quot;*&quot;, &quot;B&quot;, &quot;R&quot;, … $ DateTime &lt;dttm&gt; 2020-01-19 07:11:52, 2020-01-19 07:13:52, 2020-01-19 07:15:… $ Lat &lt;dbl&gt; 21.79983, 21.80517, 21.81050, 21.81583, 21.82133, 21.82667, … $ Lon &lt;dbl&gt; -159.7652, -159.7657, -159.7662, -159.7668, -159.7673, -159.… $ OnEffort &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… $ Cruise &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2001, 2001, 2001, 20… $ Mode &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, … $ OffsetGMT &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, -10, -10, -10, -10, … $ EffType &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;… $ ESWsides &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, 2, 2, 2, 2, 2… $ Course &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 350,… $ SpdKt &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 9.9,… $ Bft &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 4, 4, 4,… $ SwellHght &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 6, 6, 6,… $ WindSpdKt &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 15, 15, … $ RainFog &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ HorizSun &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ VertSun &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ Glare &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ Vis &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ ObsL &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;126&quot;, &quot;126&quot;… $ Rec &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;307&quot;, &quot;307&quot;… $ ObsR &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;238&quot;, &quot;238&quot;… $ ObsInd &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ Data1 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;2001&quot;, &quot;F&quot;, &quot;126&quot;, … $ Data2 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;C&quot;, NA, &quot;307&quot;, &quot;06&quot;… $ Data3 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;-10&quot;, NA, &quot;238&quot;, &quot;1… $ Data4 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;N&quot;, NA, NA, NA, NA,… $ Data5 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;15.0&quot;, … $ Data6 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ Data7 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ Data8 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ Data9 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ Data10 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ Data11 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ Data12 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ EffortDot &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… $ EventNum &lt;chr&gt; &quot;001&quot;, &quot;002&quot;, &quot;003&quot;, &quot;004&quot;, &quot;005&quot;, &quot;006&quot;, &quot;007&quot;, &quot;008&quot;, &quot;009… $ file_das &lt;chr&gt; &quot;HICEASwinter2020.das&quot;, &quot;HICEASwinter2020.das&quot;, &quot;HICEASwinte… $ line_num &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1… Process strata Run the following function to add strata and study-area information to each row of DAS data: dass &lt;- process_strata(das, settings) This function loops through each stratum data.frame you have provided it in settings$strata, formats the stratum, and asks whether each DAS row occurs within it. For each stratum, a column named stratum_&lt;StratumName&gt; is added to the das object; each row in this column is TRUE (included) or FALSE. A similar procedure is run if a dataframe is provided in settings$study_area. A column named study_area is added to das containing a boolean (TRUE if the sub-segment or sighting occurs within the study area). Format DAS data into a cruz object The function format_das() takes care of some final formatting and initiates the cruz object data structure. cruz &lt;- format_das(dass, verbose=TRUE) This function (1) remove rows with invalid Cruise numbers, times, or locations; (ii) calculate the distance, in km, between each row of data; (iii) adds a ship column to the dataset, with initials for the ship corresponding to each cruise; (iv) creates a new list, cohorts, which copies the cruise data for each cohort specified in your settings; and (v) adds a stratum column to the data in each cohort. That column specifies a single stratum assignment for each row of DAS data in the event of overlapping strata, based upon the cohort setting stratum_overlap_handling. The cruz object The function format_das() returns a list, which we have saved in an object named cruz, with several slots: cruz %&gt;% names [1] &quot;settings&quot; &quot;strata&quot; &quot;study_area&quot; &quot;cohorts&quot; The slots strata and study_area provide the area, in square km, of each polygon being used: cruz$strata stratum area 1 HI_EEZ 2474595.8 2 WHICEAS 402946.7 3 OtherCNP 34215265.2 cruz$study_area [1] 34215265 The slot cohorts is itself a list with one slot for each cohort. The slots are named using the id cohort setting. cruz$cohorts %&gt;% names [1] &quot;default&quot; &quot;fkw_insular&quot; Each cohort slot has a copy of the DAS data with a new stratum column, with a stratum assignment tailored to its cohort-specific settings. For instance, the default cohort, whose stratum_overlap_handling is set to \"smallest\", assigns the smallest stratum in the event of overlapping or nested strata: cruz$cohorts$default$stratum %&gt;% table(useNA=&#39;ifany&#39;) . HI_EEZ WHICEAS 143 22272 The fkw_insular cohort, whose stratum_overlap_handling is set to \"each\" (i.e., effort is allowed to belong to multiple segments, if they overlap, and all analyses will be conducted for each stratum separately), has stratum assignments that look like this; cruz$cohorts$fkw_insular$stratum %&gt;% table(useNA=&#39;ifany&#39;) . HI_EEZ&amp;OtherCNP HI_EEZ&amp;WHICEAS&amp;OtherCNP 143 22272 When a row of DAS effort occurs in two overlapping strata, the stratum assignment for that row is a concatentation of the names of the strata it falls within, with names separated by “&amp;”. This list, with these five primary slots, will be referred to as a cruz object. Segmentize the data To allocate survey data into discrete ‘effort segments’, which are used in variance estimation in subsequent steps, run the function segmentize(). This process is controlled by both survey-wide and cohort-specific settings, which are now carried in a slot within the cruz object. The process is outlined in detail in the Appendix on Segmentizing. cruz &lt;- segmentize(cruz, verbose=FALSE) This function does not change the high-level structure of the cruz object … cruz %&gt;% names [1] &quot;settings&quot; &quot;strata&quot; &quot;study_area&quot; &quot;cohorts&quot; … or the cohort names in the cohorts slot: cruz$cohorts %&gt;% names [1] &quot;default&quot; &quot;fkw_insular&quot; For each cohorts slot, the list structure is the same: cruz$cohorts$default %&gt;% names [1] &quot;segments&quot; &quot;das&quot; cruz$cohorts$fkw_insular %&gt;% names [1] &quot;segments&quot; &quot;das&quot; The segments slot contains summary data for each effort segment, including start/mid/end coordinates, average conditions, and segment distance: cruz$cohorts$default$segments %&gt;% glimpse Rows: 273 Columns: 38 $ Cruise &lt;dbl&gt; 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 200… $ ship &lt;chr&gt; &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;… $ stratum &lt;chr&gt; &quot;HI_EEZ&quot;, &quot;HI_EEZ&quot;, &quot;HI_EEZ&quot;, &quot;HI_EEZ&quot;, &quot;HI_EEZ&quot;, &quot;HI_EEZ… $ study_area &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRU… $ seg_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… $ yday &lt;dbl&gt; 21, 22, 39, 68, 68, 22, 19, 19, 20, 20, 21, 22, 23, 23, 2… $ dist &lt;dbl&gt; 0.0000000, 0.0000000, 41.1937696, 0.3496968, 6.7175579, 1… $ lat1 &lt;dbl&gt; 22.33300, 22.68750, 20.79283, 21.66800, 21.67700, 22.6875… $ lon1 &lt;dbl&gt; -161.2520, -161.1208, -153.6117, -161.9232, -161.8537, -1… $ DateTime1 &lt;dttm&gt; 2020-01-21 07:28:48, 2020-01-22 07:42:01, 2020-02-08 06:… $ timestamp1 &lt;dbl&gt; 1579591728, 1579678921, 1581144814, 1583692575, 158368981… $ lat2 &lt;dbl&gt; 22.68750, 20.79217, 21.66967, 21.66717, 21.69233, 22.6861… $ lon2 &lt;dbl&gt; -161.1208, -153.6090, -161.9245, -161.9325, -161.9128, -1… $ DateTime2 &lt;dttm&gt; 2020-01-22 07:42:01, 2020-02-08 06:51:34, 2020-03-08 18:… $ timestamp2 &lt;dbl&gt; 1579678921, 1581144694, 1583692455, 1583692935, 158369107… $ mlat &lt;dbl&gt; 22.69117, 20.79133, 21.64500, 21.66700, 21.68417, 22.6875… $ mlon &lt;dbl&gt; -161.1353, -153.6063, -161.7297, -161.9262, -161.8822, -1… $ mDateTime &lt;dttm&gt; 2020-01-22 07:31:52, 2020-02-08 06:49:34, 2020-03-08 17:… $ mtimestamp &lt;dbl&gt; 1579591728, 1579678921, 1581144814, 1583692575, 158368981… $ use &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FAL… $ Mode &lt;chr&gt; NA, &quot;P&quot;, NA, &quot;C&quot;, &quot;C&quot;, &quot;P&quot;, &quot;C&quot;, &quot;C&quot;, NA, NA, &quot;C&quot;, &quot;C&quot;, &quot;… $ EffType &lt;chr&gt; NA, &quot;S&quot;, NA, &quot;N&quot;, &quot;N&quot;, &quot;S&quot;, NA, &quot;F&quot;, NA, NA, &quot;F&quot;, &quot;S&quot;, &quot;S… $ OnEffort &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE… $ ESWsides &lt;dbl&gt; NA, 2, NA, 2, 2, 2, NA, 2, NA, NA, 2, 2, 2, 2, 2, 2, 2, 2… $ year &lt;dbl&gt; 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 202… $ month &lt;dbl&gt; 1, 1, 2, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … $ day &lt;int&gt; 21, 22, 8, 8, 8, 22, 19, 19, 20, 20, 21, 22, 23, 23, 24, … $ min_line &lt;int&gt; 1080, 1549, 9811, 20883, 20845, 1550, 11, 347, 668, 772, … $ max_line &lt;int&gt; 1548, 9810, 20882, 20887, 20867, 1554, 346, 667, 771, 147… $ n_rows &lt;int&gt; 14, 3, 93, 5, 23, 5, 95, 153, 104, 195, 123, 103, 86, 85,… $ avgBft &lt;dbl&gt; NaN, NaN, 6.909636, 6.000000, 6.000000, 2.000000, 4.67892… $ avgSwellHght &lt;dbl&gt; NaN, NaN, 7.000000, 7.000000, 7.000000, 4.000000, 7.45332… $ avgHorizSun &lt;dbl&gt; NaN, NaN, 10.970763, 11.000000, 11.000000, NaN, 7.346991,… $ avgVertSun &lt;dbl&gt; NaN, NaN, 1.424158, 3.000000, 2.523285, NaN, 1.402253, 2.… $ avgGlare &lt;dbl&gt; NaN, NaN, 0.3337938, 1.0000000, 1.0000000, NaN, 0.4405329… $ avgVis &lt;dbl&gt; NaN, NaN, 4.954818, 4.500000, 4.500000, 6.200000, 5.72714… $ avgCourse &lt;dbl&gt; NaN, NaN, 281.00635, 202.00000, 286.52329, 105.00000, 217… $ avgSpdKt &lt;dbl&gt; NaN, NaN, 9.777758, 7.400000, 9.613971, 9.000000, 8.74709… # Number of segments cruz$cohorts$default$segments %&gt;% nrow [1] 273 # Segment length distribution hist(cruz$cohorts$default$segments$dist, breaks = seq(0,60,by=1), xlab=&#39;Segment lengths (km)&#39;, main=paste0(&#39;Target km: &#39;,settings$survey$segment_target_km)) And the das slot holds the original data.frame of DAS data, modified slightly: the column OnEffort has been modified according to Beaufort range conditions, and the column seg_id indicates which segment the event occurs within cruz$cohorts$default$das %&gt;% names [1] &quot;Event&quot; &quot;DateTime&quot; &quot;Lat&quot; &quot;Lon&quot; [5] &quot;OnEffort&quot; &quot;Cruise&quot; &quot;Mode&quot; &quot;OffsetGMT&quot; [9] &quot;EffType&quot; &quot;ESWsides&quot; &quot;Course&quot; &quot;SpdKt&quot; [13] &quot;Bft&quot; &quot;SwellHght&quot; &quot;WindSpdKt&quot; &quot;RainFog&quot; [17] &quot;HorizSun&quot; &quot;VertSun&quot; &quot;Glare&quot; &quot;Vis&quot; [21] &quot;ObsL&quot; &quot;Rec&quot; &quot;ObsR&quot; &quot;ObsInd&quot; [25] &quot;Data1&quot; &quot;Data2&quot; &quot;Data3&quot; &quot;Data4&quot; [29] &quot;Data5&quot; &quot;Data6&quot; &quot;Data7&quot; &quot;Data8&quot; [33] &quot;Data9&quot; &quot;Data10&quot; &quot;Data11&quot; &quot;Data12&quot; [37] &quot;EffortDot&quot; &quot;EventNum&quot; &quot;file_das&quot; &quot;line_num&quot; [41] &quot;stratum_HI_EEZ&quot; &quot;stratum_WHICEAS&quot; &quot;stratum_OtherCNP&quot; &quot;study_area&quot; [45] &quot;year&quot; &quot;month&quot; &quot;day&quot; &quot;yday&quot; [49] &quot;km_int&quot; &quot;km_cum&quot; &quot;ship&quot; &quot;stratum&quot; [53] &quot;seg_id&quot; &quot;use&quot; The segmentize() function and its associated settings were designed to give researchers full control over how data are segmented, be it for design-based density analysis (which tend to use long segments of 100 km or more and allow for non-contiguous effort to be included in the same segment) or for habitat modeling (which tend to use short segments of 5 - 10 km and disallow non-contiguous effort to be pooled into the same segment). To demonstrate that versatility, checkout the appendix on segmentizing. Process sightings To process sightings for each cohort of species, use the function process_sightings(). This function has three basic steps: for each cohort, the function (1) prepares a sightings table using the function das_sight() from swfscDAS; (2) filters those sightings to species codes specified for the cohort in your settings input; and (3) evaluates each of those sightings, asking if each should be included in the analysis according to your settings. cruz &lt;- process_sightings(cruz) The function produces a formatted dataset and adds it to a new sightings slot. It does this for each analysis (density and, if specified, distance) in each cohort. cruz$cohorts$default %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; cruz$cohorts$fkw_insular %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; Note that the sightings table has a column named included (TRUE = yes, use it in the analysis). Any sightings that do not meet the inclusion criteria as specified in your settings will be included = FALSE, but they won’t be removed from the data. Since the sightings in each cohort are processed slightly differently according to the cohort’s specific settings, you should expect different numbers of included/excluded sightings in each cohort-analysis dataset: cruz$cohorts$default$sightings$included %&gt;% table . FALSE TRUE 93 235 cruz$cohorts$fkw_insular$sightings$included %&gt;% table . TRUE 4 When this function’s verbose argument is TRUE (the default), a message is printed each time a sighting does not meet the inclusion criteria (see above). Sightings data structure The sightings table has many other variables: cruz$cohorts$default$sightings %&gt;% names [1] &quot;Event&quot; &quot;DateTime&quot; &quot;Lat&quot; &quot;Lon&quot; [5] &quot;OnEffort&quot; &quot;Cruise&quot; &quot;Mode&quot; &quot;OffsetGMT&quot; [9] &quot;EffType&quot; &quot;ESWsides&quot; &quot;Course&quot; &quot;SpdKt&quot; [13] &quot;Bft&quot; &quot;SwellHght&quot; &quot;WindSpdKt&quot; &quot;RainFog&quot; [17] &quot;HorizSun&quot; &quot;VertSun&quot; &quot;Glare&quot; &quot;Vis&quot; [21] &quot;ObsL&quot; &quot;Rec&quot; &quot;ObsR&quot; &quot;ObsInd&quot; [25] &quot;EffortDot&quot; &quot;EventNum&quot; &quot;file_das&quot; &quot;line_num&quot; [29] &quot;stratum_HI_EEZ&quot; &quot;stratum_WHICEAS&quot; &quot;stratum_OtherCNP&quot; &quot;study_area&quot; [33] &quot;year&quot; &quot;month&quot; &quot;day&quot; &quot;yday&quot; [37] &quot;km_int&quot; &quot;km_cum&quot; &quot;ship&quot; &quot;stratum&quot; [41] &quot;seg_id&quot; &quot;use&quot; &quot;SightNo&quot; &quot;Subgroup&quot; [45] &quot;SightNoDaily&quot; &quot;Obs&quot; &quot;ObsStd&quot; &quot;Bearing&quot; [49] &quot;Reticle&quot; &quot;DistNm&quot; &quot;Cue&quot; &quot;Method&quot; [53] &quot;Photos&quot; &quot;Birds&quot; &quot;CalibSchool&quot; &quot;PhotosAerial&quot; [57] &quot;Biopsy&quot; &quot;CourseSchool&quot; &quot;TurtleSp&quot; &quot;TurtleGs&quot; [61] &quot;TurtleJFR&quot; &quot;TurtleAge&quot; &quot;TurtleCapt&quot; &quot;PinnipedSp&quot; [65] &quot;PinnipedGs&quot; &quot;BoatType&quot; &quot;BoatGs&quot; &quot;PerpDistKm&quot; [69] &quot;species&quot; &quot;best&quot; &quot;low&quot; &quot;high&quot; [73] &quot;prob&quot; &quot;mixed&quot; &quot;ss_tot&quot; &quot;lnsstot&quot; [77] &quot;ss_percent&quot; &quot;n_sp&quot; &quot;n_obs&quot; &quot;n_best&quot; [81] &quot;n_low&quot; &quot;n_high&quot; &quot;calibr&quot; &quot;mixed_max&quot; [85] &quot;spp_max&quot; &quot;included&quot; Columns 43 onwards correspond to sightings information. Columns of note: species contains the species code. There is only one species-code per row (i.e, multi-species sightings have been expanded to multiple rows). best, low, and high contain the refined group size estimates, averaged across observers and calibrated according to the cohort’s settings specifications. For multi-species sightings, these numbers represent the number of individuals for the single species represented in the row (i.e., the original group size estimate has been scaled by the percentage attritbuted to this species). The columns following those group size estimates (prob through spp_max) detail how group sizes were estimated: prob indicates whether probable species codes were accepted; mixed indicates whether this species’ sighting is part of a mixed-species sighting; n_sp provides the number of species occurring in this sighitng; n_obs gives the number of observers who contributed group size estimates; n_best through n_high gives the number of valid group size estimates given; and calibr indicates whether or not calibration was attempted for this sighting based on the settings (see next section); mixed_max indicates whether this species was the most abundant in the sighting (if multi-species); spp_max indicates the species code for the most abundant species in the sighting (if multi-species). As explained above, the final column, included, indicates whether this species should be included in the analysis. Here is a glimpse of the data: cruz$cohorts$fkw_insular$sightings %&gt;% glimpse Rows: 4 Columns: 86 $ Event &lt;chr&gt; &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot; $ DateTime &lt;dttm&gt; 2020-01-28 16:35:09, 2020-02-27 11:52:38, 2020-02-28 … $ Lat &lt;dbl&gt; 20.80800, 18.83133, 19.13683, 18.20767 $ Lon &lt;dbl&gt; -158.3488, -157.1577, -156.3863, -154.7485 $ OnEffort &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE $ Cruise &lt;dbl&gt; 2001, 2001, 2001, 2001 $ Mode &lt;chr&gt; &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot; $ OffsetGMT &lt;int&gt; -10, -10, -10, -10 $ EffType &lt;chr&gt; &quot;S&quot;, &quot;S&quot;, &quot;N&quot;, &quot;S&quot; $ ESWsides &lt;dbl&gt; 2, 2, 2, 2 $ Course &lt;dbl&gt; 122, 285, 90, 107 $ SpdKt &lt;dbl&gt; 9.6, 9.4, 8.6, 7.9 $ Bft &lt;dbl&gt; 3, 5, 5, 6 $ SwellHght &lt;dbl&gt; 5, 8, 7, 11 $ WindSpdKt &lt;dbl&gt; 10, 17, 17, 24 $ RainFog &lt;dbl&gt; 5, 5, 5, 5 $ HorizSun &lt;dbl&gt; 4, 7, 3, 4 $ VertSun &lt;dbl&gt; 2, 1, 1, 1 $ Glare &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE $ Vis &lt;dbl&gt; 6.0, 5.5, 5.0, 4.0 $ ObsL &lt;chr&gt; &quot;238&quot;, &quot;238&quot;, &quot;197&quot;, &quot;125&quot; $ Rec &lt;chr&gt; &quot;125&quot;, &quot;125&quot;, &quot;227&quot;, &quot;197&quot; $ ObsR &lt;chr&gt; &quot;197&quot;, &quot;197&quot;, &quot;126&quot;, &quot;227&quot; $ ObsInd &lt;chr&gt; NA, NA, NA, NA $ EffortDot &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE $ EventNum &lt;chr&gt; &quot;422&quot;, &quot;213&quot;, &quot;238&quot;, &quot;371&quot; $ file_das &lt;chr&gt; &quot;HICEASwinter2020.das&quot;, &quot;HICEASwinter2020.das&quot;, &quot;HIC… $ line_num &lt;int&gt; 5016, 15756, 16275, 18395 $ stratum_HI_EEZ &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE $ stratum_WHICEAS &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE $ stratum_OtherCNP &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE $ study_area &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE $ year &lt;dbl&gt; 2020, 2020, 2020, 2020 $ month &lt;dbl&gt; 1, 2, 2, 3 $ day &lt;int&gt; 28, 27, 28, 3 $ yday &lt;dbl&gt; 28, 58, 59, 63 $ km_int &lt;dbl&gt; 0, 0, 0, 0 $ km_cum &lt;dbl&gt; 1646.400, 5375.415, 5560.489, 6239.292 $ ship &lt;chr&gt; &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot; $ stratum &lt;chr&gt; &quot;HI_EEZ&amp;WHICEAS&amp;OtherCNP&quot;, &quot;HI_EEZ&amp;WHICEAS&amp;OtherCNP&quot;,… $ seg_id &lt;int&gt; 158, 232, 111, 242 $ use &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE $ SightNo &lt;chr&gt; &quot;131&quot;, &quot;255&quot;, &quot;258&quot;, &quot;285&quot; $ Subgroup &lt;chr&gt; NA, NA, NA, NA $ SightNoDaily &lt;chr&gt; &quot;20200128_8&quot;, &quot;20200227_19&quot;, &quot;20200228_18&quot;, &quot;20200303… $ Obs &lt;chr&gt; &quot;197&quot;, &quot;125&quot;, &quot;126&quot;, &quot;125&quot; $ ObsStd &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE $ Bearing &lt;dbl&gt; 79, 0, 15, 314 $ Reticle &lt;dbl&gt; 1.8, NA, 1.5, 5.0 $ DistNm &lt;dbl&gt; 1.88, 0.20, 2.09, 0.92 $ Cue &lt;dbl&gt; 3, 2, 3, 3 $ Method &lt;dbl&gt; 4, 1, 4, 4 $ Photos &lt;chr&gt; &quot;Y&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot; $ Birds &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot; $ CalibSchool &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot; $ PhotosAerial &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot; $ Biopsy &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot; $ CourseSchool &lt;dbl&gt; NA, NA, NA, NA $ TurtleSp &lt;chr&gt; NA, NA, NA, NA $ TurtleGs &lt;dbl&gt; NA, NA, NA, NA $ TurtleJFR &lt;chr&gt; NA, NA, NA, NA $ TurtleAge &lt;chr&gt; NA, NA, NA, NA $ TurtleCapt &lt;chr&gt; NA, NA, NA, NA $ PinnipedSp &lt;chr&gt; NA, NA, NA, NA $ PinnipedGs &lt;dbl&gt; NA, NA, NA, NA $ BoatType &lt;chr&gt; NA, NA, NA, NA $ BoatGs &lt;dbl&gt; NA, NA, NA, NA $ PerpDistKm &lt;dbl&gt; 3.417790, 0.000000, 1.001806, 1.225640 $ species &lt;chr&gt; &quot;033&quot;, &quot;033&quot;, &quot;033&quot;, &quot;033&quot; $ best &lt;dbl&gt; 35.096667, 1.000000, 13.913043, 6.956522 $ low &lt;dbl&gt; 19.672365, NA, 9.165151, 5.000000 $ high &lt;dbl&gt; 43.42268, NA, 16.58312, 20.00000 $ prob &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE $ mixed &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE $ ss_tot &lt;dbl&gt; 35.096667, 1.000000, 13.913043, 6.956522 $ lnsstot &lt;dbl&gt; 3.558106, 0.000000, 2.632827, 1.939680 $ ss_percent &lt;dbl&gt; 1, NaN, 1, 1 $ n_sp &lt;dbl&gt; 1, 1, 1, 1 $ n_obs &lt;int&gt; 3, 1, 2, 1 $ n_best &lt;int&gt; 3, 0, 2, 1 $ n_low &lt;int&gt; 3, 0, 2, 1 $ n_high &lt;int&gt; 3, 0, 2, 1 $ calibr &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE $ mixed_max &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE $ spp_max &lt;chr&gt; &quot;033&quot;, NA, &quot;033&quot;, &quot;033&quot; $ included &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE Note that the process_sightings() function draws upon cruz$settings for inclusion criteria, but some of those settings can be overridden with the function’s manual inputs if you want to explore your options (see below). School size estimates In the settings we are using in this tutorial, school size estimates are adjusted using the calibration models from Barlow, Gerrodette, and Perryman (1998) (their analysis is refined slightly and further explained in Gerrodette, Perryman and Barlow, 2002). These calibration corrections are observer-specific. Some observers tend to underestimate school size and their estimates are adjusted up; others tend to overestimate and their estimates are adjusted down. Some observers do not have calibration coefficients, and for them a generic adjustment (upwards, by dividing estimates by 0.8625) is used. Each observer’s estimate is calibrated, then all observer estimates are averaged. To do that averaging, our settings specify that we shall use a geometric weighted mean, instead of an arithmetic mean, that weights school size estimates from multiple observers according to the variance of their calibration coefficients. Here are our current best estimates of school size: cruz$cohorts$default$sightings$best %&gt;% head(20) [1] 1.1594203 19.1620266 3.4702883 1.1594203 2.3188406 2.3188406 [7] 2.3188406 3.4782609 2.3188406 2.3188406 1.1594203 1.1594203 [13] 0.9370535 73.2878434 1.1594203 1.1594203 4.6376812 7.7776277 [19] 1.1594203 1.1594203 Let’s compare those estimates to unadjusted ones, in which calibration (and therefore weighted geometric mean) is turned off: cruz_demo &lt;- process_sightings(cruz, calibrate = FALSE, verbose = FALSE) cruz_demo$cohorts$defaul$sightings$best %&gt;% head(20) [1] 1.000000 19.077486 3.454978 1.000000 2.000000 2.000000 2.000000 [8] 3.000000 2.000000 2.000000 1.000000 1.000000 1.000000 62.970508 [15] 1.000000 1.000000 4.000000 6.708204 1.000000 1.000000 Note that, since calibration is only used for schools above a certain size, the difference between calibration and non-calibrated estimates becomes clearer in larger groups. You can also carry out calibration corrections without using a geometric weighted mean (the arithmetic mean will be used instead): cruz_demo &lt;- process_sightings(cruz, calibrate = TRUE, geometric_mean = FALSE, verbose = FALSE) cruz_demo$cohorts$default$sightings$best %&gt;% head(20) [1] 1.1594203 22.7906203 4.1274352 1.1594203 2.3188406 2.3188406 [7] 2.3188406 3.4782609 2.3188406 2.3188406 1.1594203 1.1594203 [13] 0.9370535 89.9371981 1.1594203 1.1594203 4.6376812 8.1159420 [19] 1.1594203 1.1594203 Note that when geometric_mean = TRUE but calibration is not carried out, the simple geometric mean is calculated instead of the weighted geometric mean, since the weights are the variance estimates from the calibration routine. Also note that school size calibration is only carried out if settings$group_size_calibration is not NULL. However, even when calibration coefficients are provided, it is possible to specify that calibration should only be carried out for raw estimates above a minimum threshold (see cohort setting calibration_floor, whose default is 0), since observers may be unlikely to mis-estimate the school size of a lone whale or pair. For observers who have calibration coefficients in the settings$group_size_coefficients table, that minimum is specified for each observer individually. For observers not in that table, calibration will only be applied to raw school size estimates above settings$cohorts[[i]]$calibration_floor or above. Subgroup size estimates After sightings data are processed, the process_surveys() function calls the subroutine process_subgroups() to find and calculate subgroup school size estimates for false killer whales, if any occur in the DAS data (Event code “G”). If subgroups are found, a subgroups slot is added to the analysis list for a cohort. This subgroups slot holds a list with three dataframes: events (each row is a school size estimate for a single subgroup during a single phase – 1 or 2 – within a single sighting); subgroups (each row is a single phase for a single subgroup, with all school size estimates averaged together (both arithmetically and geometrically); and sightings (each row is a school size estimate for a single phase for a single sighting, with all subgroup school sizes summed together). Examples of these datasets will be provided at a later date. Review By the end of this process, you have a single data object, cruz, with all the data you need to move forward into the next stages of mapping and analysis. cruz %&gt;% names [1] &quot;settings&quot; &quot;strata&quot; &quot;study_area&quot; &quot;cohorts&quot; Each species-specific cohort has its own list under cruz$cohorts: cruz$cohorts %&gt;% names [1] &quot;default&quot; &quot;fkw_insular&quot; Each of these cohorts has the same list structure: cruz$cohorts$default %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; segments is a summary table of segments. das is the raw DAS data, modified with seg_id to associate each row with a segment. sightings is a dataframe of sightings processed according to this cohort’s settings. subgroups (if any subgroup data exist in your survey) is a list with subgroup details. In each of theese data.frames, there are three critically important columns to keep in mind: seg_id: this column is used to indicate the segment ID that a row of data belongs to. use: this column indicates whether a row of effort should be used in the line-transect analysis Every row of data within a single segment with have the same use value. included: this column occurs in the sightings dataframe only. It indicates whether the sightings should be included in line-transect analysis based on the specified settings. Any sighting with use == FALSE will also have included == FALSE, but it is possible for sightings to have use == TRUE with included == FALSE. For example, if the setting abeam_sightings is set to FALSE, a sighting with a bearing angle beyond the ship’s beam can be excluded from the analysis (included == FALSE) even though the effort segment it occurs within will still be used (use == TRUE). "],["maps.html", " 5 Maps Publishable maps Interactive maps Interactive dashboard", " 5 Maps To build a flexible system for mapping cruise data, we have the following functions: Publishable maps Base maps Begin with a basic map, including EEZ borders: m &lt;- map_base(region=&#39;cnp&#39;) m We also have a base map for the California Current … m &lt;- map_base(region=&#39;ccs&#39;) m And the ETP: m &lt;- map_base(region=&#39;etp&#39;) m Add strata Add your research strata to your map: m &lt;- map_base(region=&#39;cnp&#39;) m &lt;- map_strata(m, cruz$settings, region=&#39;cnp&#39;) m Add survey tracks m1 &lt;- map_effort(m, cruz) m1 The defaults of map_effort() assume, for simplicity, that you want to see the segments to be included in density estimation for the first cohort specified in your settings. You can adjust this and other defaults using the function arguments. Customizing effort Inputs To demonstrate some of the customization options, consider this map that shows segments to be excluded from the \"distance\" (detection function) analysis for our second cohort (fkw_insular). map_effort(m, cruz, cohort = 2, use_type = c(FALSE), effort_color=&#39;firebrick&#39;, effort_stroke=2.5, effort_linetype=1,) Color-code conditions Your second customization option is to add format variables to the segments slot of the cohort of interest in the cruz object. This gives you full control of line color, thickness, and line-type according to whatever specifications you wish to set, e.g., color-coding by effort type or Beaufort sea state. This is possible because the function map_effort() looks for the variables col (line color), lwd (line thickness or stroke), and lty (line type) in the columns of cruz$segments. If these columns exist, the values therein will be used instead of the function defaults. For example, color-code by Beaufort scale: # Save copy of cruz object data to modify cruz2 &lt;- cruz segments &lt;- cruz2$cohorts$default$segments # Add column `col`: color code by BFT sea state bft_colors &lt;- c(&#39;steelblue4&#39;,&#39;steelblue2&#39;,&#39;cadetblue1&#39;,&#39;grey&#39;) segments$col &lt;- bft_colors[4] segments$col[ segments$avgBft &lt;= 7 ] &lt;- bft_colors[3] # bft 5 + segments$col[ segments$avgBft &lt;= 4 ] &lt;- bft_colors[2] # bft 3 - 4 segments$col[ segments$avgBft &lt;= 2 ] &lt;- bft_colors[1] # bft 0 -2 # Update sub_segments slot in `cruz` object cruz2$cohorts$default$segments &lt;- segments # Update map m_custom2 &lt;- map_effort(m, cruz2) # Add legend using native functions from mapping package `tmap` m_custom2 &lt;- m_custom2 + tmap::tm_add_legend(&#39;line&#39;, col = bft_colors, lwd = 3, labels = c(&#39; 0 - 2&#39;, &#39; 3 - 4&#39;, &#39; 5 +&#39;, &#39; no data&#39;), title=&quot;Beaufort sea state&quot;) + tmap::tm_layout(legend.position=c(&#39;left&#39;,&#39;bottom&#39;)) # Show map m_custom2 Add sightings Use the function map_sightings() to add sightings to your map: map_sightings(m, cruz) Customizing sightings To demonstrate some of the customization options, consider this map that shows sightings of false killer whales with custom dot color, shape, and size: map_sightings(m, cruz, include_species = &#39;033&#39;, color_base = &#39;purple&#39;, shape_base = 18, size_base = 1) Next is a map of humpback whales and sperm whales, color-coded by species and shape-coded by whether or not the sighting will be included in the analysis: map_sightings(m, cruz, include_species = c(&#39;076&#39;,&#39;046&#39;), color_code = TRUE, shape_code = TRUE) Overview Here is an overview of the steps needed to map strata, survey tracks, and sightings all together: m &lt;- map_base(&#39;cnp&#39;) m &lt;- map_strata(m, cruz$settings) m &lt;- map_effort(m, cruz) m &lt;- map_sightings(m, cruz, size_base=.4) m Interactive maps LTabundR also has an interactive map function, which maps survey data using the leaflet package. map_cruz(cruz, cohort=1, eez_show=FALSE, strata_show=FALSE, effort_show=TRUE, effort_resolution=1, sightings_show=TRUE, sightings_color = &#39;firebrick&#39;, verbose=FALSE) Note that you can also click on sightings and tracklines to see their details. Refer to the documentation for this function (?map_cruz) to see all the options available for stylizing these maps. Interactive dashboard Finally, note that LTabundR comes with an interactive data explorer app (a Shiny app) for filtering survey data according to effort scenario and species code, toggling map_cruz() settings, and reviewing summary tables of effort and sightings (including inspection of truncation distances). cruz_explorer(cruz) Screenshots from this app: "],["summarize.html", " 6 Summarize survey Summarize effort Summarize by Beaufort Summarize sightings Summarize certain species cruz_explorer()", " 6 Summarize survey Summarize effort The summarize_effort() functions builds tables with total kilometers and days surveyed. effort &lt;- summarize_effort(cruz, cohort=1) This function summarizes effort in three default tables: effort %&gt;% names() [1] &quot;total&quot; &quot;total_by_cruise&quot; &quot;total_by_year&quot; &quot;total_by_effort&quot; [5] &quot;total_by_stratum&quot; Total surveyed The slot $total provides the grand total distance and unique dates surveyed: library(DT) effort$total %&gt;% DT::datatable(options=list(initComplete = htmlwidgets::JS( &quot;function(settings, json) {$(this.api().table().container()).css({&#39;font-size&#39;: &#39;9pt&#39;});}&quot;) )) Total surveyed by effort The slot $total_by_effort provides the total distance and days surveyed, grouped by segments that will be included in the analysis and those that won’t: Total surveyed by stratum The slot $total_by_stratum provides the total distance and days surveyed within each stratum, again grouped by segments that will be included in the analysis and those that won’t: Summarize by Beaufort bft &lt;- summarize_bft(cruz, cohort=1) This function summarizes effort by Beaufort in four default tables: bft %&gt;% names() [1] &quot;overall&quot; &quot;by_year&quot; &quot;by_stratum&quot; &quot;details&quot; Simple overall breakdown The slot $overall provides the total effort – and proportion of effort – occurring in each Beaufort state: Breakdown by year The slot $by_year provides the above for each year separately (note that our example data have only one year): Breakdown by stratum The slot $by_stratum provides the above for each geostratum separately: Detailed breakdown The slot $details provides the above for each cruise-year-study area-geostratum combination within the data: Summarize sightings The summarize_sightings() function builds tables summarizing the sightings within each cohort-analysis. (Eventually, we may want to include an option to merge all sightings from all cohort-analyses into a single table.) sightings &lt;- summarize_sightings(cruz, cohort=1) This function summarizes sightings in four default tables: sightings %&gt;% names() [1] &quot;simple_totals&quot; &quot;analysis_totals&quot; [3] &quot;stratum_simple_totals&quot; &quot;stratum_analysis_totals&quot; Simple species totals The slot $simple_totals includes all sightings, even if they will not be inluded in analysis: Analysis totals The slot $analysis_totals only includes sightings that meet all inclusion criteria for the analysis: Simple totals for each stratum The slot $stratum_simple_totals splits the first table (simple species totals) so that sightings are tallied for each geo-stratum separately: Analysis totals for each stratum The slot $stratum_analysis_totals splits the second table (analysis totals for each species) so that sightings are tallied for each geo-stratum separately: Summarize certain species To deep-dive into details for a ceratin species (or group of species), use the function summarize_species(). species &lt;- summarize_species(spp=&#39;046&#39;, cruz) This functions a list with a variety of summaries: species %&gt;% names [1] &quot;species&quot; &quot;n_total&quot; &quot;n_analysis&quot; [4] &quot;school_size&quot; &quot;yearly_total&quot; &quot;yearly_analysis&quot; [7] &quot;regional_total&quot; &quot;regional_analysis&quot; &quot;detection_distances&quot; [10] &quot;sightings&quot; The slots $n_total and $n_analysis provide the total number of sightings and the number eligible for inclusion in the analysis: species$n_total [1] 14 species$n_analysis [1] 10 School size details This table only includes the sightings eligible for analysis: Annual summaries (all sightings) Annual summaries (analysis only) Regional summaries (all sightings) Regional summaries (analysis only) Detection distances This table can be used to determine the best truncation distance to use, based on the percent truncation you wish and the number of sightings available at each option. All sightings data Finally, this last slot holds a dataframe of all sightings data for the specified species: cruz_explorer() Note that all of these summary tables can be viewed interactively using the function cruz_explorer(), which allows you to efficiently subset the data according to various filters. This was already mentioned in the mapping module, but as a reminder that function can be called like so: cruz_explorer(cruz) "],["lta.html", " 7 Line-transect analysis Inputs Variance estimation Other inputs lta() output Behind the scenes", " 7 Line-transect analysis Once you have produced a cruz object with process_surveys(), you are ready to carry out line-transect analyses of your survey data. The main LTabundR function for doing so is lta(), which requires three mandatory inputs in addition to cruz: lta(cruz, fit_filters, df_settings, estimates) Below we explain each of these inputs, discuss other optional inputs, and explore the results produced by lta(). In our examples, we will use processed data from 1986-2017 NOAA surveys in the Central North Pacific, which is provided as a dataset built-in to LTabundR: data(&quot;cnp_1986_2020_150km&quot;) cruz &lt;- cnp_1986_2020_150km We will use these data to estimate the abundance of striped dolphins (scientific), Fraser’s dolphins (scientific), and Melon-headed whales (scientific) within the Hawaii EEZ in 2010 and 2017, mirroring the analysis carried out in Bradford et al. (2021). Inputs fit_filters The fit_filters input specifies how to filter the data before fitting the detection function. It accepts a named list, which in our example will look like this: fit_filters = list(spp = c(&#39;013&#39;, &#39;026&#39;, &#39;031&#39;), pool = &#39;Multi-species pool 1&#39;, cohort = &#39;most&#39;, truncation_distance = 5, other_species = &#39;remove&#39;, years = 1986:2017, regions = NULL, not_regions = NULL) spp: A character vector of species codes. Using multiple species codes may be useful when you have low sample sizes for a cohort of similar species. cohort: The cohort containing these species, provided as a number indicating which slot in cruz$cohorts should be referenced. truncation_distance: The truncation distance to apply during model fitting. The remaining inputs are optional (i.e., they all have defaults): pool: A character string, providing a title for this species pool. If not specified, the species codes used will be concatenated to produce a title automatically. other_species: A character vector with four recognized values: If \"apply\" (the default if not specified), the species code will be changed to \"Other\" for sightings in which the species was in a mixed-species school but was not the species with the largest percentage of the total school size. In those cases, the species was not as relevant to the detection of the school as the other species were, which may bias the detection function. This creates a factor level for the detection function to use (when \"species\" is a covariate) to distinguish between cue-relevant species that are within the specified pool and those that are not. The second option for other_species is \"ignore\", which does not reassign species codes to \"Other\", and ignores whether the species of interest held the plurality for a mixed species detection. The third option is \"remove\": any species re-assigned to \"Other\" will be removed before the detection function is fit; this can be useful if only a small number of species are re-assigned to \"Other\", which would then obviate species as a viable covariate (since the sample size of all species levels would be unlikely to exceed df_settings$covariates_n_per_level – see below). The fourth and final option is coerce, which forces all species codes to \"Other\" for the purposes of detection function fitting and abundance estimation. This can be useful if you want to toggle the use of species as a covariate for a specific species pool, and/or produce abundance estimates for unidentified taxa (e.g., an ‘Unidentified dolphins’ species pool that includes multiple species codes). years: A numeric vector of years, used to filter data to include only effort/sightings from these years. regions: A character vector of geostratum names, used to filter the data. Any segment or sighting occurring within any (but not necessarily all) of the provided regions will be returned. This holds true for nested regions: for example, in analyses from the Central North Pacific, in which the Hawaii EEZ geostratum (\"HI-EEZ\") is nested within the larger geostratum representing the entire CNP study area (\"OtherCNP\"), an input of regions = \"OtherCNP\" will return segments/sightings both inside the Hawaii EEZ and outside of it. not_regions: A character vector of geostratum names, similar to above. Any segment or sighting occurring within any of these not_regions will not be returned. Using the example above, if regions = \"OtherCNP\" and not_regions = \"HI-EEZ\", only segments occuring within OtherCNP and outside of HI-EEZ will be returned. This can be particularly useful for abundance estimates for pelagic stock that exclude nested insular stocks. Note that, generally, filters such as years, regions, and not_regions are less stringent for detection function fitting than they are for density/abundance estimation, since low sample size is typically an issue. cruises: Filter data to only certain cruises, using a numeric vector of cruise numbers. If NULL, this will be ignored. not_cruises: Filter out certain cruises from the data, using a numeric vector of cruise numbers. If NULL, this will be ignored. df_settings The df_settings input specifies how to fit a detection function to the filtered data. It accepts a named list, which in our example will look like this: df_settings = list(covariates = c(&#39;bft&#39;,&#39;lnsstot&#39;,&#39;cruise&#39;,&#39;year&#39;,&#39;ship&#39;,&#39;species&#39;), covariates_factor = c(FALSE, FALSE, TRUE, TRUE, TRUE, TRUE), covariates_levels = 2, covariates_n_per_level = 10, detection_function_base = &#39;hn&#39;, base_model = &#39;~1&#39;, delta_aic = 2) (Note that all of these inputs have defaults.) covariates Covariates you wish to include as candidates in detection function models, provided as a character vector. The covariates must match columns existing within cruz$cohorts$&lt;cohort_name&gt;$sightings. #’ Note that the function will ignore case, coercing all covariates to lowercase. covariates_factor A Boolean vector, which must be the same length as covariates, indicating whether each covariate should be treated as a factor instead of a numeric. covariates_levels The minimum number of levels a factor covariate must have in order to be included as an eligible covariate. covariates_n_per_level The minimum number of observations within each level of a factor covariate. If this condition is not met, the covariate is excluded from the candidates. detection_function_base The base key for the detection function, provided as a character vector. Accepted values are \"hn\" (half-normal key, the default, which exhibit greater stability when fitting to cetacean survey data; Gerrogette and Forcada 2005), \"hr\" (hazard-rate), or c(\"hn\", \"hr), which will loop through both keys and attempt model fitting. base_model The initial model formula, upon which to build using candidate covariates. If not provided by the user, the default is \"~ 1\". delta_aic The AIC difference between the model yielding the lowest AIC and other candidate models, used to define the best-fitting models. Typically, AIC differences of less than 2 (the default) indicate effectively equal model performance. If this value is not zero, then model averaging will be done: if multiple models are within delta_aic of the model with the lowest AIC, all “best” models will be used in subsequent steps and their results will be averaged. See Details below. estimates The estimates input specifies which estimates of density and abundance to produce based on the fitted detection function. This input accepts a list of named sub-lists, which in our example will look like this: estimates = list( list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, g0 = .33, g0_cv = 0.20, years = 2010, regions = &#39;HI-EEZ&#39;), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, g0 = .32, g0_cv = 0.21, years = 2017, regions = &#39;HI-EEZ&#39;), list(spp = &#39;026&#39;, title = &#39;Frasers dolphin&#39;, g0 = .33, g0_cv = 0.20, years = 2010, regions = &#39;HI-EEZ&#39;), list(spp = &#39;026&#39;, title = &#39;Frasers dolphin&#39;, g0 = .32, g0_cv = 0.21, years = 2017, regions = &#39;HI-EEZ&#39;), list(spp = &#39;026&#39;, title = &#39;Melon-headed whale&#39;, g0 = .33, g0_cv = 0.20, years = 2010, regions = &#39;HI-EEZ&#39;), list(spp = &#39;026&#39;, title = &#39;Melon-headed whale&#39;, g0 = .32, g0_cv = 0.21, years = 2017, regions = &#39;HI-EEZ&#39;)) Each of these sub-lists specify the details for a single estimate of density/abundance, making it possible to produce multiple estimates from the same detection function model. Generally, there needs to be a sub-list for each species-region-year combination of interest, but there are options (see below) that allow for specifying multiple estimates with a single sub-list. Each of these sub-lists accepts the following named slots: spp: A character vector of species codes. If estimates is a non-nested list and spp is NULL, the codes in fit_filters$spp will be used. If estimates is a list of lists, spp must be specified in each nested list. title: A title for this abundance estimate, given as a character vector, ’ e.g., \"Striped dolphin - pelagic\". If left blank, the species code(s) will be concatenated to use as a title. Note that, if spp_method (below) is 'each', then title must be the same length as spp. spp_method: A character vector; if \"each\" (the default if not specified), density/abundance will be estimated for each species code in spp separately; if \"pool\", species will be pooled and a single estimate will be produced. The latter may be useful if you wish to combine a specific species code (e.g., \"075\", blue whale) with an uncertain one (e.g., \"079\", unidentified large whale). If c(\"each\", \"pool\") is provided, each species will be estimated separately, then a new estimate will be produced for all species pooled. g0: A numeric vector of length 2: the g(0) for small and large groups. Typically, this value is drawn from previously published tables, such as Barlow (2015), but you may estimate it yourself using the LTabundR functions g0_bft_model() and g0_weighted_var(). Note that each list can only accept a single pair of g0 estimates (the first for small schools below g0_threshold, the second for large schools above that threshold), which will be applied to all species in that list. g0_cv: A numeric vector of length 2: the coefficient of variation in g(0) for small and large groups. This value can also be drawn from previously published tables (e.g., Barlow 2015), but can also be modeled using recently developed Monte Carlo methods (Moore and Barlow 2017); you may also estimate it yourself using the LTabundR functions g0_bft_model() and g0_weighted_var(). g0_threshold: The school size threshold between small and large groups. years: A numeric vector of years, used to filter data to include only effort/sightings from these years. years_method: A character vector; if \"each\" (the default if not specified), density/abundance will be estimated separately for each year represented in the data; if \"pool\", years will be pooled and a single estimate will be produced. If c(\"each\", \"pool\") is provided, each year will be estimated separately, then a new estimate will be produced for all years pooled. regions: A character vector of geostratum names, used to filter the data. #’ Any segment or sighting occurring within any (but not necessarily all) of the provided regions will be returned. This holds true for nested regions: for example, in analyses from the Central North Pacific, in which the Hawaii EEZ geostratum (\"HI-EEZ\") is nested within the larger geostratum representing the entire CNP study area (\"OtherCNP\"), an input of regions = \"OtherCNP\" will return segments/sightings both inside the Hawaii EEZ and outside of it. regions_method: A character vector; if \"each\" (the default if not specified), density/abundance will be estimated separately for each region represented in the data; if \"pool\", regions will be pooled and a single estimate will be produced. In this case, then the area slot must be used to specify an area; if not provided, density but not abundance will be estimated. If c(\"each\", \"pool\") is provided, each region will be estimated separately, then a new estimate will be produced for all regions pooled. regions_remove: A character vector of geostratum names, similar to above. Any segment or sighting occurring within any of these not_regions will not be returned. Using the example above, if regions = \"OtherCNP\" and not_regions = \"HI-EEZ\", only segments occuring within OtherCNP and outside of HI-EEZ will be returned. This can be particularly useful for abundance estimates for pelagic stock that exclude nested insular stocks. Note that if this argument is specified, you will need to specify an area argument (below) in order to obtain an abundance estimate in addition to a density estimate. forced_effort: If this is a single numeric value instead of NULL (NULL is the default), this value will be used as the survey effort, in km, in a brute-force method; this same value will be used for every year and region. This is only helpful if you are looking for a relatively easy way to compare results from your own analysis to another (e.g., comparing LTabundR results to reports from NOAA reports prior to 2021, in which effort was calculated slightly differently). area: A numeric indicating the area, in square km, of the pooled region. This must be provided in order to get an abundance estimate (instead of just a density estimate) if regions_method contains \"pool\", or if regions_remove is specified and not NULL. Note that none of these inputs is actually required; all have defaults, which essentially copy from the inputs from fit_filters(). All the _method arguments have the default \"each\". Variance estimation lta(cruz, fit_filters, df_settings, estimates, bootstraps = 1000) Other inputs lta(cruz, fit_filters, df_settings, estimates, use_g0 = TRUE, ss_correction = 1, bootstraps = 10 toplot = TRUE, verbose = TRUE,) use_g0: A Boolean, with default TRUE, indicating whether or not to use custom g(0) value(s). If FALSE, the assumed g(0) value will be 1. ss_correction: Should a correction be applied to school sizes? School sizes will be scaled by this number. The default, 1, means no changes will occur. toplot: A Boolean, with default TRUE, indiciating whether detection function plots (Distance::plot.ds()) should be displayed as the candidate models are tested. verbose: A Boolean, with default TRUE, indicating whether or not updates should be printed to the Console. lta() output During processing While llta() is running, it will print things to the Console (if verbose is TRUE) and plot detection function fits (if toplot is TRUE). To demonstrate this, we will run the estimate for striped dolphins in 2010 only: # Setup inputs: fit_filters = list(spp = c(&#39;013&#39;, &#39;026&#39;, &#39;031&#39;), pool = &#39;Multi-species pool 1&#39;, cohort = &#39;most&#39;, truncation_distance = 5, other_species = &#39;remove&#39;, years = 1986:2017, regions = NULL, not_regions = NULL) df_settings = list(covariates = c(&#39;bft&#39;,&#39;lnsstot&#39;,&#39;cruise&#39;,&#39;year&#39;,&#39;ship&#39;,&#39;species&#39;), covariates_factor = c(FALSE, FALSE, TRUE, TRUE, TRUE, TRUE), covariates_levels = 2, covariates_n_per_level = 10, detection_function_base = &#39;hn&#39;, base_model = &#39;~1&#39;, delta_aic = 2) estimates = list( list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, g0 = .32, g0_cv = 0.21, years = 2017, regions = &#39;HI-EEZ&#39;)) # Run it: demo &lt;- lta(cruz, fit_filters, df_settings, estimates) Outputs The lta() function returns a list of objects. To demonstrate this output, we will pull in the built-in dataset representing the result of the analysis above, for all three species in both years (with 5 bootstrap iterations): data(&#39;lta_result&#39;) names(lta_result) [1] &quot;pool&quot; &quot;inputs&quot; &quot;estimate&quot; &quot;df&quot; &quot;bootstrap&quot; pool: The species pool pertaining to these estimates. inputs: A list of the inputs used to produce these estimates. estimate: A table of density/abundance estimates for each species/region/year combination specified in the estimates input. demo$estimate title species Region Area year segments km Area_covered 1 Striped dolphin 013 HI-EEZ 2454639 2017 140 17260.92 69221.41 ESW_mean n g0_est ER_clusters D_clusters N_clusters size_mean size_sd 1 4.010296 16 0.32 0.0009269493 0.0003706212 909.7414 36.85783 17.52562 ER D N 1 0.03416534 0.01311434 32190.98 df: A named list with details for the detection function. demo$df %&gt;% names [1] &quot;best_models&quot; &quot;all_models&quot; &quot;best_objects&quot; &quot;sightings&quot; &quot;sample_size&quot; demo$df$best_models Model Key_function Formula Pmean AIC 1 9 hn ~1 + ship + bft + lnsstot 0.5576547 899.071 2 11 hn ~1 + ship + bft + lnsstot + species 0.5506719 899.998 $\\\\Delta$AIC Covariates tested pool 1 0.000 bft, lnsstot, ship, species Multi-species pool 1 2 0.927 bft, lnsstot, ship, species Multi-species pool 1 bootstrap: If bootstrap variance estimation was carried out, the output would also include bootstrap, a named list with results from the bootstrap process, only returned if the bootstraps input is greater than 1. demo$bootstrap %&gt;% names NULL demo$bootstrap$summary NULL Summary tables To summarize lta() results using the standard table format provided in recent NOAA stock assessment reports, use the function lta_report(). tables &lt;- lta_report(lta_result, verbose = TRUE) tables %&gt;% names [1] &quot;table2&quot; &quot;table3&quot; &quot;table4&quot; Table 2 in reports: Sample sizes tables$table2 %&gt;% DT::datatable(options=list(initComplete = htmlwidgets::JS( &quot;function(settings, json) {$(this.api().table().container()).css({&#39;font-size&#39;: &#39;9pt&#39;});}&quot;) )) Table 3: Parameter estimates tables$table3 %&gt;% DT::datatable(options=list(initComplete = htmlwidgets::JS( &quot;function(settings, json) {$(this.api().table().container()).css({&#39;font-size&#39;: &#39;9pt&#39;});}&quot;) )) Table 4: Density/abundance tables$table4 %&gt;% DT::datatable(options=list(initComplete = htmlwidgets::JS( &quot;function(settings, json) {$(this.api().table().container()).css({&#39;font-size&#39;: &#39;9pt&#39;});}&quot;) )) Behind the scenes Covariates in detection function estimation Before detection functions are modelled, any covariates supplied by the user and specified as a factor are first tested for eligibility. Only factors with at least two levels (or whatever you specified with df_settings$covariates_levels) and 10 observations in each level (or whatever you specified with df_settings$covariates_n_per_level) are eligible for inclusion. Fitting a detection function The detection function is estimated using functions in the package mrds, primarily the main function mrds::ddf(), which uses a Horvitz-Thompson-like estimator to predict the probability of detection for each sighting. If multiple base key functions (e.g., half-normal or hazard-rate) are provided, and/or if covariates are specified, model fitting is done in a forward stepwise procedure: In the first round, the base model (no covariates, i.e., \"~1\") is fit first. In the second round, each covariate is added one at a time; at the end of the round, the covariate, if any, that produces the lowest AIC below the AIC from the previous round is added to the formula. This process is repeated in subsequent rounds, adding a new covariate term in each round, until the AIC no longer improves. If a second base key is provided, the process is repeated for that second key. All models within delta_aic of the model with the lowest AIC qualify as best-fitting models. The best-fitting model(s) is(are) then used to estimate the Effective Strip half-Width (ESW) based on the covariates associated with each sighting. If multiple best-fitting models occur, we will find the average ESW for each sighting across all models, using a weighted mean approach in which we weight according to model AIC. To turn off this model averaging step, set delta_aic to 0 to avoid passing multiple models to the abundance estimation stage. This stage of the lta() command is executed within a backend function, LTabundR::fit_df(), which has its own documentation for your reference. Estimating density &amp; abundance Estimates are produced for various combinations of species, regions, and years, according to the arguments specified in your estimates list(s). Before these estimates are produced, we filter the data used to fit the detection function to strictly systematic (design-based) effort (i.e., EffType = \"S\"), in which standard protocols are in use (i.e., OnEffort = TRUE) and the Beaufort sea state is less than 7. This stage of the lta() command is executed within a back-end function, LTabundR::abundance(), which has its own documentation for your reference. Bootstrap variance estimation If the bootstraps input value is greater than 1, bootstrap variance estimation will be attempted. In each bootstrap iteration, survey segments are re-sampled with replacement before fitting the detection function and estimating density/abundance. Note that the entire process is repeated in each bootstrap: step-wise fitting of the detection function, averaging of the best-fitting models, and density/abundance estimation for all species/region/year combinations specified in your estimates input. At the end of the bootstrap process, results are summarized for each species/region/year combination. 95% confidence intervals are calculated using the BCA method (package coxed, function bca()). g(0) values during bootstrapping When conducting the non-parametric bootstrap routine to estimate the CV of density and abundance, uncertainty is incorporated into the g(0) value in each iteration using a parametric bootstrapping subroutine: First, a logit-transformed distribution is modeled based upon the mean and CV of g(0) provided by the user in the estimates input (see documentation for LTabundR::g0_optimize() for details on this step). This modeled distribution is used to randomly draw a g(0) value for each iteration of the density/abundance bootstrap routine. In this way, the uncertainty in g(0) is propagated into uncertainty in density/abundance. "],["multistock.html", " 8 Multi-stock LTA", " 8 Multi-stock LTA Coming soon! "],["g0.html", " 9 g(0)", " 9 g(0) Coming soon! "],["casestudies.html", " 10 Central North Pacific", " 10 Central North Pacific To demonstrate what we are building, below is template for quickly running all of the steps detailed in subsequent chapters. This template uses the same data and settings used throughout the vignette: library(swfscDAS) library(LTabundR) library(dplyr) # Settings ===================================================================== # Load strata dataframes data(strata_cnp) data(study_cnp) # Load group size coefficients data(group_size_coefficients) # Load ships data(ships) # Load species codes data(species_codes) # Survey settings survey &lt;- load_survey_settings(out_handling = &#39;stratum&#39;, max_row_interval = 700, segment_method = &#39;equallength&#39;, segment_target_km = 30, segment_max_interval = 48, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), ship_list = ships, species_codes = species_codes, group_size_coefficients = group_size_coefficients, smear_angles = FALSE, verbose = TRUE) # Cohort 1 (default) cohort1 &lt;- load_cohort_settings() # Cohort 2 cohort2 &lt;- load_cohort_settings(id=&#39;fkw_insular&#39;, species=33, use_low_if_na = TRUE, truncation_km = 5, strata_overlap_handling = &#39;each&#39;) # Finalize settings settings &lt;- load_settings(strata = strata_cnp, study_area = study_cnp, survey = survey, cohorts=list(cohort1, cohort2)) # Processing =================================================================== das_file &lt;- &#39;data/surveys/HICEASwinter2020.das&#39; cruz &lt;- process_surveys(das_file, settings, verbose=TRUE) # Data evaluation ============================================================== # Map m &lt;- map_base(&#39;cnp&#39;) m &lt;- map_strata(m, cruz$settings) m &lt;- map_effort(m, cruz) m &lt;- map_sightings(m, cruz, size_base=.4) m # Review interactively cruz_explorer(cruz) "],["california-current-system.html", " 11 California Current System", " 11 California Current System library(swfscDAS) library(LTabundR) library(dplyr) # Settings ===================================================================== # Load strata &amp; study area data(strata_ccs) # Survey-wide survey &lt;- load_survey_settings(segment_method = &#39;equallength&#39;, segment_target_km = 100, segment_max_km_gap = 5, segment_max_interval = 48, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), verbose = TRUE) # Cohort 1 (default) cohort1 &lt;- load_cohort_settings() # Cohort 2 cohort2 &lt;- load_cohort_settings(id=&#39;beakers&#39;, species=c(1,49,52:59, 61:65, 81:83, 106, 109), use_low_if_na = TRUE, distance_types = c(&#39;S&#39;,&#39;F&#39;,&#39;N&#39;)) # Finalize settings settings &lt;- load_settings(strata = strata_ccs, study_area = NULL, survey = survey, cohorts=list(cohort1, cohort2)) # Processing =================================================================== das_file &lt;- &#39;data/surveys/CAORWA_91-09.das&#39; cruz &lt;- process_surveys(das_file, settings, verbose=TRUE) # Data evaluation ============================================================== # Map m &lt;- map_base(&#39;ccs&#39;) m &lt;- map_strata(m, cruz$settings) m &lt;- map_effort(m, cruz) m &lt;- map_sightings(m, cruz, size_base=.4) m # Review interactively cruz_explorer(cruz) "],["eastern-tropical-pacific.html", " 12 Eastern Tropical Pacific", " 12 Eastern Tropical Pacific Coming soon! "],["practicalities.html", " 13 Practicalities", " 13 Practicalities Reproducibility &amp; project management One of the features of the ABUND framework we need to retain is that each analysis is self-contained and reproducible. The folder for an analysis must have all the files and data needed for someone else to be able to replicate it. In the R package framework we are developing, the contents of a project folder will be straightforward (once the code base is bundled into a package). Contents of a project folder: DAS file(s) with survey data Stratum and study area polygon(s), as csv(s) (optional) Group size calibration files (optional) An analysis.R file (or perhaps a .Rmd), containing an adaptation of the code provided in the template above. This script will contain project-specific settings (e.g., the load_settings() call), which allow for the script to be reproducible. "],["stratagallery.html", " 14 Strata gallery Central North Pacific California Current ETP", " 14 Strata gallery This packages comes with several built-in datasets of geographic strata that are commonly used in NOAA/NMFS surveys. The functions strata_explore() and strata_select() were developed to help you explore those built-in options. Central North Pacific strata_explore(&#39;cnp&#39;) To acquire the filepath to one of these strata, pass the index (or indices) printed in the map titles above to the function strata_select(): strata &lt;- strata_select(selections = c(1,3), region = &#39;cnp&#39;) This function returns a named list that can be passed directly to the strata argument in load_settings(). strata %&gt;% names [1] &quot;HI_EEZ&quot; &quot;OtherCNP&quot; The second slot, $paths, contains the filepaths you would need to pass to the load_settings() function. California Current strata_explore(&#39;ccs&#39;) ETP You can do the same for the Eastern Tropical Pacific (ETP); there are about 70 polygons built-in to LTabundR for this region. We won’t show them here! strata_explore(&#39;etp&#39;) "],["segmentizing.html", " 15 Segmentizing Approach Defaults Day vs Equal Length Contiguous vs. non-contiguous effort Segment remainder handling Typical settings", " 15 Segmentizing The package’s segmentize() function and its associated settings were designed to give researchers full control over how data are segmented, be it for design-based density analysis (which tend to use long segments of 100 km or more and allow for non-contiguous effort to be included in the same segment) or for habitat modeling (which tend to use short segments of 5 - 10 km and disallow non-contiguous effort to be pooled into the same segment). Approach Segments are built and stored separately for each cohort of species, since each cohort has specific settings for segmentizing. Within each cohort, the survey is first grouped into blocs of data that all share the same “effort scenario”, i.e., all rows share the same Cruise number, study area status (in or out), geographic stratum, and year. Since a survey may leave a stratum then return to it many days hence, it is normal for these blocs to contain non-contiguous data with large spatial gaps. These gaps will be addressed a few steps down. The blocs are split a final time according to whether the effort scenario meets inclusion criteria for the analysis. These inclusion criteria are controlled by the cohort-specific settings such as distance_types, distance_modes, and distance_on_off. Rows of data that meet the inclusion criteria are relegated to their own data bloc, and given a new column, use, with the value TRUE. Data that do not meet the criteria are relegated to their own bloc as well (column use is FALSE). This means that, at the end of this process, we will have segments that will be used in the density/detection function analysis, and segments that will not. (The excluded segments are not deleted or transformed in any other way; they can still be used in summarizing detections, etc.) Next, the segmentize() function loops through each of these blocs of effort and parses its data into segments according to the segment_method. If segmentizing by \"day\", this is straightforward: all data occurring on a unique date are assigned to its own segment. Segmentizing by \"equallength\" is a bit more complicated in terms of coding: segments are built up one line of data at a time; if the segment_target_km is reached or the segment_max_interval is exceeded, a new segment begins. At the end of this process, you have lists of data sorted into their segments, each with a unique seg_id, as well as a summary dataframe that provides the distance (km); time and coordinates for the beginning, middle, and end of the segment; and the weighted averages of sighting conditions and weather data contained in the segment. Setting up this demo The demonstration of segmentize() on in Processing chapter relies on the settings list that is attached as a slot in the cruz object. But you can override those settings with direct function inputs in segmentize(), which gives us a chance to explore segmentization options. First we load the demo data and carry out initial processing: data(settings) das_file &lt;- &#39;data/surveys/HICEASwinter2020.das&#39; das &lt;- load_das(das_file, perform_checks = FALSE, print_glimpse = FALSE) cruz &lt;- process_strata(das, settings, verbose=FALSE) cruz &lt;- format_das(cruz) And this is the histogram function we will be using to display the results of each run of segmentize(): segment_histogram &lt;- function(cruz, cohort=1, by_day=FALSE){ (settings &lt;- cruz$settings) (segs &lt;- cruz$cohorts[[cohort]]$segments) (segmax &lt;- max(segs$dist,na.rm=TRUE)*1.1) if(by_day){ main_use &lt;- paste0(&#39;Segments (use) | by day&#39;) main_exclude &lt;- paste0(&#39;Segments (exclude) | by day&#39;) }else{ (main_use &lt;- paste0(&#39;Segments (use) | target: &#39;,settings$survey$segment_target_km,&#39; km&#39;)) (main_exclude &lt;- paste0(&#39;Segments (exclude) | target: &#39;,settings$survey$segment_target_km,&#39; km&#39;)) } par(mfrow=c(1,2)) par(mar=c(4.2,4.2,2.5,.5)) hist(segs$dist[segs$use], breaks = seq(0,segmax,by=segmax/40), xlab=&#39;Segment lengths (km)&#39;, main=main_use, cex.main = .8, cex.axis = .8, cex.lab = .8) hist(segs$dist[!segs$use], breaks = seq(0,segmax,by=segmax/40), xlab=&#39;Segment lengths (km)&#39;, main=main_exclude, cex.main = .8, cex.axis = .8, cex.lab = .8) par(mfrow=c(1,1)) } Defaults Here is the segmentize() function parameterized with the “factory default” settings from load_settings(). cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 30, segment_max_interval = 48, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), distance_types = c(&#39;S&#39;,&#39;F&#39;,&#39;N&#39;), distance_modes = c(&#39;P&#39;,&#39;C&#39;), distance_on_off = c(TRUE), verbose=FALSE) # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow [1] 273 # Plot segment_histogram(cruz_demo) Day vs Equal Length By day cruz_demo &lt;- segmentize(cruz, segment_method = &#39;day&#39;, segment_target_km = 30, segment_max_interval = 48, verbose=FALSE) # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow [1] 113 # Plot segment_histogram(cruz_demo, by_day = TRUE) By target length of 150 km cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 150, segment_max_interval = 48, verbose=FALSE) # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow [1] 66 # Plot segment_histogram(cruz_demo, by_day = TRUE) Contiguous vs. non-contiguous effort The example above allows for non-contiguous effort; a segment is allowed to contain effort separated by gaps as large as 24 hours (settings$max_interval). To coerce segments to represent only contiguous effort, make that setting very small: cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 30, segment_max_interval = .1, verbose=FALSE) # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow [1] 567 # Plot segment_histogram(cruz_demo, by_day = TRUE) You can see that many contiguous periods of effort were much shorter than the target length of 30 km. This is why allowing for non-contiguous effort can be advantageous for target segment lengths larger than 5 - 10 km. Segment remainder handling The default setting for segment_remainder_handling, c('append','segment'), means that remainders less than half the target length will be randomly appended to another segment, while remainders more than half will be treated as their own segment (and will be placed randomly along the trackline). If you don’t want that level of complexity, you can simply assign a single setting: 'append' will append the remainder in all cases, regardless of remainder length relative to the target length. The same idea goes for 'segment'. The other possible setting is 'disperse', which disperses the remainder evenly across all segments. To demonstrate, let’s use a target length of 100 km. cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 100, segment_max_interval = 48, verbose=FALSE) # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow [1] 90 # Plot segment_histogram(cruz_demo, by_day = TRUE) Note that most segments are longer than the target length, due to the impact of dispersing the remainder. If you wanted, you could combat this by making the target length slightly smaller: cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 90, segment_max_interval = 48, verbose=FALSE) # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow [1] 98 # Plot segment_histogram(cruz_demo, by_day = TRUE) But in general, the disperse option may be more appropriate for shorter segment lengths. Typical settings Design-based line transect analysis To replicate methods used in density estimation analyses, use large segment lengths (100 km or more) or simply segmentize by day. (See the examples above.) Remember that long segment lengths won’t work well unless you allow for non-contiguous effort. Habitat modeling To replicate the methods used in typical habitat modeling studies, use smaller segment lengths of contiguous effort. cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 5, segment_max_interval = .1, verbose=FALSE) # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow [1] 1728 # Plot segment_histogram(cruz_demo, by_day = TRUE) "],["abund9_compare.html", " 16 LTabundR vs. ABUND9 Differences in total effort Differences in on-effort distance Differences it total sightings Differences in on-effort sightings Differences in school size estimation Processing time", " 16 LTabundR vs. ABUND9 We have tried to develop LTabundR with the flexibility either to replicate ABUND results or to produce customizable results that could potentially vary from ABUND quite significantly (e.g., formatted for habitat modeling). However, even when we use LTabundR settings intended to replicate ABUND results, there are likely to be some small differences. These are detailed below: Differences in total effort After loading the data, LTabundR removes rows with invalid Cruise numbers, invalid times, and invalid coordinates. As far as we can tell, ABUND does not remove such missing data. This is a relatively minor point; in processing the 1986-2020 data (623,640 rows), 287 rows are missing Cruise info; 1,430 are missing valid times; and 556 are missing valid coordinates, for a total of 2,273 rows removed out of more than 625,000 (0.3% of rows). The custom ABUND functions that calculate whether DAS coordinates occur within geostrata are difficult to validate, and it is possible that they differ from the functions used in R for the same purpose. Both ABUND and LTabundR calculate the distance surveyed based on the sum of distances between adjacent rows in the DAS file. They do this differently, which could yield minor differences in segment track lengths. ABUND loops through the data one row at a time, calculating distance traveled at the same time as allocating effort to segments and processing sightings. It calculates the distance between each new row and the beginning of a segment of effort. That beginning location (object BEGTIME in the Fortran code) is reset with various triggers (including a new date), and the distance traveled is calculated using a subroutine (DISTRAV). For surveys occurring after 1991, the distance between a new coordinate and the BEGTIME coordinate is calculated using a subroutine named GRCIRC (great-circle distance). Prior to 1991, the ship speed and the time since BEGTIME is used to estimate distance traveled. After 1991, the function calculates distance based on coordinates. For all years, the distance calculation only happens if the time gap in time is at least 1.2 minutes (line 405 in ABUND9.FOR), otherwise the distance is returned as 0 km. This function also seems to allow for large gaps between subsequent rows within a single day of effort. The subroutine prints a warning message when the gap is greater than 30 km, but does not modify its estimate of distance traveled. This allows for the possibility that, in rare cases, estimates of distance surveyed will be spuriously large. LTabundR processes data using a modular approach rather than a single large loop. Prior to the segmentizing stage, it calculates the distance between rows of data. Its approach is to calculate the distance between each row and its subsequent row (it does so using the swfscDAS function distance_greatcircle(), which is a nearly-exact recode of GRCIRC for R. There are two important differences that LTabundR applies: (1) In anticipation of WinCruz surveys that operate on much smaller scales with more frequent position updates, we calculate distances for time gaps as small as 30 seconds, not 1.2 minutes. This may generate minor differences in the total length of tracks; (2) If the distance between rows is greater than 30 km, then it is assumed that effort has stopped and the distance is changed to 0 km. This approach should avoid the misinterpretation of large gaps in effort as large periods of effort. Differences in on-effort distance LTabundR works with DAS data that are loaded and formatted using swfscDAS:das_read() and das_process(). It is possible that these functions categorize events as On- or Off-Effort slightly differently than ABUND, or apply other differences that would be difficult for us to know or track. While ABUND uses a minimum length threshold to create segments, such that full-length segments are never less than that threshold and small remainder segments always occur at the end of a continuous period of effort, LTabundR uses an approach more similar to the effort-chopping functions in swfscDAS: it looks at continuous blocs of effort, determines how many full-length segments can be defined in each bloc, then randomly places the remainder within that bloc according to a set of user-defined settings. This process produces full-length segments whose distribution of exact lengths is centered about the target length, rather than always being greater than the target length. To control the particularities of segmentizing, LTabundR uses settings such as segment_max_interval, which controls how discontinuous effort is allowed to be pooled into the same segment. These rules may produce slight differences in segment lengths. Note that, since ABUND is a loop-based routine while LTabundR is modular, segments identified by the two program will never be exactly identical, and a 1:1 comparison of segments produced by the two programs is not possible. Differences it total sightings In ABUND9, only sightings that occur while OnEffort == TRUE are returned; in contrast, LTabundR does not remove any sightings (it just flags them differently, using the included variable). But we can easily filter LTabundR sightings to emulate ABUND9 output. Differences in on-effort sightings LTabundR includes an additional criterion for inclusion in analysis: the sighting must occur at or forward of the beam (this can be deactivated with a survey setting). Since geostratum handling is different in the two programs, it is possible that sightings occurring near stratum margins may be included/excluded differently. Differences in school size estimation If an observer is not included in the Group Size Calibration Coefficients .DAT file, ABUND applies a default coefficient (0.8625) to scale group size estimates; however, it applies this calibration to group sizes of all sizes, including solo animals or small groups of 2-3. In LTabundR, we restrict calibrations for unknown observers to group size estimates of 4 or above. This means that school size estimates for small schools will be slightly larger, on average (by about 13%), in ABUND. Note that ABUND9 calibrates school sizes differently than ABUND7. The ABUND9 release notes mention a bug in previous versions that incorrectly calibrated school size. LTabundR corresponds perfectly with ABUND9 school size calibrations, but not with ABUND8 or earlier. Processing time LTabundR is about 6x slower than ABUND (e.g., for CNP 1986 - 2020 surveys, ABUND processes in 50 seconds). LTabundR processes in 5:00 minutes). This lag is due to the facts that… R is generally slower than Fortran LTabundR runs swfscDAS functions first, then processes those outputs LTabundR is designed to provide detailed outputs, to be more versatile in both LTA and habitat modeling LTabundR is designed to be more easily troubleshooted, which increases processing time. "],["misc_functions.html", " 17 Miscellaneous functions", " 17 Miscellaneous functions species_translator() To streamline the management of species codes, scientific names, common names, etc., in the functions throughout this package, we have developed a “translator” function that returns the various identifiers for a species according to a variety of search terms. You can search by species code: # source(&#39;R/species_translator.R`) species_translator(id = &#39;035&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common_name1 &quot;Long-finned pilot whale&quot; common_name2 &quot; Atlantic pilot whale&quot; common_name3 &quot; blackfish&quot; common_name4 &quot; pothead&quot; By the short code name: species_translator(id = &#39;LONG_PILOT&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common_name1 &quot;Long-finned pilot whale&quot; common_name2 &quot; Atlantic pilot whale&quot; common_name3 &quot; blackfish&quot; common_name4 &quot; pothead&quot; By the scientific name: species_translator(id = &#39;Globicephala melas&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common_name1 &quot;Long-finned pilot whale&quot; common_name2 &quot; Atlantic pilot whale&quot; common_name3 &quot; blackfish&quot; common_name4 &quot; pothead&quot; Or by one of the species’ common names: species_translator(id = &#39;Long-finned pilot whale&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common_name1 &quot;Long-finned pilot whale&quot; common_name2 &quot; Atlantic pilot whale&quot; common_name3 &quot; blackfish&quot; common_name4 &quot; pothead&quot; The function will return any species for which there is a partial match: species_translator(id = &#39;megap&#39;) %&gt;% t() 76 code &quot;076&quot; short_name &quot;HUMPBACK_W&quot; scientific_name &quot;Megaptera novaeangliae&quot; common_name1 &quot;Humpback whale&quot; common_name2 &quot;&quot; common_name3 &quot;&quot; common_name4 &quot;&quot; species_translator(id = &#39;killer&#39;) code short_name scientific_name common_name1 32 032 PYGMY_KLLR Feresa attenuata Pygmy killer whale 33 033 FALSE_KLLR Pseudorca crassidens False killer whale 37 037 KILLER_WHA Orcinus orca Killer whale 110 110 Orcinus orca Transient killer whale 111 111 Orcinus orca Resident killer whale 112 112 Orcinus orca Offshore killer whale 113 113 Orcinus orca Type A Antarctic killer whale 114 114 Orcinus orca Type B Antarctic killer whale 115 115 Orcinus orca Type C Antarctic killer whale common_name2 common_name3 common_name4 32 slender blackfish 33 37 110 111 112 113 114 115 Note that if species_codes is NULL, as in the examples above, the list of codes used in ABUND9 will be used as a default. "],["backlog.html", " 18 Appendix: Backlog", " 18 Appendix: Backlog Remove random.seed from settings functions. Investigate discrepancies between output from format_distance() and summarize_effort(). Follow-through on converting format_distance() to match the formatting from ABUND9? (It is currently matching the format from ABUND7). Fix polygon area estimation! (Currently does not subtract land area! Ack!) Should LTabundR accept Bft values of NA? Or include an option for interpolating replacement values based on the preceding and proceding values? Or just leave it to analysts to correct the DAS data? Consider making the minimum group size for group size calibration (for unknown observers) a cohort-specific setting that users can define. Update all package and vignette documentation accordingly. Consider making the km gap handling arguments for format_das() (and its subroutine process_km()) into survey settings that users can define. Update all package and vignette documentation accordingly. Add options to das_load() that allows you to determine timezone from date and lat/long, then adjust DateTime accordingly. This would avoid errors in the manual specification of GMT offset within the DAS data. Jay uses the phrase ‘stratify by IO’: add segmentizing feature in which segment is reset if IO status changes. Main Islands map bbox Map bbox for SoCal, Central Cal, and northern CCS Figure out which ETP strata to have as defaults for both survey strata and study area polygons (there are like 100 in the folder Jeff sent us). May be a question for Tim G. Test this code on lots of cruise data files to establish basic functionality for Hawaii, California Current, and ETP. Troubleshoot why longitudes are not displayed on Hawaii base map Accommodate subgroup settings for FKW analyses Method for stratifying by Beaufort. study_area_explore() and study_area_select(). LTabundR is finding more effort in Cruise 1642 than is ABUND. See the slides detailing the ABUND v LTabundR comparison. swfscDAS can accidentally exclude sightings when the GMTOffset field is incorrect; this can be addressed by (i) correcting the DAS file so that GMToffset is correct, (ii) modifying the swfscDAS code, or (iii) building a function for LTabundR that tests and corrects the GMToffset field based upon lat/long and the date. This currently affects only two sightings in Cruise 1004. When observer positions are not entered correctly, sightings will be excluded from analysis. For example, when a sighting is manually entered without correctly establishing observer positions in a previous DAS line, the swfscDAS functions will put NA’s in the ObsL and ObsR data fields. This currently affects only one sighting in Cruise 1607. This can be addressed by (i) leaving as is, (ii) assuming invalid ObsL and ObsR are allowed to be included, or (iii) developing a LTabundR feature in which NA values are replaced based on preceding/subsequent values. When the Bft value is missing, LTabundR does not include the sighting in analysis. This can be addressed by (i) leaving as is (if the DAS data are incorrect, LTabundR will not fix it); (ii) assuming all Bft == NA are valid; or by (iii) developing a LTabundR feature in which NA values are replaced based on preceding/subsequent values. This currently affects two sightings (one in cruise 1607, one in cruise 1621). When sightings occur exactly upon the boundary of a geostratum, LTabundR considers it ‘out’ and excludes it from analysis. ABUND, in contrast, considers it in. This affects one sighting in cruise 1080. We have put an item in the backlog to consider changing this feature. ABUND has some TotSS estimates that are less than 1 (n= about 50). Most of these sightings have ‘raw’ SS estimates of 1 and at least 1 observer(s) who is in the SS calibration DAT file, which means their calibration must have produced an estimate less than 1. It makes sense to me that LTabundR should implement a minimum SS of 1 for each observer’s estimate, which should be applied after calibration. In the ABUND SSCAL routine, when an observer’s coefficients are drawn from the DAT file, the first line of best-high-low weights is used regardless of whether the general model or the year-specific model is used. Is that how it should be? For a few observers (only a few), the second row of weights differs from the first row. The ABUND SSCAL routine assumes Bft == 0 if it is NA. Are we OK with assuming that, or should we bypass calibration in the event of a missing Bft value? There is just 1 sighting with Bft == NA (Cruise 1607, 1997-04-27 07:16:01, SightNo 68, SpCode 037 killer whale). "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
