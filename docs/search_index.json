[["index.html", "User’s guide for LTabundR Overview", " User’s guide for LTabundR Overview The R package LTabundR offers tools that facilitate and standardize design-based line-transect abundance estimation of cetaceans, based on typical workflow used following NOAA Fisheries ship surveys in the central and eastern Pacific (e.g., Barlow 2006, Barlow and Forney 2007, Bradford et al. 2017, Bradford et al. 2021). That workflow typically involves four stages: (1) Data processing This step involves reading in &amp; processing raw DAS files (the files produced by the software Wincruz commonly used during NOAA Fisheries line-transect surveys in the Pacific), breaking effort into discrete segments for variance estimation, correcting group size estimates according to calibration models, and then averaging together group size estimates for each sighting. Most importantly, this step standardizes the data structure in a way that all downstream analyses depend upon. The name we will use for this standardized data object is a cruz object. The key LTabundR functions you will use in this stage are load_settings() and process_surveys(). (2) Data exploration This step involves summarizing effort and sightings totals, determining the appropriate truncation distances for each species – or pool of species – based on sample sizes, and producing maps. The key LTabundR functions in this stage are cruz_explorer() (a Shiny dashboard for data exploration) and the summarize... functions, such as summarize_species() and summarize_effort(). (3) Data analysis This step involves estimating Beaufort-specific “relative” trackline detection probabilities – i.e., g(0) estimates; estimating density/abundance with detection functions and determining the CV of that estimate; handling stratified analyses; and evaluating if random variation in the encounter rate of a species may be driving differences in abundance estimates over time. The key LTabundR functions in this stage are g0_table() and lta(). Most analyses are group-based analyses, but false killer whales (Pseudorca crassidens) are analyzed differently using a subgroup-based approach. For this exception, the function lta_subgroup() will be used. (4) Reports &amp; plots This step produces summary tables of the processed data and line-transect estimates; plots the best-fitting detection function model(s); and plots species-specific abundance estimates (and their CV). They key LTabundR functions in this stage are lta_report(), df_plot(), and lta_plot(). This user’s guide is structured around these four workflow stages. Those pages are followed by a case study with full-fledged example code. The guide concludes with appendices that offer further details and minutiae on certain aspects of the package. Throughout this user’s guide, we will primarily be using example data from the winter Hawaiian Islands Cetacean Ecosystem and Assessment Survey (WHICEAS) of 2020, along with the summer-fall HICEAS 2017 data collected within the WHICEAS study area. Installation Note: BETA testing only. This package is currently in beta testing and is not yet ready for widespread use. The LTabundR package is available on GitHub here. To install directly within R, use the following code: # Install support packages, if needed if (!require(&#39;devtools&#39;)) install.packages(&#39;devtools&#39;) # Increase timeout for download, since there are datasets options(timeout=9999999) # Install LTabundR remotely from GitHub devtools::install_github(&#39;PIFSC-Protected-Species-Division/LTabundR&#39;) You may need to download “Rtools” if it is not already installed on your computer. You can do so here. # Load into session library(LTabundR) # Check package version utils::packageVersion(&#39;LTabundR&#39;) [1] &#39;0.4.4&#39; Note that this package contains large built-in datasets and may take several minutes to install. Credits This R package is a consolidation of code developed over many years by many NOAA Fisheries scientists, primarily Jay Barlow, Tim Gerrodette, Jeff Laake, Karin Forney, Amanda Bradford, and Jeff Moore. This package was developed by Eric Keen and Amanda Bradford with support from the NOAA Fisheries National Protected Species Toolbox Initiative. "],["settings.html", " 1 Settings Survey strata Survey-wide settings Cohort-specific settings Example code", " 1 Settings To customize the way data are processed and included in your analysis, use the load_settings() function. This function emulates and expands upon the settings file, ABUND.INP, that was used to run ABUND7/9 in FORTRAN. ABUND7/9 was a program written by Jay Barlow (NOAA Fisheries) to process DAS files. This function allows you to use ‘factory defaults’ if you don’t wish to specify anything special such as strata or study area polygons: settings &lt;- load_settings() If you do not want to use all the defaults, you can provide load_settings() with custom inputs. The function accepts three arguments: strata: dataframe(s) of coordinates survey: settings that will apply universally to the analysis cohorts: settings that are specific to groups of species. By providing cohort-specific settings, the code for a single analysis becomes simpler and more easily reproduced, since the code only needs to be run once without modification. The output of load_settings() is a named list with a slot for each of these arguments: settings %&gt;% names [1] &quot;strata&quot; &quot;survey&quot; &quot;cohorts&quot; Survey strata Stratum polygons can be provided as a named list of data.frame objects. Each data.frame must have Lon and Lat as the first two columns, providing coordinates in decimal degrees in which West and South coordinates are negative. Other columns are allowed, but the first two need to be Lon and Lat. The name of the slot holding the data.frame will be used as a reference name for the stratum. It is acceptable if vertices in the eastern hemisphere are described using negative longitudes below -180, e.g., -185. (LTabundR will correct these to proper decimal degrees, e.g., -185 will become 175.) If strata is NULL, abundance will not be estimated; only density within the searched area (i.e., the total segment length x effective strip width). While users are welcome to upload polygons of their own, the package comes with built-in polygons for strata that are commonly used in three NOAA Fisheries study regions in the central and eastern Pacific: the Central North Pacific (CNP, including Hawaii) … data(strata_cnp) names(strata_cnp) [1] &quot;HI_EEZ&quot; &quot;OtherCNP&quot; &quot;MHI&quot; &quot;WHICEAS&quot; [5] &quot;Spotted_OU&quot; &quot;Spotted_FI&quot; &quot;Spotted_BI&quot; &quot;Bottlenose_KaNi&quot; [9] &quot;Bottlenose_OUFI&quot; &quot;Bottlenose_BI&quot; &quot;NWHI&quot; # HI-EEZ: U.S. Exclusive Economic Zone around the Hawaiian Islands. # OtherCNP: Extended Central North Pacific study area. # MHI: Main Hawaiian Islands survey stratum during HICEAS 2002. # WHICEAS: Study area for winter HICEAS of 2020. # Spotted_OU: Stock boundary for the Oahu population of spotted dolphins. # Spotted_FI: Stock boundary for the 4-Islands population of spotted dolphins. # Spotted_BI: Stock boundary for the Hawaii Island population of spotted dolphins. # Bottlenose_KaNi: Stock boundary for the Kauai/Niihau population of bottlenose dolphins. # Bottlenose_OUFI: Collective stock boundaries for the adjacent Oahu and 4-Islands populations of bottlenose dolphins. # Bottlenose_BI: Stock boundary for the Hawaii Island population of bottlenose dolphins. # NWHI: Stock boundary for the Northwestern Hawaiian Islands population of false killer whales. …the California Current System (CCS) … data(strata_ccs) names(strata_ccs) [1] &quot;CCS&quot; &quot;Southern_CA&quot; &quot;Central_CA&quot; &quot;Nothern_CA&quot; &quot;OR_WA&quot; #For more information on these strata, see Barlow (2010). … and the Eastern Tropical Pacific (ETP): data(strata_etp) names(strata_etp) [1] &quot;MOPS_AreaCoreM&quot; &quot;MOPS_AreaIn&quot; &quot;MOPS_AreaIn1&quot; [4] &quot;MOPS_AreaIn2&quot; &quot;MOPS_AREAINS&quot; &quot;MOPS_AREAMID&quot; [7] &quot;MOPS_AreaMid1&quot; &quot;MOPS_AreaMid2&quot; &quot;MOPS_AreaMOPS&quot; [10] &quot;MOPS_AREANORS&quot; &quot;MOPS_AreaOuterM&quot; &quot;MOPS_AreaSou&quot; [13] &quot;MOPS_AREASOUS&quot; &quot;MOPS_AreaSpin&quot; &quot;MOPS_AreaSpinS&quot; [16] &quot;MOPS_AREAWES&quot; &quot;PODS_93STRAT1&quot; &quot;PODS_93STRAT2&quot; [19] &quot;PODS_Area92&quot; &quot;PODS_AREA92RS&quot; &quot;PODS_Area92s&quot; [22] &quot;PODS_AREA93&quot; &quot;PODS_AREA93A&quot; &quot;PODS_AREA93AR&quot; [25] &quot;PODS_AREA93AS&quot; &quot;PODS_AREA93BR&quot; &quot;PODS_AREA93M&quot; [28] &quot;PODS_AREA93MS&quot; &quot;PODS_AREA93R&quot; &quot;PODS_AREA93R1&quot; [31] &quot;PODS_AREA93R2&quot; &quot;PODS_AREA93RS&quot; &quot;PODS_AREA93S&quot; [34] &quot;PODS_AREANCOR&quot; &quot;PODS_GOCpoly&quot; &quot;Pre1986_Area79ES1&quot; [37] &quot;Pre1986_Area79ES1s&quot; &quot;Pre1986_Area79ES2&quot; &quot;Pre1986_Area79ES2s&quot; [40] &quot;Pre1986_Area79NE1&quot; &quot;Pre1986_Area79NE1s&quot; &quot;Pre1986_Area79NE2&quot; [43] &quot;Pre1986_Area79NE2s&quot; &quot;Pre1986_Area79NE3&quot; &quot;Pre1986_Area79NE3s&quot; [46] &quot;Pre1986_AreaCal&quot; &quot;Pre1986_AreaCals&quot; &quot;Pre1986_AreaMid&quot; [49] &quot;Pre1986_AreaMidS&quot; &quot;Pre1986_AreaNorth&quot; &quot;Pre1986_AreaNorthS&quot; [52] &quot;Pre1986_AreaSouth&quot; &quot;Pre1986_AreaSouthS&quot; &quot;STAR_Area98a&quot; [55] &quot;STAR_Area98b&quot; &quot;STAR_AreaCore&quot; &quot;STAR_AreaCore2&quot; [58] &quot;STAR_AreaCoreS&quot; &quot;STAR_AreaNCoast&quot; &quot;STAR_AreaNCstS&quot; [61] &quot;STAR_AreaOuter&quot; &quot;STAR_AreaOuter00&quot; &quot;STAR_AreaSCoast&quot; [64] &quot;STAR_AreaSCstS&quot; &quot;STAR_AreaSPn&quot; &quot;STAR_AreaSPs&quot; [67] &quot;STAR_AreaSTAR&quot; &quot;STAR_AreaSTAR2&quot; &quot;STAR_AreaSTARlite&quot; [70] &quot;STAR_Dcaparea&quot; #For more information on these strata, contact &lt;swfsc.info@noaa.gov&gt; The package includes functions for visualizing and selecting from these strata. See the Strata Gallery appendix. To create your own geostratum, you would use code like this: # Create a dataframe of coordinates # (this example is a closed rectangle) mine1 &lt;- data.frame(Lon = c(-120, -125, -125, -120, 120), Lat = c(20, 20, 40, 40, 20)) # Make into a list my_strata &lt;- list(mine1 = mine1) # Check it out my_strata $mine1 Lon Lat 1 -120 20 2 -125 20 3 -125 40 4 -120 40 5 120 20 # Supply to load_settings() my_settings &lt;- load_settings(strata = my_strata) To keep some of the strata from strata_cnp then add one of your own design: # Subset `strata_cnp`: my_cnp &lt;- strata_cnp[c(2,3)] # Check it out my_cnp %&gt;% names [1] &quot;OtherCNP&quot; &quot;MHI&quot; # Create your new geostratum mine1 &lt;- data.frame(Lon = c(-120, -125, -125, -120, 120), Lat = c(20, 20, 40, 40, 20)) # Assemble your list: my_strata &lt;- c(my_cnp, list(mine1 = mine1)) # Check it out my_strata $OtherCNP Lon Lat 1 -131 40.00 2 -126 32.05 3 -120 25.00 4 -120 -5.00 5 -185 -5.00 6 -185 40.00 7 -131 40.00 $MHI Lon Lat 1 -156.00 22.00 2 -154.40 20.60 3 -153.50 19.20 4 -154.35 18.55 5 -155.20 17.75 6 -157.00 18.25 7 -157.50 19.20 8 -161.00 21.84 9 -160.00 23.00 10 -157.00 22.50 11 -156.00 22.00 $mine1 Lon Lat 1 -120 20 2 -125 20 3 -125 40 4 -120 40 5 120 20 # Supply to load_settings() my_settings &lt;- load_settings(strata = my_strata) Survey-wide settings Survey-wide settings apply universally to all species in the analysis. Defaults settings$survey $out_handling [1] &quot;remove&quot; $interpolate NULL $max_row_interval [1] 900 $segment_method [1] &quot;day&quot; $segment_target_km [1] 150 $segment_max_interval [1] 48 $segment_remainder_handling [1] &quot;segment&quot; $ship_list NULL $species_codes NULL $group_size_coefficients NULL $smear_angles [1] FALSE Defaults for the survey argument list are built up efficiently using the function load_survey_settings() (see example code at bottom). Details The survey_settings input accepts a list with any of the following named slots: out_handling: the first slot allows you to specify how data occurring outside of geo-strata should be handled. If this is set to \"remove\", those rows will be filtered out of the data early in the process. This reduces memory usage, speeds up processing, and gives you geographic control of how effort and sightings will be summarize. If this is set to \"stratum\", those data will be assigned to a fake geo-stratum, named \"out\". Effort in the \"out\" stratum will not be segmentized, but \"out\" sightings will be processed and retained in the final datasets. This setting might be useful if you want to use \"out\" data for survey summaries and/or detection function estimation. The default is \"remove\", since that saves the most time and memory. interpolate: This argument allows you to interpolate the DAS data at the onset of processing if your position updates are separated by large time intervals, which would make spatial effort and stratum assignments less exact. If this argument is NULL, then no interpolation will occur. If it is a number, e.g., 30, LTabundR will interpolate the data using simple-linear methods (i.e., no great-sphere calculations), such that position updates occur every 30 seconds or less. If adjacent DAS rows are from different dates or cruises, the interpolation routine will skip to the next pair of related rows. Interpolation will only occur for On-Effort rows. max_row_interval: The maximum alloweable time interval, in seconds, between rows before LTabundR assumes that there has been a break in survey data logging. The default is 900 seconds, or 15 minutes. segment_method: This and the next few slots are devoted to controlling how effort will be “segmentized”, or chopped into discrete sections for the purposes of estimating the variance of the density/abundance estimates. The two method options are \"day\" – all effort within the same Cruise-StudyArea-Stratum-Year-Effort scenario will be binned into segments by calendar date – and \"equallength\" – effort within each unique effort scenario (Cruise-StudyArea-etc.) will be divided into segments of approximately equal length. See the Appendix on segmentizing for details. segment_target_km: if segmentizing by \"equallength\", this field allows you to specify what that target length is, in km. The default is 150 km, the distance generally surveyed in one day on NOAA Fisheries surveys. segment_max_interval: if segmentizing by \"equallength\", this setting allows you to specify the time gaps in effort that are allowed to be contained within a single segment. For example, if your goal is a few large segments of equal length (e.g., 150-km segments, for bootstrap estimation of density variance), you are probably willing for discrete periods of effort to be concatenated into a single segment, even if the gaps between effort are as large as 1 or 2 days, in which case you would set segment_max_interval to 24 or 48 (hours), respectively. However, if your goal is many smaller segments (e.g., 5-km segments, for habitat modeling), you want to ensure that effort is contiguous so that segment locations can be accurately related to environmental variables, in which case you would set segment_max_interval to be very small (e.g., .2 hours, or 12 minutes). Setting this interval to a small number, such as 0.2, also allows the segmentizing function to overlook momentary breaks in effort. segment_remainder_handling: if segmentizing by \"equallength\", periods of effectively-contiguous effort (as specified by segment_max_interval) are unlikely to be perfectly divisible by your segment_target_km; there is going to be a remainder. You can handle this remainder in three ways: (1) \"disperse\" allows the function to adjust segment_target_km so that there is in fact no remainder, effectively dispersing the remainder evenly across all segments within that period of contiguous effort; (2) \"append\" asks the function to append the remainder to a randomly selected segment, such that most segments are the target length with the exception of one longer one; or (3) \"segment\" asks the function to simply place the remainder in its own segment, placed randomly within the period of contiguous effort. This setting also has a second layer of versatility because it can accept a one- or two-element character vector. If a two-element vector is provided (e.g., c(\"append\",\"segment\")), the first element will be used in the event that the remainder is less than or equal to half your segment_target_km; if the remainder is more than half that target length, the second element will be used. This feature allows for replication of the segmentizing methods in Becker et al. (2010). The remaining slots in survey_settings pertain to various datasets and settings used in data processing: ship_list: A data.frame containing a list of survey numbers and ship names. If not provided the default version, which was current as of the release of ABUND9 in 2020, will be used (data(ships)), although note that surveys not included there will not be associated with a ship. Supplied data.frames must match the column naming structure of data(ships). species_codes: A data.frame containing species codes. This is an optional input, chiefly used to format species names in the reporting stage of the workflow (lta_report() especially). If missing, neither data processing nor line transect analysis will be obstructed. If the user supplies a data.frame, it must match the column naming structure of data(species_codes). group_size_coefficients: A data.frame of calibration factors. Find details in the subsection on processing sightings and estimating group size. smear_angles: If TRUE (the default is FALSE), bearing angles to a group of animals will be “smeared” by adding a uniformly distributed random number between -5 and +5 degrees. This has not been used in any recent analyses because observers have not been rounding angles as much as they used to. It was suggested by Buckland et al. (2001) as a method for dealing with rounding which is especially influential when rounding to zero places many sightings at zero perpendicular distance. Cohort-specific settings Cohort-specific settings apply only to a group of species. Since you can add as many cohorts to a settings object as you need, this allows you to stage your entire analysis and run your code once without modifying code or creating multiple versions of your code for each analysis of each cohort. Defaults The default is to use a single cohort for all species: settings$cohorts %&gt;% names [1] &quot;default&quot; Default values for the default cohort: settings$cohorts$default $id [1] &quot;default&quot; $species NULL $strata NULL $probable_species [1] FALSE $sighting_method [1] 0 $cue_range [1] 0 1 2 3 4 5 6 7 $school_size_range [1] 0 10000 $school_size_calibrate [1] TRUE $calibration_floor [1] 0 $use_low_if_na [1] FALSE $io_sightings [1] 0 $geometric_mean_group [1] TRUE $truncation_km [1] 5.5 $beaufort_range [1] 0 1 2 3 4 5 6 $abeam_sightings [1] FALSE $strata_overlap_handling [1] &quot;smallest&quot; $distance_types [1] &quot;S&quot; &quot;F&quot; &quot;N&quot; $distance_modes [1] &quot;P&quot; &quot;C&quot; $distance_on_off [1] TRUE Defaults for the cohorts argument list are built up efficiently using the function load_cohort_settings() (see example code at bottom). Details The cohort_settings input accepts a list of any length. Each slot in that list can contain settings for a different cohort. Each cohort list can have any of the following named slots: id: An informal identifier for this cohort, to help you keep track of which cohort is which. For example, settings for a cohort of large whales species could be named \"big whales\"; settings for small delphinids and phocoenids could be named \"small_odontocetes\"; settings for beaked whales could be named \"beakers\". species: A character vector of species codes to include in this cohort. If NULL (the default), all species within the survey data will be included. Note that if you specify a vector, all species to be used in modeling a detection function for this cohort must be included here. For example, in Hawaii the bottlenose dolphin is analyzed as part of a multi-species pool along with the rough-toothed dolphin, Risso’s dolphin, and pygmy killer whale. However, the bottlenose dolphin has insular populations that need to be differentiated from their pelagic counterpart, which requires some special geostratum handling that behooves the preparation of a dedicated cohort for bottlenose dolphin. Even so, in the cohort_settings object for the bottlenose dolphin cohort, the species codes for the rough-toothed, Risso’s, and pygmy-killer-whale dolphins need to be provided in this species argument. Conversely, in the cohort_settings object that holds most other species, including the rough-toothed, Risso’s, and pygmy-killer-whale dolphins, the bottlenose dolphin’s code still needs to be included in this species argument. strata: A character vector of geostratum names. These must match the names listed in the strata slot of your survey settings (see documentation for load_survey_settings()). If NULL (the default), all geostrata in your survey settings will be used. This argument is an opportunity to subset the geostrata used for a cohort. For example, as discussed above, certain dolphin species in Hawaiian waters have unique geostrata that apply only to their insular/pelagic populations, and should only have a role in breaking effort segments in the bootstrap variance analysis for these specific species. Those dolphins should be given their own cohort, and those insular/pelagic geostrata should be included in this strata argument. Conversely, all other species should be placed in a separate cohort and only the generic geostrata should be included in this strata argument. See the WHICEAS example below for a demonstration. probable_species: If TRUE (default is FALSE), the “probable” species identifications will be used in place of the “unidentified” categories. sighting_method: A coded integer which determines which sightings will be included based on how they were first seen. Allowable codes are 0=any method, 1=with 25X only, 2=neither with 25x binoculars nor from the helicopter (i.e., naked eyes and 7x binoculars only). These codes match those used in ABUND7/9. cue_range: Numeric vector of acceptable “observation cues” for sightings used in estimates of abundance. (0=this detail is missing in the data, 1=associated birds, 2=splashes, 3=body of the marine mammal, 4=associated vessel, 5=?, 6=blow / spout, 7=associated helicopter). These codes match those used in ABUND7/9. school_size_range: Minimum and maximum group sizes to be included in estimates of abundance. This is the overall group size, not the number of the given species that are present in a mixed-species group. school_size_calibrate: A logical (TRUE or FALSE) specifying whether or not to carry out group size adjustments according to the calibration table provided in survey$group_size_coefficients (if that table is provided). This setting allows you to toggle the survey-wide setting for certain cohorts. For example, perhaps you want to carry out calibration for a cohort of dolphin species, but not for a cohort of large whales whose group sizes tend to be smaller and easier to estimate accurately. calibration_floor: A numeric indicating the minimum school size estimate for which group size calibration will be attempted. This pertains only to observers who do no have an entry in the group_size_coefficients table provided in load_survey_settings() (that table has a calibration floor for each observer). The default is 0, meaning that calibration will be attempted for all group size estimates, regardless of the raw estimate. use_low_if_na: If this setting is TRUE, and an observer(s) does not make a best estimate of group size, mean group size will be calculated from “low” estimates. This will be done only if no observer has a “best” estimate. io_sightings: A coded integer which specifies how sightings by the independent observer will be handled. Allowable codes, which are inherited from those used in ABUND7/9, are \"_1\"=include independent observer sightings wih all other sightings, \"0\"=ignore sightings by independent observer, \"1\"=use only sightings made by regular observer team WHEN an independent observer was present, \"2\"=include only sightings made by the independent observer. IO sightings are typically used only for making g(0) estimates, otherwise IO sightings are usually ignored (code = \"0\"). geometric_mean_group: This logical variable specifies whether to use a weighted geometric mean when calculating mean group size. Barlow et al. (1998) found that this gave slightly better performance than an arithmetic mean group size for calibrated estimates. Default is TRUE, but a geometric mean will only be calculated if group_size_coefficients is not NULL. If group_size_coefficients is NULL, then an arithmetic mean will be calculated. (This setting does not apply to subgroup analyses.) truncation_km: Specifies the maximum perpendicular distance for groups that could potentially be included for abundance estimation. This is not the stage at which you set the truncation distance for detection function modeling; it is simply a preliminary cutoff for sightings made at unrealiable detection distances. The default is set at 5.5 km, which is the maximum distance sightings are typically approached (“closing mode”) during NOAA Fisheries surveys. beaufort_range: Vector of Beaufort sea states (integers) that are acceptable in estimating the detection function and density. Beaufort data with a decimal place will be rounded to the nearest integer to evaluate for inclusion. abeam_sightings: = If TRUE, sightings that occur aft of beam (i.e., 90 degrees) are included in estimating the detection function and densities. Default is FALSE: all abeam sightings will be ignored. strata_overlap_handling: This setting informs how effort is split into segments when surveys cross stratum boundaries, and also which stratum name is assigned to each row of data. Note that the main impact of this setting is on how effort is broken into segments; the assigned stratum name is for display only and will not constrain options for including/excluding strata in analyses farther along in the LTabundR workflow. The default option is \"smallest\", which means that effort will always be assigned to the smallest stratum when multiple strata overlap spatially. This is a safe option for surveys with “nested” strata (such as the Central North Pacific strata used by NOAA Fisheries; see below). Another option is \"each\"in which each time a stratum boundary is crossed the current segment will end and a new segment will begin. Also, stratum assignments for each row of effort will be shown as a concatenation of all the stratum layers overlapping at its position (e.g., “OtherCNP&amp;HI_EEZ”). Note that the \"each\" option segmentizes effort in the exact same was as \"smallest\" when strata are fully nested; its main advantage is in dealing with partially overlapping strata (such as strata used in the Marianas Archipelago; see below). The third option is \"largest\", in which the largest of overlapping strata is used to assign a stratum name to each row. (We are not sure what use case this would serve, but we offer it as an option for niche analyses.) distance_types: A character vector of the effort types that will be included in detection function estimation and density estimation, and therefore considered in effort segmentizing. Accepted values are \"S\" (systematic/standard effort), \"F\" (fine-scale effort), and \"N\" (non-systematic/non-standard effort, in which systematic protocols are being used but effort is not occurring along design-based transect routes). The default values are c(\"S\",\"F\",\"N\"). distance_modes: The effort modes that will be included in detection function estimation and density estimation, and therefore considered in effort segmentizing. Accepted values are \"P\" (passing) and \"C\" (closing), and the default values are c(\"P\",\"C\"). distance_on_off: The value(s) of OnEffort (On Effort is TRUE, Off Effort is FALSE) that will be included in detection function estimation and density estimation, and therefore considered in effort segmentizing. Default is TRUE only. (We don’t expect FALSE or c(TRUE,FALSE) to be used much, if at all, but we make this option available). Example code Use settings defaults No strata or group size calibration. settings &lt;- load_settings() Use settings defaults, but with strata # Load strata dataframes data(strata_cnp) settings &lt;- load_settings(strata = strata_cnp) Customize survey, but not cohorts This code will process survey data such that effort segments are 5 km in length, with any effort falling outside of the provided geostrata relegated to a virtual geostratum named \"out\". Since a cohort is not specified, the default values will be used. # Load built-in datasets data(strata_cnp) data(group_size_coefficients) data(ships) data(species_codes) # Survey settings survey &lt;- load_survey_settings(out_handling = &#39;stratum&#39;, max_row_interval = 700, segment_method = &#39;equallength&#39;, segment_target_km = 5, segment_max_interval = .3, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), ship_list = ships, species_codes = species_codes, group_size_coefficients = group_size_coefficients, smear_angles = FALSE) # Load settings settings &lt;- load_settings(strata = strata_cnp, survey) Fully custom: WHICEAS case study These are the settings we will use in the remainder of the tutorial. Survey-wide settings To emulate the analysis done in Bradford et al. (2021), we want to process effort with 150-km segments, relegating any remainder to its own segments. We also want to make sure to remove any survey data that falls outside of the geostrata, to ensure that detection functions are regionally specific. We will use the built-in tables for species codes, ship codes, and group size calibration coefficients. data(species_codes) data(ships) data(group_size_coefficients) survey &lt;- load_survey_settings( out_handling = &#39;remove&#39;, max_row_interval = Inf, segment_method = &quot;equallength&quot;, segment_target_km = 150, segment_max_interval = 24, segment_remainder_handling = c(&quot;segment&quot;), ship_list = ships, species_codes = species_codes, group_size_coefficients = group_size_coefficients, smear_angles = FALSE ) Geostrata We will use the built-in dataset of Central North Pacific geostrata: data(strata_cnp) strata_cnp %&gt;% names [1] &quot;HI_EEZ&quot; &quot;OtherCNP&quot; &quot;MHI&quot; &quot;WHICEAS&quot; [5] &quot;Spotted_OU&quot; &quot;Spotted_FI&quot; &quot;Spotted_BI&quot; &quot;Bottlenose_KaNi&quot; [9] &quot;Bottlenose_OUFI&quot; &quot;Bottlenose_BI&quot; &quot;NWHI&quot; Species cohorts Cohort 1: All species At least one cohort needs to be specified in order to run process survey data, so this first cohort will serve as a ‘catch-all’ for species who do not need special handling. It does not hurt to include all species in this catch-all cohort – except perhaps by increasing the file-size of your processed data by a few kilobytes – even if you will be creating a separate, dedicated cohort for one of these species downstream. Having a catch-all cohort like this serves two purposes: (1) it avoids unforeseen complications if you will be modeling detection functions with multi-species pools, as mentioned above; and (2) it will simplify the code you will use to produce summary statistics of effort and sightings totals, since you will not need to pool together statistics from multiple cohorts. To build this catch-all cohort, we will not specify any species so that all species in the data are included, and we will specify that only the generic geostrata should be used, so that species specific insular stock boundaries are ignored. Here we show all possible inputs. Most of these match the built-in defaults. Those that do not (there are just 4) are noted with a commented asterisk. all_species &lt;- load_cohort_settings( id = &quot;all&quot;, # * species = NULL, strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;), # * probable_species = FALSE, sighting_method = 0, cue_range = 0:7, school_size_range = c(0, 10000), school_size_calibrate = TRUE, calibration_floor = 0, use_low_if_na = TRUE, # * io_sightings = 0, geometric_mean_group = TRUE, truncation_km = 7.5, # * beaufort_range = 0:6, abeam_sightings = FALSE, strata_overlap_handling = c(&quot;smallest&quot;), distance_types = c(&#39;S&#39;,&#39;F&#39;,&#39;N&#39;), distance_modes = c(&#39;P&#39;,&#39;C&#39;), distance_on_off = TRUE ) Cohort 2: Bottlenose dolphin As mentioned above, bottlenose dolphins are going to be analyzed as part of a multi-species pool that includes the rough-toothed dolphin, Risso’s dolphin, and pygmy killer whale. Because those species’ codes will be needed to model the detection function used in bottlenose dolphin density/abundance estimation, those species codes will be included in this cohort’s settings. Also mentioned above: Hawaii has a pelagic population of bottlenose dolphins as well as several distinct insular stocks. In this case study, we are interested in estimating only the abundance of the pelagic population, which means we will need to include geostrata of the insular stock boundaries in order to make sure the effort and sightings within those insular areas are ignored. That means we will specify the generic geostrata (\"WHICEAS\", \"HI_EEZ\", and \"Other_CNP\"), as well as the geostrata for the insular stocks. We can spell out as many any inputs that we wish, but here will only show the inputs that are non-default and/or different from Cohort 1 above. bottlenose &lt;- load_cohort_settings( id = &quot;bottlenose&quot;, species = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;, &#39;Bottlenose_BI&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_KaNi&#39;), use_low_if_na = TRUE, truncation_km = 7.5) Cohort 3: Pantropical spotted dolphin Similar to the bottlenose dolphin above, spotted dolphins in Hawaiian waters belong to pelagic stocks as well as insular stocks. We will be estimating density/abundance for the only pelagic stocks here, but we need to include the geostrata for the insular stocks in order to ignore their effort and sightings. spotted &lt;- load_cohort_settings( id = &quot;spotted&quot;, species = &#39;002&#39;, strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;, &#39;Spotted_OU&#39;,&#39;Spotted_FI&#39;,&#39;Spotted_BI&#39;), use_low_if_na = TRUE, truncation_km = 7.5) Compile settings Finally, we create our settings object, providing cohorts as a list of lists: settings &lt;- load_settings(strata = strata_cnp, survey = survey, cohorts = list(all_species, bottlenose, spotted)) Save this settings object locally, to use in downstream scripts: save(settings, file=&#39;whiceas_settings.RData&#39;) "],["das.html", " 2 DAS editing Reviewing a DAS file Staging edits", " 2 DAS editing The LTabundR package includes several functions that facilitate the exploration of DAS files of WinCruz survey data, as well as a function that allows you to apply “edits” to the survey data in a reproducible way (i.e., using code, without modifying the actual data file). Reviewing a DAS file das_readtext() The swfscDAS package has functions for reading in a DAS file and parsing it into columns of fixed-width text. To complement those functions, LTabundR includes the function das_readtext(), which reads in a DAS file without applying any column parsing, so that the data can be read in its true raw format. # Local path to DAS file das_file &lt;- &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39; das &lt;- das_readtext(das_file) das$das %&gt;% head(20) [1] &quot; 1B 1805 073086 N31:57. W116:57. 989&quot; [2] &quot; 1S 1805 073086 N31:57. W116:57. 01 004 6 4 300 1.77 1.5&quot; [3] &quot; 2A 1805 073086 N31:57. W116:57. 01 15.0 N 099&quot; [4] &quot; 1 004 0001 0001 0001 100 000 000 000&quot; [5] &quot; 1S 0610 073186 N30:05. W116:04. 01 004 3 4 340 5.00 0.1&quot; [6] &quot; 2A 0610 073186 N30:05. W116:04. 01 18.3 N 005&quot; [7] &quot; 1 004 0150 0175 0125 100 000 000 000&quot; [8] &quot; 3B.1311 073186 N29:46. W115:52. 989 N&quot; [9] &quot; 4R.1311 073186 N29:46. W115:52. N&quot; [10] &quot; 5P.1311 073186 N29:46. W115:52. 022 031 056&quot; [11] &quot; 6V.1311 073186 N29:46. W115:52. 4 16.1&quot; [12] &quot; 7N.1311 073186 N29:46. W115:52. 167 10.5&quot; [13] &quot; 8W.1311 073186 N29:46. W115:52. 2&quot; [14] &quot; 9V.1329 073186 N29:43. W115:51. 5 16.1&quot; [15] &quot; 10P.1404 073186 N29:37. W115:50. 004 056 062&quot; [16] &quot; 11V.1404 073186 N29:37. W115:50. 5 18.3&quot; [17] &quot; 12S.1406 073186 N29:37. W115:50. 02 004 3 4 355 1.4 1.8&quot; [18] &quot; 13A.1406 073186 N29:37. W115:50. 02 18.3 Y 005&quot; [19] &quot; 1 022 0030 0100 0020 100 000 000 000&quot; [20] &quot; 2 004 0250 0300 0200 100 000 000 000&quot; To follow along, this data file can be downloaded here. das_inspector() We also provide a function, das_inspector(), that allows you to explore a DAS file within an interactive Shiny app. This app can also be used to find, prepare, and preview coded edits to the DAS data. dasi &lt;- das_inspector(das_file) In this app, you specify the rows and ‘columns’ (i.e., character indices) that will be affected by your edit, the edit you want to apply, and the type of edit it will be (see next subsection). A screenshot of the das_inspector() app: . Once you have an edit prepared and previewed within the app, you can then “save” that edit with the click of a button, and when you close the app your log of staged edits will be returned as a list. The list produced from the edit staged in the screenshot above: . For this reason it can be useful to type the above command such that the output is saved into an object; in the example above we saved the output into an object named dasi. You can then pass this list of edits to das_editor() (next subsection on this page), or as an argument within process_surveys() (next page). However, for the sake of full code reproducibility, it may be most useful to use this output to write code to stage this edit using the das_editor() function below. Staging edits The das_editor() function allows you to apply edits to a DAS file without modifiyng the original data. You supply edits to this function as a set of instructions saved within a list object. You can prepare these instructions manually or use the das_inspector() function above to get help. The das_editor() function then loops through each edit, applies them to a local version of the data, and returns that modified data to the user. This allows survey data to be modified reproducibly before being processed with LTabundR::process_surveys() without touching the original DAS data files or requiring analysts to duplicate files and make one-off modifications manually. We expect it would be rare for a user to call das_editor() directly; instead, they would supply their edits as an argument in process_surveys(), and that function will call das_editor() internally to amend the survey data before it is processed (this is discussed on the next page). Note that there is no limit to the number of edits that can be provided in a single list, and they can be provided in any order. The das_editor() function will sort the edits by DAS file and by edit type, and then apply edits in increasing order of “disruption”, i.e., text replacements first, then moving rows of data (no net change in number of rows), then copying-pasting, inserting, and deleting. The das_editor() function can currently handle 6 types of edits, which we shall demonstrate below. Types of editing To demonstrate the types of editing actions that can be achieved through das_editor(), we will use the DAS file from the 2020 WHICEAS survey (you can download here). # Local path to das_file das_file &lt;- &quot;data/surveys/HICEASwinter2020.das&quot; das &lt;- das_readtext(das_file) Verbatim text replacement The edit will be interpreted verbatim as text that will replace the specified data. edits &lt;- list(list(das_file = das_file, type = &#39;text&#39;, rows = 10:15, chars = 20:39, edit = &#39;lat, lon&#39;)) Here is what this edit log object will look like: edits [[1]] [[1]]$das_file [1] &quot;data/surveys/HICEASwinter2020.das&quot; [[1]]$type [1] &quot;text&quot; [[1]]$rows [1] 10 11 12 13 14 15 [[1]]$chars [1] 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 [[1]]$edit [1] &quot;lat, lon&quot; And here is the effect it will have: # Before das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; # After dase &lt;- das_editor(edits) Applying edits to DAS file: data/surveys/HICEASwinter2020.das --- verbatim/function text replacements ... --- --- working on edit 1 ... dase$das[[1]]$das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 lat, lon&quot; [3] &quot;011B.073111 011920 lat, lon 2001 C -10 N&quot; [4] &quot;012R.073111 011920 lat, lon F&quot; [5] &quot;013P.073111 011920 lat, lon 126 307 238 &quot; [6] &quot;014V.073111 011920 lat, lon 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 lat, lon 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; Function-based text replacement The edit will be evaluated as a function that is applied to the specified characters in each of rows. edits &lt;- list(list(das_file = das_file, type = &#39;function&#39;, rows = 10:15, chars = 20:39, edit = &#39;tolower&#39;)) # Before das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; # After dase &lt;- das_editor(edits) Applying edits to DAS file: data/surveys/HICEASwinter2020.das --- verbatim/function text replacements ... --- --- working on edit 1 ... dase$das[[1]]$das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 n21:50.91 w159:46.33&quot; [3] &quot;011B.073111 011920 n21:51.12 w159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 n21:51.12 w159:46.36 F&quot; [5] &quot;013P.073111 011920 n21:51.12 w159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 n21:51.12 w159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 n21:51.12 w159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; A special application of this form of editing is adjusting timestamps using the LTabundR function das_time(). In this next example, we subtract an hour from the first 5 rows of timestamps: edits &lt;- list(list(das_file = das_file, type = &#39;function&#39;, rows = 1:5, chars = 6:40, edit = &#39;function(x){das_time(x, tz_adjust = -1)$dt}&#39;)) # Before das$das[1:6] [1] &quot;001* 071152 011920 N21:47.99 W159:45.91&quot; [2] &quot;002* 071352 011920 N21:48.31 W159:45.94&quot; [3] &quot;003* 071552 011920 N21:48.63 W159:45.97&quot; [4] &quot;004* 071752 011920 N21:48.95 W159:46.01&quot; [5] &quot;005* 071952 011920 N21:49.28 W159:46.04&quot; [6] &quot;006* 072152 011920 N21:49.60 W159:46.08&quot; # After dase &lt;- das_editor(edits) Applying edits to DAS file: data/surveys/HICEASwinter2020.das --- verbatim/function text replacements ... --- --- working on edit 1 ... dase$das[[1]]$das$das[1:6] [1] &quot;001* 061152 011920 N21:47.99 W159:45.91&quot; [2] &quot;002* 061352 011920 N21:48.31 W159:45.94&quot; [3] &quot;003* 061552 011920 N21:48.63 W159:45.97&quot; [4] &quot;004* 061752 011920 N21:48.95 W159:46.01&quot; [5] &quot;005* 061952 011920 N21:49.28 W159:46.04&quot; [6] &quot;006* 072152 011920 N21:49.60 W159:46.08&quot; In the event that a survey was conducted using UTC timestamps instead of local time, you can adjust each timestamp according to its actual timezone as determined from its corresponding lat/long coordinates. Let’s say the first 5 timestamps were collected in UTC by accident. The following code could correct that mistake: edits &lt;- list(list(das_file = das_file, type = &#39;function&#39;, rows = 1:5, chars = 6:40, edit = &#39;function(x){das_time(x, tz_adjust = &quot;from utc&quot;)$dt}&#39;)) # Before das$das[1:6] [1] &quot;001* 071152 011920 N21:47.99 W159:45.91&quot; [2] &quot;002* 071352 011920 N21:48.31 W159:45.94&quot; [3] &quot;003* 071552 011920 N21:48.63 W159:45.97&quot; [4] &quot;004* 071752 011920 N21:48.95 W159:46.01&quot; [5] &quot;005* 071952 011920 N21:49.28 W159:46.04&quot; [6] &quot;006* 072152 011920 N21:49.60 W159:46.08&quot; # After dase &lt;- das_editor(edits) Applying edits to DAS file: data/surveys/HICEASwinter2020.das --- verbatim/function text replacements ... --- --- working on edit 1 ... dase$das[[1]]$das$das[1:6] [1] &quot;001* 201152 011820 N21:47.99 W159:45.91&quot; [2] &quot;002* 201352 011820 N21:48.31 W159:45.94&quot; [3] &quot;003* 201552 011820 N21:48.63 W159:45.97&quot; [4] &quot;004* 201752 011820 N21:48.95 W159:46.01&quot; [5] &quot;005* 201952 011820 N21:49.28 W159:46.04&quot; [6] &quot;006* 072152 011920 N21:49.60 W159:46.08&quot; Moving data The rows will be deleted from their current location and pasted immediately below the row number specified by edit. The moved rows will be given the same date, time, latitude, and longitude, as the edit row. edits &lt;- list(list(das_file = das_file, type = &#39;move&#39;, rows = 10, chars = NULL, edit = 15)) # Before das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; # After dase &lt;- das_editor(edits) Applying edits to DAS file: data/surveys/HICEASwinter2020.das --- move, copy/paste, insertion, and deletion events ... --- --- working on edit 1 ... dase$das[[1]]$das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [3] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [4] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [5] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [6] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [7] &quot;010* 073111 011920 N21:51.12 W159:46.36&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; Copying &amp; pasting data The rows will be copied from their current location and pasted immediately below the row number specified by edit. The pasted rows will be given the same date, time, latitude, and longitude, as the edit row. edits &lt;- list(list(das_file = das_file, type = &#39;copy&#39;, rows = 10, edit = 15)) # Before das$das[9:17] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; [9] &quot;017*.073152 011920 N21:51.24 W159:46.39&quot; # After dase &lt;- das_editor(edits) Applying edits to DAS file: data/surveys/HICEASwinter2020.das --- move, copy/paste, insertion, and deletion events ... --- --- working on edit 1 ... dase$das[[1]]$das$das[9:17] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;010* 073111 011920 N21:51.12 W159:46.36&quot; [9] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; Inserting data The text provided in edit will be inserted verbatim immediately below the first of the rows provided. edits &lt;- list(list(das_file = das_file, type = &#39;insert&#39;, rows = 10, edit = &quot;SEQ* HHMMSS MMDDYY NDG:MI.NT WDEG:MI.NT&quot;)) # Before das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; # After dase &lt;- das_editor(edits) Applying edits to DAS file: data/surveys/HICEASwinter2020.das --- move, copy/paste, insertion, and deletion events ... --- --- working on edit 1 ... dase$das[[1]]$das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;SEQ* HHMMSS MMDDYY NDG:MI.NT WDEG:MI.NT&quot; [4] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [5] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [6] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [7] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [8] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; Deleting data The specified rows will be deleted. edits &lt;- list(list(das_file = das_file, type = &#39;delete&#39;, rows = 10)) # Before das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; # After dase &lt;- das_editor(edits) Applying edits to DAS file: data/surveys/HICEASwinter2020.das --- move, copy/paste, insertion, and deletion events ... --- --- working on edit 1 ... dase$das[[1]]$das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [3] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [4] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [5] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [6] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [7] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; [8] &quot;017*.073152 011920 N21:51.24 W159:46.39&quot; Actual edits The above examples were silly demonstrations of the types of edits that can be handled by das_editor(). Here we show the preparation of four edits that we will actually use when we process surveys on the next page. These edits will be applied to the following DAS file of survey data from 1986-2020: # Local path to das_file das_file &lt;- &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39; das &lt;- das_readtext(das_file) Cruise 1607 sighting 55 This sighting, at sequence ID 032 below, currently triggers errors in swfscDAS due to a manually entered R event a few lines above that does not have the P (observer positions) event that typically follows it. Without tht P entry, the observer positions of the sighting are unknown. das[128111:128125,] [1] &quot;022P 120643 041597 N37:00.08 W151:55.47 143 091 005&quot; [2] &quot;023C 120643 041597 N37:00.08 W151:55.47 both right and left on 7x and naked eye&quot; [3] &quot;024N 120643 041597 N37:00.08 W151:55.47 075 08.0&quot; [4] &quot;025W 120643 041597 N37:00.08 W151:55.47 1 310 6.0&quot; [5] &quot;026C 120916 041597 N37:00.33 W151:55.18 both right and left on 7x and naked eye&quot; [6] &quot;027* 121120 041597 N37:00.45 W151:54.85&quot; [7] &quot;028C 121933 041597 N37:00.75 W151:53.50 both right and left back on 25x&quot; [8] &quot; R.121933 041597 N37:00.75 W151:53.50 S&quot; [9] &quot;029*.122120 041597 N37:00.81 W151:53.22&quot; [10] &quot;030V.122120 041597 N37:00.81 W151:53.22 4 07 320 21.0&quot; [11] &quot;031C.122946 041597 N37:01.12 W151:51.77 wind speed appears lower than bridge speed, 10-15&quot; [12] &quot;032S.123023 041597 N37:01.14 W151:51.70 055 005 3 4 058 4.0 0.8&quot; [13] &quot;033A.123023 041597 N37:01.14 W151:51.70 055 N N 022&quot; [14] &quot; 1 005 0002 0002 0002 100&quot; [15] &quot;034C.123120 041597 N37:01.17 W151:51.51 remained on effort after sighting&quot; (Note that we used das_inspector() to get the row numbers for this region of the data.) To fix this, we can stage an edit that copies the P line that occurs minutes earlier and pastes that line just below the rogue R line. edit_1607_55 &lt;- list(das_file = das_file, type = &#39;copy&#39;, rows = 128111, chars = NULL, edit = 128118) Here is what this change will look like: dase &lt;- das_editor(list(edit_1607_55)) Applying edits to DAS file: data/surveys/CenPac1986-2020_Final_alb.das --- move, copy/paste, insertion, and deletion events ... --- --- working on edit 1 ... dase$das[[1]]$das$das[128111:128123] [1] &quot;022P 120643 041597 N37:00.08 W151:55.47 143 091 005&quot; [2] &quot;023C 120643 041597 N37:00.08 W151:55.47 both right and left on 7x and naked eye&quot; [3] &quot;024N 120643 041597 N37:00.08 W151:55.47 075 08.0&quot; [4] &quot;025W 120643 041597 N37:00.08 W151:55.47 1 310 6.0&quot; [5] &quot;026C 120916 041597 N37:00.33 W151:55.18 both right and left on 7x and naked eye&quot; [6] &quot;027* 121120 041597 N37:00.45 W151:54.85&quot; [7] &quot;028C 121933 041597 N37:00.75 W151:53.50 both right and left back on 25x&quot; [8] &quot; R.121933 041597 N37:00.75 W151:53.50 S&quot; [9] &quot;022P 121933 041597 N37:00.75 W151:53.50 143 091 005&quot; [10] &quot;029*.122120 041597 N37:00.81 W151:53.22&quot; [11] &quot;030V.122120 041597 N37:00.81 W151:53.22 4 07 320 21.0&quot; [12] &quot;031C.122946 041597 N37:01.12 W151:51.77 wind speed appears lower than bridge speed, 10-15&quot; [13] &quot;032S.123023 041597 N37:01.14 W151:51.70 055 005 3 4 058 4.0 0.8&quot; Cruise 1607 sighting 68 This sighting faces a similar issue: a rogue R event without the follow-up P event. This case is also missing the follow-up V event (viewing conditions). das[129980:129993,] [1] &quot;012* 065738 042797 N31:03.98 W136:03.60&quot; [2] &quot;013C 065944 042797 N31:04.25 W136:03.55 Both observers on 7X since going ON EFFORT this morning. JP.&quot; [3] &quot;014P 070118 042797 N31:04.43 W136:03.51 091 005 148&quot; [4] &quot;015V 070118 042797 N31:04.43 W136:03.51 5 06 320 20.0&quot; [5] &quot;016N 070118 042797 N31:04.43 W136:03.51 010 08.0&quot; [6] &quot;017W 070118 042797 N31:04.43 W136:03.51 1 02 03 035 6.0&quot; [7] &quot;018C 070257 042797 N31:04.65 W136:03.47 This rotation is now on the 25X&quot; [8] &quot; R.070257 042797 N31:04.65 W136:03.47 S&quot; [9] &quot;019*.070738 042797 N31:05.25 W136:03.36&quot; [10] &quot;020S.071601 042797 N31:06.34 W136:03.16 068 148 3 4 028 1.8 1.5&quot; [11] &quot;021A.071601 042797 N31:06.34 W136:03.16 068 N N 037&quot; [12] &quot; 1 091 0009 0011 0008 100&quot; [13] &quot; 2 005 0014 0015 0012 100&quot; [14] &quot; 3 148 0018 0023 0015 100&quot; To fix this we will stage a similar edit, this time copying and pasting two rows (P and V events) below the rogue R event: edit_1607_68 &lt;- list(das_file = das_file, type = &#39;copy&#39;, rows = c(129982, 129983 , 129985), chars = NULL, edit = 129987) Preview of change: dase &lt;- das_editor(list(edit_1607_68)) Applying edits to DAS file: data/surveys/CenPac1986-2020_Final_alb.das --- move, copy/paste, insertion, and deletion events ... --- --- working on edit 1 ... dase$das[[1]]$das$das[129982:129995] [1] &quot;014P 070118 042797 N31:04.43 W136:03.51 091 005 148&quot; [2] &quot;015V 070118 042797 N31:04.43 W136:03.51 5 06 320 20.0&quot; [3] &quot;016N 070118 042797 N31:04.43 W136:03.51 010 08.0&quot; [4] &quot;017W 070118 042797 N31:04.43 W136:03.51 1 02 03 035 6.0&quot; [5] &quot;018C 070257 042797 N31:04.65 W136:03.47 This rotation is now on the 25X&quot; [6] &quot; R.070257 042797 N31:04.65 W136:03.47 S&quot; [7] &quot;014P 070257 042797 N31:04.65 W136:03.47 091 005 148&quot; [8] &quot;015V 070257 042797 N31:04.65 W136:03.47 5 06 320 20.0&quot; [9] &quot;017W 070257 042797 N31:04.65 W136:03.47 1 02 03 035 6.0&quot; [10] &quot;019*.070738 042797 N31:05.25 W136:03.36&quot; [11] &quot;020S.071601 042797 N31:06.34 W136:03.16 068 148 3 4 028 1.8 1.5&quot; [12] &quot;021A.071601 042797 N31:06.34 W136:03.16 068 N N 037&quot; [13] &quot; 1 091 0009 0011 0008 100&quot; [14] &quot; 2 005 0014 0015 0012 100&quot; Cruise 1621 sighting 245 This is another case of a rogue R event, again missing both the requisite P and the V post-R events. das[271930:271939,] [1] &quot;036E 085345 103002 N20:22.56 W160:02.21 U&quot; [2] &quot;037R.085346 103002 N20:22.56 W160:02.21 S&quot; [3] &quot;038P.085346 103002 N20:22.56 W160:02.21 126 224 200&quot; [4] &quot;039V.085346 103002 N20:22.56 W160:02.21 5 07 000 21.0&quot; [5] &quot;040N.085346 103002 N20:22.56 W160:02.21 285 09.7&quot; [6] &quot;041W.085346 103002 N20:22.56 W160:02.21 1 06 02 045 6.0&quot; [7] &quot;042E 085352 103002 N20:22.56 W160:02.22 U&quot; [8] &quot;043R.085354 103002 N20:22.57 W160:02.23 S&quot; [9] &quot;048S.085359 103002 N20:22.57 W160:02.24 245 200 3 4 057 11.0 0.35 270 2.0&quot; [10] &quot;049A.085359 103002 N20:22.57 W160:02.24 245 N N 015&quot; Staged edit: edit_1621_245 &lt;- list(das_file = das_file, type = &#39;copy&#39;, rows = 271932:271933, chars = NULL, edit = 271937) Preview of change: dase &lt;- das_editor(list(edit_1621_245)) Applying edits to DAS file: data/surveys/CenPac1986-2020_Final_alb.das --- move, copy/paste, insertion, and deletion events ... --- --- working on edit 1 ... dase$das[[1]]$das$das[271930:271941] [1] &quot;036E 085345 103002 N20:22.56 W160:02.21 U&quot; [2] &quot;037R.085346 103002 N20:22.56 W160:02.21 S&quot; [3] &quot;038P.085346 103002 N20:22.56 W160:02.21 126 224 200&quot; [4] &quot;039V.085346 103002 N20:22.56 W160:02.21 5 07 000 21.0&quot; [5] &quot;040N.085346 103002 N20:22.56 W160:02.21 285 09.7&quot; [6] &quot;041W.085346 103002 N20:22.56 W160:02.21 1 06 02 045 6.0&quot; [7] &quot;042E 085352 103002 N20:22.56 W160:02.22 U&quot; [8] &quot;043R.085354 103002 N20:22.57 W160:02.23 S&quot; [9] &quot;038P.085354 103002 N20:22.57 W160:02.23 126 224 200&quot; [10] &quot;039V.085354 103002 N20:22.57 W160:02.23 5 07 000 21.0&quot; [11] &quot;048S.085359 103002 N20:22.57 W160:02.24 245 200 3 4 057 11.0 0.35 270 2.0&quot; [12] &quot;049A.085359 103002 N20:22.57 W160:02.24 245 N N 015&quot; Timestamp issues with Cruise 1004 This edit will correct for the fact that all of Cruise 1004 was conducted using UTC timestamps instead of local timestamps. edit_1004_utc &lt;- list(das_file = das_file, type = &#39;function&#39;, rows = 433327:437665, chars = 6:39, edit = &#39;function(x){das_time(x, tz_adjust = &quot;from utc&quot;)$dt}&#39;) # Before das$das[433326:433330] # beginning of cruise [1] &quot;229* 181728 020510 N13:14.49 E145:00.34&quot; [2] &quot;001* 201109 041910 N13:35.01 E145:59.35&quot; [3] &quot;002* 201309 041910 N13:35.03 E145:59.42&quot; [4] &quot;003* 201509 041910 N13:35.08 E145:59.54&quot; [5] &quot;004* 201709 041910 N13:35.13 E145:59.70&quot; das$das[437664:437667] # end of cruise [1] &quot;431* 044625 050410 N21:15.78 W158:53.32&quot; [2] &quot;432* 044825 050410 N21:16.05 W158:53.31&quot; [3] &quot;001* 144327 080410 N32:39.39 W117:13.60&quot; [4] &quot;002* 144827 080410 N32:38.58 W117:13.49&quot; dase &lt;- das_editor(list(edit_1004_utc)) # After dase$das[[1]]$das$das[433326:433330] [1] &quot;229* 181728 020510 N13:14.49 E145:00.34&quot; [2] &quot;001* 061109 042010 N13:35.01 E145:59.35&quot; [3] &quot;002* 061309 042010 N13:35.03 E145:59.42&quot; [4] &quot;003* 061509 042010 N13:35.08 E145:59.54&quot; [5] &quot;004* 061709 042010 N13:35.13 E145:59.70&quot; dase$das[[1]]$das$das[437664:437667] [1] &quot;431* 184625 050310 N21:15.78 W158:53.32&quot; [2] &quot;432* 184825 050310 N21:16.05 W158:53.31&quot; [3] &quot;001* 074327 080410 N32:39.39 W117:13.60&quot; [4] &quot;002* 144827 080410 N32:38.58 W117:13.49&quot; Note that this type of edit can be dangerous, however, since ships can cross time zone boundaries mid-day, potentially repeating timestamps and giving the appearance that the DAS data is out of chronological order, which may bring about consequences for data processing that are difficult to predict. This type of edit is also time-consuming; since the time zone needs to be calculated in each DAS row individually, this edit could take &gt;20 minutes to process. An expedited (and safer) approximation of this edit would be to simply adjust the timezone by the GMT offset for Guam (UTC + 10 hours). edit_1004_gmt10 &lt;- list(das_file = das_file, type = &#39;function&#39;, rows = 433327:437665, chars = 6:39, edit = &#39;function(x){das_time(x, tz_adjust = 10)$dt}&#39;) dase &lt;- das_editor(list(edit_1004_gmt10)) Applying edits to DAS file: data/surveys/CenPac1986-2020_Final_alb.das --- verbatim/function text replacements ... --- --- working on edit 1 ... dase$das[[1]]$das$das[433326:433330] [1] &quot;229* 181728 020510 N13:14.49 E145:00.34&quot; [2] &quot;001* 061109 042010 N13:35.01 E145:59.35&quot; [3] &quot;002* 061309 042010 N13:35.03 E145:59.42&quot; [4] &quot;003* 061509 042010 N13:35.08 E145:59.54&quot; [5] &quot;004* 061709 042010 N13:35.13 E145:59.70&quot; dase$das[[1]]$das$das[437664:437667] [1] &quot;431* 144625 050410 N21:15.78 W158:53.32&quot; [2] &quot;432* 144825 050410 N21:16.05 W158:53.31&quot; [3] &quot;001* 144327 080410 N32:39.39 W117:13.60&quot; [4] &quot;002* 144827 080410 N32:38.58 W117:13.49&quot; Combining and saving edits Finally, we will collect these edits into a single list and save them for use during survey processing (next page). # Combine edits &lt;- list(edit_1607_55, edit_1607_68, edit_1621_245, #edit_1004_utc, edit_1004_gmt10) # Save saveRDS(edits,file=&#39;cnp_1986_2020_edits.RData&#39;) "],["processing.html", " 3 Data processing Behind the scenes Review Validation", " 3 Data processing In our WHICEAS case study example, we are interested in estimating density/abundance for 2017 and 2020 only, but we want to use surveys from previous years to help model species detection functions. We will therefore be using a dataset of NOAA Fisheries surveys in the Central North Pacific from 1986 to 2020. # Local path to DAS file das_file &lt;- &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39; To follow along, this data file can be downloaded here. You can process your survey data using a single function, process_surveys(), which takes two primary arguments: the filepath(s) to your DAS survey data, and your settings object. For example: cruz &lt;- process_surveys(das_file, settings) That single command will convert your raw DAS data to a “cruz” object, a list of polished datasets that are prepared to be passed to subsequent analyses. In our case we will use a third argument to apply edits to the DAS data before processing (see previous page for details on those edits): edits &lt;- readRDS(&#39;cnp_1986_2020_edits.RData&#39;) cruz &lt;- process_surveys(das_file, settings, edits) Behind the scenes The process_surveys() function is a wrapper for several discrete stages of data formatting/processing. Behind the scenes, each of those stages is carried out using a specific LTabundR function. The remainder of this page is a detailed step-by-step explanation of the data processing that occurs when you call process_surveys(). Edit cruise data If the edits input argument is supplied to process_surveys(), temporary copies of the DAS file(s) are made and edited before processing. This step is discussed on the previous page. Bring in cruise data Read in and process your .DAS file using the functions in Sam Woodward’s swfscDAS package. To do so quickly, we built a wrapper function that makes this quick and easy: das &lt;- das_load(das_file, perform_checks = TRUE, print_glimpse = TRUE) Interpolate cruise data Run the following function if you wish to interpolate the DAS data to a more frequent time interval, which can be useful for stratum assignment and effort calculations in some circumstances. This is only done in process_surveys() if the settings object passed to it has the interpolate input for load_survey_settings() set to a number (indicating the desired interval in seconds). In this example the interval is set to 120 seconds. Interpolation will only occur for On-Effort rows. das &lt;- das_interpolate(das_file, new_interval = 120) Process strata Run the following function to add strata and study-area information to each row of DAS data: das_strata &lt;- process_strata(das, settings) This function loops through each stratum data.frame you have provided it in settings$strata, formats the stratum, and asks whether each DAS row occurs within it. For each stratum, a column named stratum_&lt;StratumName&gt; is added to the das object; each row in this column is TRUE (included) or FALSE. Format DAS data into a cruz object The function das_format() takes care of some final formatting and initiates the cruz object data structure. cruz &lt;- das_format(das_strata, verbose=TRUE) This function (1) removes rows with invalid Cruise numbers, times, or locations; (ii) calculates the distance, in km, between each row of data; (iii) adds a ship column to the dataset, with initials for the ship corresponding to each cruise; (iv) creates a new list, cohorts, which copies the cruise data for each cohort specified in your settings; and (v) adds a stratum column to the data in each cohort. That column specifies a single stratum assignment for each row of DAS data in the event of overlapping strata, based upon the cohort setting stratum_overlap_handling. The cruz object The function das_format() returns a list, which we have saved in an object named cruz, with several slots: cruz %&gt;% names [1] &quot;settings&quot; &quot;strata&quot; &quot;cohorts&quot; The slots strata and study_area provide the area, in square km, of each polygon being used: cruz$strata stratum area 1 HI_EEZ 2474595.762 [km^2] 2 OtherCNP 33817779.065 [km^2] 3 MHI 212033.063 [km^2] 4 WHICEAS 402948.734 [km^2] 5 Spotted_OU 5102.666 [km^2] 6 Spotted_FI 10509.869 [km^2] 7 Spotted_BI 39454.720 [km^2] 8 Bottlenose_KaNi 2755.024 [km^2] 9 Bottlenose_OUFI 14417.027 [km^2] 10 Bottlenose_BI 4668.072 [km^2] 11 NWHI 449375.569 [km^2] The slot cohorts is itself a list with one slot for each cohort. The slots are named using the id cohort setting. cruz$cohorts %&gt;% names [1] &quot;all&quot; &quot;bottlenose&quot; &quot;spotted&quot; Each cohort slot has a copy of the DAS data with a new stratum column, which contains a stratum assignment tailored to its cohort-specific settings. For instance, the all cohort, whose stratum_overlap_handling is set to \"smallest\", assigns the smallest stratum in the event of overlapping or nested strata: cruz$cohorts$all$stratum %&gt;% table(useNA=&#39;ifany&#39;) . HI_EEZ OtherCNP WHICEAS 117715 126248 85669 Since the bottlenose cohort uses a different subset of geostrata, its distribution of stratum assignments will also differ: cruz$cohorts$bottlenose$stratum %&gt;% table(useNA=&#39;ifany&#39;) . Bottlenose_BI Bottlenose_KaNi Bottlenose_OUFI HI_EEZ OtherCNP 3415 1495 6862 117715 126248 WHICEAS 73897 This list, with these three primary slots, will be referred to from hereon as a cruz object. Segmentize the data To allocate survey data into discrete ‘effort segments’, which are used in variance estimation in subsequent steps, run the function segmentize(). This process is controlled by both survey-wide and cohort-specific settings, which are now carried in a slot within the cruz object. The process is outlined in detail in the Appendix on Segmentizing. cruz &lt;- segmentize(cruz, verbose=FALSE) This function does not change the high-level structure of the cruz object … cruz %&gt;% names [1] &quot;settings&quot; &quot;strata&quot; &quot;cohorts&quot; … or the cohort names in the cohorts slot: cruz$cohorts %&gt;% names [1] &quot;all&quot; &quot;bottlenose&quot; &quot;spotted&quot; For each cohorts slot, the list structure is the same: cruz$cohorts$all %&gt;% names [1] &quot;segments&quot; &quot;das&quot; cruz$cohorts$bottlenose %&gt;% names [1] &quot;segments&quot; &quot;das&quot; cruz$cohorts$spotted %&gt;% names [1] &quot;segments&quot; &quot;das&quot; The segments slot contains summary data for each effort segment, including start/mid/end coordinates, average conditions, and segment distance: cruz$cohorts$all$segments %&gt;% glimpse Rows: 1,901 Columns: 39 $ seg_id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… $ Cruise &lt;dbl&gt; 1001, 1004, 1004, 1004, 1004, 1004, 1621, 1621, 1621, 162… $ ship &lt;chr&gt; &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;DSJ&quot;, &quot;DSJ&quot;, &quot;… $ stratum &lt;chr&gt; &quot;HI_EEZ&quot;, &quot;OtherCNP&quot;, &quot;OtherCNP&quot;, &quot;OtherCNP&quot;, &quot;OtherCNP&quot;,… $ use &lt;chr&gt; &quot;FALSE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;FALSE&quot;,… $ Mode &lt;chr&gt; &quot;P-NA&quot;, &quot;P-NA-C&quot;, &quot;C-NA&quot;, &quot;NA-C&quot;, &quot;C-NA&quot;, &quot;C-NA-P&quot;, &quot;NA-C… $ EffType &lt;chr&gt; &quot;S-NA&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;NA-S&quot;, &quot;NA-S&quot;, &quot;S-NA&quot;, … $ OnEffort &lt;chr&gt; &quot;FALSE-TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;FA… $ ESWsides &lt;dbl&gt; 2, 2, 2, 2, 2, 2, NA, NA, 2, NA, 2, 2, 2, 2, NA, 2, NA, 2… $ dist &lt;dbl&gt; 19.807985, 120.822226, 149.909677, 150.429378, 152.495006… $ minutes &lt;dbl&gt; NA, NA, 560.233, 1086.850, 1193.383, 1445.617, NA, 6348.5… $ n_rows &lt;int&gt; 69, 121, 162, 106, 139, 151, 9, 39, 102, 237, 305, 236, 1… $ min_line &lt;int&gt; 429982, 435498, 435757, 436034, 436146, 436394, 258079, 2… $ max_line &lt;int&gt; 430354, 435756, 436033, 436145, 436393, 436737, 258110, 2… $ year &lt;dbl&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2002, 2002, 2002, 200… $ month &lt;dbl&gt; 1, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 10,… $ day &lt;int&gt; 21, 27, 28, 29, 29, 30, 6, 10, 12, 25, 27, 31, 3, 6, 10, … $ lat1 &lt;dbl&gt; 21.36650, 20.35267, 20.73317, 20.87850, 21.12333, 21.2966… $ lon1 &lt;dbl&gt; -161.6093, 175.0068, 178.7612, -179.5383, -176.0272, -172… $ DateTime1 &lt;dttm&gt; 2010-01-21 13:30:30, 2010-04-27 21:31:11, 2010-04-28 19:… $ timestamp1 &lt;dbl&gt; 1264080630, 1272403871, 1272482320, 1272516029, 127258136… $ yday1 &lt;dbl&gt; 21, 117, 118, 119, 119, 120, 218, 222, 224, 237, 239, 243… $ lat2 &lt;dbl&gt; 21.40017, 20.73217, 20.87633, 21.12217, 21.29600, 21.3991… $ lon2 &lt;dbl&gt; -168.8445, 178.7402, -179.5642, -176.0593, -172.5143, -16… $ DateTime2 &lt;dttm&gt; 2010-01-23 07:53:33, 2010-04-28 19:12:24, 2010-04-29 04:… $ timestamp2 &lt;dbl&gt; 1264233213, 1272481944, 1272515558, 1272580769, 127265237… $ yday2 &lt;dbl&gt; 23, 118, 119, 119, 120, 121, 218, 222, 226, 239, 243, 246… $ mlat &lt;dbl&gt; 21.37650, 20.45333, 20.79733, 21.08483, 21.16550, 21.3325… $ mlon &lt;dbl&gt; -162.0307, 175.9282, 179.4958, -176.6600, -175.2862, -171… $ mDateTime &lt;dttm&gt; 2010-01-21 15:43:13, 2010-04-28 02:31:13, 2010-04-28 23:… $ mtimestamp &lt;dbl&gt; 1264088593, 1272421873, 1272497711, 1272569369, 127259658… $ avgBft &lt;dbl&gt; 4.559338, 3.669203, 2.307485, 2.654979, 2.309207, 2.77247… $ avgSwellHght &lt;dbl&gt; 5.937330, 4.000000, 4.282085, 3.246260, 3.188012, 4.20871… $ avgHorizSun &lt;dbl&gt; 8.648165, 7.434095, 6.823564, 10.173233, 7.616618, 9.5735… $ avgVertSun &lt;dbl&gt; 0.9528752, 1.4683896, 2.6529293, 2.2939487, 2.4556125, 1.… $ avgGlare &lt;dbl&gt; 0.00000000, 0.43257043, 0.25509577, 0.73320199, 0.1841997… $ avgVis &lt;dbl&gt; 5.987837, 5.134704, 5.304659, 6.946100, 6.921997, 6.51393… $ avgCourse &lt;dbl&gt; 269.36547, 85.71532, 84.85097, 86.34360, 87.06384, 87.948… $ avgSpdKt &lt;dbl&gt; 10.362816, 10.549272, 10.986455, 10.899932, 11.051168, 10… # Number of segments cruz$cohorts$all$segments %&gt;% nrow [1] 1901 # Segment length distribution x &lt;- cruz$cohorts$all$segments$dist hist(x, breaks = seq(0,ceiling(max(x, na.rm=TRUE)),by=1), xlab=&#39;Segment lengths (km)&#39;, main=paste0(&#39;Target km: &#39;,settings$survey$segment_target_km)) And the das slot holds the original data.frame of DAS data, modified slightly: the column OnEffort has been modified according to Beaufort range conditions, and the column seg_id indicates which segment the event occurs within. cruz$cohorts$all$das %&gt;% names [1] &quot;Event&quot; &quot;DateTime&quot; &quot;Lat&quot; &quot;Lon&quot; [5] &quot;OnEffort&quot; &quot;Cruise&quot; &quot;Mode&quot; &quot;OffsetGMT&quot; [9] &quot;EffType&quot; &quot;ESWsides&quot; &quot;Course&quot; &quot;SpdKt&quot; [13] &quot;Bft&quot; &quot;SwellHght&quot; &quot;WindSpdKt&quot; &quot;RainFog&quot; [17] &quot;HorizSun&quot; &quot;VertSun&quot; &quot;Glare&quot; &quot;Vis&quot; [21] &quot;ObsL&quot; &quot;Rec&quot; &quot;ObsR&quot; &quot;ObsInd&quot; [25] &quot;Data1&quot; &quot;Data2&quot; &quot;Data3&quot; &quot;Data4&quot; [29] &quot;Data5&quot; &quot;Data6&quot; &quot;Data7&quot; &quot;Data8&quot; [33] &quot;Data9&quot; &quot;Data10&quot; &quot;Data11&quot; &quot;Data12&quot; [37] &quot;EffortDot&quot; &quot;EventNum&quot; &quot;file_das&quot; &quot;line_num&quot; [41] &quot;stratum_HI_EEZ&quot; &quot;stratum_OtherCNP&quot; &quot;stratum_WHICEAS&quot; &quot;year&quot; [45] &quot;month&quot; &quot;day&quot; &quot;yday&quot; &quot;km_int&quot; [49] &quot;km_cum&quot; &quot;ship&quot; &quot;stratum&quot; &quot;use&quot; [53] &quot;eff_bloc&quot; &quot;seg_id&quot; The segmentize() function and its associated settings were designed to give researchers full control over how data are segmented, be it for design-based density analysis (which tend to use long segments of 100 km or more and allow for non-contiguous effort to be included in the same segment) or for habitat modeling (which tend to use short segments of 5 - 10 km and disallow non-contiguous effort to be pooled into the same segment). To demonstrate that versatility, checkout the appendix on segmentizing. Process sightings To process sightings for each cohort of species, use the function process_sightings(). This function has three basic steps: for each cohort, the function (1) prepares a sightings table using the function das_sight() from swfscDAS; (2) filters those sightings to species codes specified for the cohort in your settings input; and (3) evaluates each of those sightings, asking if each should be included in the analysis according to your settings. cruz &lt;- process_sightings(cruz) The function produces a formatted dataset and adds it to a new sightings slot. cruz$cohorts$all %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; cruz$cohorts$bottlenose %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; cruz$cohorts$spotted %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; Note that the sightings table has a column named included (TRUE = yes, use it in the analysis). Any sightings that do not meet the inclusion criteria as specified in your settings will be included = FALSE, but they won’t be removed from the data. The sightings table also has a new column, ss_valid, indicating whether or not the group size estimate for this sighting is valid and appropriate for use in abundance estimation and detection function fitting when group size is used as a covariate. Since the sightings in each cohort are processed slightly differently according to the cohort’s specific settings – most importantly the species that will be included – you should expect different numbers of included/excluded sightings in each cohort dataset: cruz$cohorts$all$sightings$included %&gt;% table . FALSE TRUE 811 3123 cruz$cohorts$bottlenose$sightings$included %&gt;% table . FALSE TRUE 114 409 When this function’s verbose argument is TRUE (the default), a message is printed each time a sighting does not meet the inclusion criteria. Sightings data structure The sightings table has many other variables: cruz$cohorts$all$sightings %&gt;% names [1] &quot;Event&quot; &quot;DateTime&quot; &quot;Lat&quot; &quot;Lon&quot; [5] &quot;OnEffort&quot; &quot;Cruise&quot; &quot;Mode&quot; &quot;OffsetGMT&quot; [9] &quot;EffType&quot; &quot;ESWsides&quot; &quot;Course&quot; &quot;SpdKt&quot; [13] &quot;Bft&quot; &quot;SwellHght&quot; &quot;WindSpdKt&quot; &quot;RainFog&quot; [17] &quot;HorizSun&quot; &quot;VertSun&quot; &quot;Glare&quot; &quot;Vis&quot; [21] &quot;ObsL&quot; &quot;Rec&quot; &quot;ObsR&quot; &quot;ObsInd&quot; [25] &quot;EffortDot&quot; &quot;EventNum&quot; &quot;file_das&quot; &quot;line_num&quot; [29] &quot;stratum_HI_EEZ&quot; &quot;stratum_OtherCNP&quot; &quot;stratum_WHICEAS&quot; &quot;year&quot; [33] &quot;month&quot; &quot;day&quot; &quot;yday&quot; &quot;km_int&quot; [37] &quot;km_cum&quot; &quot;ship&quot; &quot;stratum&quot; &quot;use&quot; [41] &quot;eff_bloc&quot; &quot;seg_id&quot; &quot;SightNo&quot; &quot;Subgroup&quot; [45] &quot;SightNoDaily&quot; &quot;Obs&quot; &quot;ObsStd&quot; &quot;Bearing&quot; [49] &quot;Reticle&quot; &quot;DistNm&quot; &quot;Cue&quot; &quot;Method&quot; [53] &quot;Photos&quot; &quot;Birds&quot; &quot;CalibSchool&quot; &quot;PhotosAerial&quot; [57] &quot;Biopsy&quot; &quot;CourseSchool&quot; &quot;TurtleSp&quot; &quot;TurtleGs&quot; [61] &quot;TurtleJFR&quot; &quot;TurtleAge&quot; &quot;TurtleCapt&quot; &quot;PinnipedSp&quot; [65] &quot;PinnipedGs&quot; &quot;BoatType&quot; &quot;BoatGs&quot; &quot;PerpDistKm&quot; [69] &quot;species&quot; &quot;best&quot; &quot;low&quot; &quot;high&quot; [73] &quot;prob&quot; &quot;mixed&quot; &quot;ss_tot&quot; &quot;lnsstot&quot; [77] &quot;ss_percent&quot; &quot;n_sp&quot; &quot;n_obs&quot; &quot;n_best&quot; [81] &quot;n_low&quot; &quot;n_high&quot; &quot;calibr&quot; &quot;ss_valid&quot; [85] &quot;mixed_max&quot; &quot;spp_max&quot; &quot;included&quot; Columns 42 onwards correspond to sightings information. Columns of note: species contains the species code. There is only one species-code per row (i.e, multi-species sightings have been expanded to multiple rows). best, low, and high contain the refined group size estimates, averaged across observers and calibrated according to the cohort’s settings specifications. For multi-species sightings, these numbers represent the number of individuals for the single species represented in the row (i.e., the original group size estimate has been scaled by the percentage attritbuted to this species). The columns following those group size estimates (prob through spp_max) detail how group sizes were estimated: prob indicates whether probable species codes were accepted; mixed indicates whether this species’ sighting is part of a mixed-species sighting; n_sp provides the number of species occurring in this sighitng; n_obs gives the number of observers who contributed group size estimates; n_best through n_high gives the number of valid group size estimates given; and calibr indicates whether or not calibration was attempted for this sighting based on the settings (see next section); mixed_max indicates whether this species was the most abundant in the sighting (if multi-species); spp_max indicates the species code for the most abundant species in the sighting (if multi-species). As explained above, the final column, included, indicates whether this species should be included in the analysis. Here is a glimpse of the data: cruz$cohorts$all$sightings %&gt;% glimpse Rows: 3,934 Columns: 87 $ Event &lt;chr&gt; &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;… $ DateTime &lt;dttm&gt; 1986-08-02 16:50:00, 1986-08-02 17:06:00, 1986-08-02… $ Lat &lt;dbl&gt; 21.3833333, 21.3500000, 21.3500000, 19.2500000, 19.25… $ Lon &lt;dbl&gt; -120.2833, -120.3000, -120.3000, -121.3500, -121.3500… $ OnEffort &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE… $ Cruise &lt;dbl&gt; 990, 990, 990, 990, 990, 990, 990, 990, 990, 990, 990… $ Mode &lt;chr&gt; &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;… $ OffsetGMT &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ EffType &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;S&quot;, &quot;S&quot;… $ ESWsides &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,… $ Course &lt;dbl&gt; 206, 206, 206, 193, 193, 193, 193, 193, 193, 193, 193… $ SpdKt &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1… $ Bft &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 3, 3, 5, 5, 5, 4, 5, 5,… $ SwellHght &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ WindSpdKt &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ RainFog &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1,… $ HorizSun &lt;dbl&gt; 2, 2, 2, 8, 8, 9, 9, 9, 12, 12, 3, 9, 9, 9, 9, NA, 12… $ VertSun &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 12, 12, 2, 2, 2, 2, 2, NA, 12… $ Glare &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… $ Vis &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ ObsL &lt;chr&gt; &quot;063&quot;, &quot;061&quot;, &quot;061&quot;, &quot;064&quot;, &quot;064&quot;, &quot;005&quot;, &quot;063&quot;, &quot;063… $ Rec &lt;chr&gt; &quot;038&quot;, &quot;063&quot;, &quot;063&quot;, &quot;055&quot;, &quot;055&quot;, &quot;064&quot;, &quot;061&quot;, &quot;061… $ ObsR &lt;chr&gt; &quot;061&quot;, &quot;038&quot;, &quot;038&quot;, &quot;005&quot;, &quot;005&quot;, &quot;055&quot;, &quot;038&quot;, &quot;038… $ ObsInd &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ EffortDot &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE… $ EventNum &lt;chr&gt; &quot;70&quot;, &quot;78&quot;, &quot;78&quot;, &quot;10&quot;, &quot;10&quot;, &quot;18&quot;, &quot;28&quot;, &quot;30&quot;, &quot;52&quot;,… $ file_das &lt;chr&gt; &quot;CenPac1986-2020_Final_alb.das&quot;, &quot;CenPac1986-2020_Fin… $ line_num &lt;int&gt; 11187, 11196, 11196, 11229, 11229, 11243, 11254, 1126… $ stratum_HI_EEZ &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… $ stratum_OtherCNP &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ stratum_WHICEAS &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… $ year &lt;dbl&gt; 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1986,… $ month &lt;dbl&gt; 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,… $ day &lt;int&gt; 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 5, 5, 5, 6, 6, 6,… $ yday &lt;dbl&gt; 214, 214, 214, 215, 215, 215, 215, 215, 215, 215, 215… $ km_int &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… $ km_cum &lt;dbl&gt; 1313.087, 1318.025, 1318.025, 1372.351, 1372.351, 139… $ ship &lt;chr&gt; &quot;MAC&quot;, &quot;MAC&quot;, &quot;MAC&quot;, &quot;MAC&quot;, &quot;MAC&quot;, &quot;MAC&quot;, &quot;MAC&quot;, &quot;MAC… $ stratum &lt;chr&gt; &quot;OtherCNP&quot;, &quot;OtherCNP&quot;, &quot;OtherCNP&quot;, &quot;OtherCNP&quot;, &quot;Othe… $ use &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE… $ eff_bloc &lt;chr&gt; &quot;192-0&quot;, &quot;193-0&quot;, &quot;193-0&quot;, &quot;193-0&quot;, &quot;193-0&quot;, &quot;193-0&quot;,… $ seg_id &lt;dbl&gt; 1256, 1264, 1264, 1264, 1264, 1264, 1264, 1264, 1264,… $ SightNo &lt;chr&gt; &quot;02&quot;, &quot;03&quot;, &quot;03&quot;, &quot;01&quot;, &quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;04&quot;, &quot;05&quot;,… $ Subgroup &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ SightNoDaily &lt;chr&gt; &quot;19860802_1&quot;, &quot;19860802_2&quot;, &quot;19860802_2&quot;, &quot;19860803_1… $ Obs &lt;chr&gt; &quot;061&quot;, &quot;038&quot;, &quot;038&quot;, &quot;005&quot;, &quot;005&quot;, &quot;005&quot;, &quot;063&quot;, &quot;038… $ ObsStd &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ Bearing &lt;dbl&gt; 92, 40, 40, 85, 85, 275, 330, 30, 5, 83, 335, 6, 25, … $ Reticle &lt;dbl&gt; 1.50, 0.50, 0.50, 1.00, 1.00, 11.40, 3.88, NA, 5.39, … $ DistNm &lt;dbl&gt; 1.7, 3.2, 3.2, 2.2, 2.2, 0.3, 0.8, 0.1, 0.6, 4.0, 2.2… $ Cue &lt;dbl&gt; 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 2, 2, 1, 1, 3, 3,… $ Method &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 4, 4, 4,… $ Photos &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ Birds &lt;chr&gt; &quot;N&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;… $ CalibSchool &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ PhotosAerial &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ Biopsy &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ CourseSchool &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleSp &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleGs &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleJFR &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleAge &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleCapt &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ PinnipedSp &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ PinnipedGs &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ BoatType &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ BoatGs &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ PerpDistKm &lt;dbl&gt; 3.14648208, 3.80941649, 3.80941649, 4.05889568, 4.058… $ species &lt;chr&gt; &quot;096&quot;, &quot;018&quot;, &quot;013&quot;, &quot;011&quot;, &quot;002&quot;, &quot;077&quot;, &quot;011&quot;, &quot;015… $ best &lt;dbl&gt; 2.318841, 7.335902, 52.144384, 32.755770, 54.360639, … $ low &lt;dbl&gt; 2.000000, 5.986599, 42.553392, 28.990837, 48.112454, … $ high &lt;dbl&gt; 2.000000, 8.909004, 63.326164, 41.310372, 68.557639, … $ prob &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… $ mixed &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, F… $ ss_tot &lt;dbl&gt; 2.318841, 59.480286, 59.480286, 87.116408, 87.116408,… $ lnsstot &lt;dbl&gt; 0.8410673, 4.0856449, 4.0856449, 4.4672453, 4.4672453… $ ss_percent &lt;dbl&gt; 1.0000000, 0.1233333, 0.8766667, 0.3760000, 0.6240000… $ n_sp &lt;dbl&gt; 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2,… $ n_obs &lt;int&gt; 1, 4, 4, 6, 6, 1, 6, 2, 2, 1, 4, 1, 2, 2, 4, 2, 1, 1,… $ n_best &lt;int&gt; 1, 3, 3, 5, 5, 1, 6, 2, 2, 1, 3, 1, 2, 2, 4, 2, 1, 1,… $ n_low &lt;int&gt; 1, 4, 4, 6, 6, 1, 6, 2, 2, 1, 4, 1, 2, 2, 4, 2, 1, 1,… $ n_high &lt;int&gt; 1, 3, 3, 5, 5, 1, 6, 2, 2, 1, 3, 1, 2, 2, 3, 1, 0, 0,… $ calibr &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ ss_valid &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ mixed_max &lt;lgl&gt; TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, TRU… $ spp_max &lt;chr&gt; &quot;096&quot;, &quot;013&quot;, &quot;013&quot;, &quot;002&quot;, &quot;002&quot;, &quot;077&quot;, &quot;011&quot;, &quot;015… $ included &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE… Note that the process_sightings() function draws upon cruz$settings for inclusion criteria, but some of those settings can be overridden with the function’s manual inputs if you want to explore your options (see below). Group size estimates In the settings we are using in this tutorial, group size estimates are adjusted using the calibration models from Barlow et al. (1998) (their analysis is refined slightly and further explained in Gerrodette et al. (2002)). These calibration corrections are observer-specific. Most observers tend to underestimate group size and their estimates are adjusted up; others tend to overestimate and their estimates are adjusted down. Some observers do not have calibration coefficients, and for them a generic adjustment (upwards, by dividing estimates by 0.8625) is used. In LTabundR, each observer’s estimate is calibrated, then all observer estimates are averaged. To do that averaging, our settings specify that we shall use a geometric weighted mean, instead of an arithmetic mean, that weights school size estimates from multiple observers according to the variance of their calibration coefficients. Here are our current best estimates of group size: cruz$cohorts$all$sightings$best %&gt;% head(20) [1] 2.318841 7.335902 52.144384 32.755770 54.360639 1.159420 [7] 70.537494 1.639668 1.159420 20.000000 123.627465 8.115942 [13] 12.197734 57.503605 7.292738 58.103051 10.434783 1.159420 [19] 1.159420 2.318841 Let’s compare those estimates to unadjusted ones, in which calibration (and therefore weighted geometric mean) is turned off: cruz_demo &lt;- process_sightings(cruz, calibrate = FALSE, verbose = FALSE) cruz_demo$cohorts$all$sightings$best %&gt;% head(20) [1] 2.000000 6.464495 45.950332 32.418210 53.800434 1.000000 58.833655 [8] 1.414214 1.000000 20.000000 74.888724 7.000000 11.739357 55.342682 [15] 5.732657 65.192024 9.000000 1.000000 1.000000 2.000000 You can also carry out calibration corrections without using a geometric weighted mean (the arithmetic mean will be used instead): cruz_demo &lt;- process_sightings(cruz, calibrate = TRUE, geometric_mean = FALSE, verbose = FALSE) cruz_demo$cohorts$all$sightings$best %&gt;% head(20) [1] 2.318841 7.382126 52.472947 37.131362 61.622261 1.159420 [7] 72.484207 1.739130 1.159420 20.000000 122.857307 8.115942 [13] 11.812500 55.687500 7.807971 74.275362 10.434783 1.159420 [19] 1.159420 2.318841 Note that when geometric_mean = TRUE but calibration is not carried out, the simple geometric mean is calculated instead of the weighted geometric mean, since the weights are the variance estimates from the calibration routine. Also note that group size calibration is only carried out if settings$group_size_calibration is not NULL. However, even when calibration coefficients are provided, it is possible to specify that calibration should only be carried out for raw estimates above a minimum threshold (see cohort setting calibration_floor, whose default is 0), since observers may be unlikely to mis-estimate the group size of a lone whale or pair. For observers who have calibration coefficients in the settings$group_size_coefficients table, that minimum is specified for each observer individually. For observers not in that table, calibration will only be applied to raw group size estimates above settings$cohorts[[i]]$calibration_floor or above. Process subgroups After sightings data are processed, the process_surveys() function calls the subroutine process_subgroups() to find and calculate subgroup group size estimates for false killer whales (or other species that may have been recorded using the subgroup functionality in WinCruz), if any occur in the DAS data (Event code “G”). cruz &lt;- process_subgroups(cruz) If subgroups are found, a subgroups slot is added to the analysis list for a cohort. cruz$cohorts$all %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; &quot;subgroups&quot; This subgroups slot holds a list with three dataframes: cruz$cohorts$all$subgroups %&gt;% names [1] &quot;sightings&quot; &quot;subgroups&quot; &quot;events&quot; $events (each row is a group size estimate for a single subgroup during a single phase of the false killer whale protocol (if applicable) within a single sighting; this is effectively the raw data). Internally, LTabundR uses the function subgroup_events() to produce this data.frame. $subgroups (each row is a single subgroup for a single protocol phase, with all group size estimates averaged together (both arithmetically and geometrically). Internally, LTabundR uses the function subgroup_subgroups() to produce this data.frame. sightings (each row is a “group” size estimate for a single sighting during a single phase protocol, with all subgroup group sizes summed together). Note for false killer whales this “group” size estimate is not likely to represent actual group size because groups can be spread out over tens of kilometers, and it is not expected that every subgroup is detected during each protocol phase. Internally, LTabundR uses the function subgroup_sightings() to produce this data.frame. For a detailed example of how subgroup data are analyzed, see the vignette page on subgroup analysis using data on Central North Pacific false killer whales. Subgroup phase assignments Currently, the protocol phase for each event is automatically determined simply according to the column OnEffort: if TRUE, the column Phase is 1; if FALSE, it is 2. Since this assignment routine is simplistic, users may find the need to manually revise some of these assignments. To facilitate that work, LTabundR offers the function subgroup_phases(), which will launch an interactive Shiny app that allows users to review phase assignments and manually stage a set of revisions. Here is a step-by-step example of what this process would look like: First, process your DAS data as usual using the process_surveys() function. The resulting cruz object will contain a subgroups slot for cohorts with G events in the DAS data: cruz$cohorts$all$subgroups %&gt;% names [1] &quot;sightings&quot; &quot;subgroups&quot; &quot;events&quot; Second, manually review the phase assignments for subgroup events in your cohort of interest using the function subgroup_phases(): edits &lt;- subgroup_phases(cruz, cohort = &#39;all&#39;) This function will launch a Shiny app that looks like this: The app provides instructions at the top of its screen. In short, you select the rows of data whose Phase column needs to be revised. Below the table, you specify what the revised Phase should be, and a button will appear (not shown above) to add this revision to a list of staged edits. You can repeat this process as many times as you need as you review the data. If you make a mistake, you can select the staged edit that you need to delete and a button will appear allowing you to delete it. By the end of this processed, you will have a list of staged edits that you will want to save. A button will be available in the top right of the screen (not shown above) that allows you to close the app and return the staged edits as a data.frame. We capture that output in the object edits in our code above. Those staged edits will look something like this: # Example of `edits` output edits cohort Cruise Species Line SubGrp Event old_phase new_phase 1 all 1108 033 480021 A 1 1 2 2 all 1108 033 480034 B 1 1 2 If you need to repeat step 2, just save your edits into new objects (e.g., edits2, edits3, etc.), then bind them together with rbind() as you would any set of data.frame’s with identifcal column names. Third, now you want to update your cruz object with these edits. To do so, run the LTabundR function process_subgroups() and include your edits as an input: cruz &lt;- process_subgroups(cruz, phase_edits = edits) This will essentially re-do the subgroup processing phase of process_surveys() but this time will take into account your phase edits. Review By the end of this process, you have a single data object, cruz, with all the data you need to move forward into the next stages of mapping and analysis. The LTabundR function cruz_structure() provides a synopsis of the data structure: cruz_structure(cruz) &quot;cruz&quot; list structure ======================== $settings $strata --- with 11 polygon coordinate sets $survey --- with 11 input arguments $cohorts --- with 3 cohorts specified, each with 19 input arguments $strata ... containing a summary dataframe of 11 geostrata and their spatial areas ... geostratum names: HI_EEZ, OtherCNP, MHI, WHICEAS, Spotted_OU, Spotted_FI, Spotted_BI, Bottlenose_KaNi, Bottlenose_OUFI, Bottlenose_BI, NWHI $cohorts $all geostrata: WHICEAS, HI_EEZ, OtherCNP $segments --- with 1900 segments (median = 149.2 km) $das --- with 329638 data rows $sightings --- with 3934 detections $subgroups --- with 255 subgroups, 62 sightings, and 389 events $bottlenose geostrata: WHICEAS, HI_EEZ, OtherCNP, Bottlenose_BI, Bottlenose_OUFI, Bottlenose_KaNi $segments --- with 2063 segments (median = 148.5 km) $das --- with 329638 data rows $sightings --- with 523 detections $spotted geostrata: WHICEAS, HI_EEZ, OtherCNP, Spotted_OU, Spotted_FI, Spotted_BI $segments --- with 2072 segments (median = 148.4 km) $das --- with 329638 data rows $sightings --- with 527 detections Each species-specific cohort has its own list under cruz$cohorts, and each of these cohorts has the same list structure: segments is a summary table of segments. das is the raw DAS data, modified with seg_id to associate each row with a segment. sightings is a dataframe of sightings processed according to this cohort’s settings. subgroups (if any subgroup data exist in your survey) is a list with subgroup details. In each cohort data.frame, there are three critically important columns to keep in mind: seg_id: this column is used to indicate the segment ID that a row of data belongs to. use: this column indicates whether a row of effort should be used in the line-transect analysis. Every row of data within a single segment will have the same use value. included: this column occurs in the sightings dataframe only. It indicates whether the sightings should be included in line-transect analysis based on the specified settings. Any sighting with use == FALSE will also have included == FALSE, but it is possible for sightings to have use == TRUE with included == FALSE. For example, if the setting abeam_sightings is set to FALSE, a sighting with a bearing angle beyond the ship’s beam can be excluded from the analysis (included == FALSE) even though the effort segment it occurs within will still be used (use == TRUE). Finally, let’s save this cruz object locally, to use in downstream scripts: save(cruz, file=&#39;whiceas_cruz.RData&#39;) Validation To validate these LTabundR functions, we can compare its output to that of ABUND9, written by Jay Barlow (NOAA Fisheries). First, we bring in the ABUND9 output files for the same DAS data: # Local paths to these files SIGHTS &lt;- read.csv(&#39;data/SIGHTS.csv&#39;) EFFORT &lt;- read.csv(&#39;data/EFFORT.csv&#39;) You may download these files here: SIGHTS.csv and EFFORT.csv. Sightings Pivot and format the ABUND SIGHTS data… abund &lt;- SIGHTS %&gt;% tidyr::pivot_longer(cols = 31:101, names_to = &#39;species&#39;, values_to = &#39;best&#39;) %&gt;% filter(best &gt; 0) %&gt;% mutate(Region = gsub(&#39; &#39;,&#39;&#39;,Region)) %&gt;% mutate(DateTime = paste0(Yr,&#39;-&#39;,Mo,&#39;-&#39;,Da,&#39; &#39;,Hr,&#39;:&#39;,Min)) …then summarize counts of species within each cruise: abund_summ &lt;- abund %&gt;% group_by(cruise = CruzNo, species) %&gt;% summarize(ntot_abund = n(), nsys_abund = length(which(! Region %in% c(&#39;NONE&#39;, &#39;Off-Transect&#39;) &amp; EffortSeg &gt; 0))) %&gt;% mutate(species = gsub(&#39;SP&#39;,&#39;&#39;,species)) Then do the same for LTabundR output: ltabundr &lt;- cruz$cohorts$all$sightings %&gt;% # Filter out species that ABUND ignored based on its INP file filter(!species %in% c(&#39;CU&#39;, &#39;PU&#39;)) ltabundr_summ &lt;- ltabundr %&gt;% filter(OnEffort == TRUE) %&gt;% group_by(cruise = Cruise, species) %&gt;% summarize(ntot_ltabundr = n(), nsys_ltabundr = length(which(included == TRUE &amp; EffType %in% c(&#39;S&#39;,&#39;F&#39;)))) Now join these two datasets by cruise and species code: mr &lt;- full_join(abund_summ, ltabundr_summ, by=c(&#39;cruise&#39;, &#39;species&#39;)) mr %&gt;% head # A tibble: 6 × 6 # Groups: cruise [1] cruise species ntot_abund nsys_abund ntot_ltabundr nsys_ltabundr &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 901 002 3 2 3 2 2 901 013 4 4 4 4 3 901 015 2 1 2 1 4 901 018 2 1 2 1 5 901 031 1 1 1 1 6 901 032 2 0 2 0 Compare the total On-Effort sightings in both outputs: mr$ntot_abund %&gt;% sum(na.rm=TRUE) [1] 3223 mr$ntot_ltabundr %&gt;% sum(na.rm=TRUE) [1] 3227 Compare total sightings valid for use in density estimation (EffType \"S\" or \"F\" only, as well as other criteria such as Bft 0 - 6): mr$nsys_abund %&gt;% sum(na.rm=TRUE) [1] 2478 mr$nsys_ltabundr %&gt;% sum(na.rm=TRUE) [1] 2478 Let’s find the rows with discrepancies in sighting counts: bads &lt;- which(mr$nsys_abund != mr$nsys_ltabundr | mr$ntot_abund != mr$ntot_ltabundr | is.na(mr$ntot_abund) | is.na(mr$ntot_ltabundr) | is.na(mr$nsys_abund) | is.na(mr$nsys_ltabundr)) bads %&gt;% length [1] 5 Let’s look at those rows in the joined dataframe: mr[bads, ] # A tibble: 5 × 6 # Groups: cruise [4] cruise species ntot_abund nsys_abund ntot_ltabundr nsys_ltabundr &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 1203 015 1 0 2 0 2 1203 049 1 1 2 1 3 1621 073 5 4 5 3 4 1165 047 NA NA 1 1 5 1631 003 NA NA 1 0 To investigate these 5 discrepancies, we will write a helper function that returns sightings details from both outputs for a given cruise-species: sight_compare &lt;- function(abund, ltabundr, cruise, spp){ message(&#39;ABUND:&#39;) abund %&gt;% filter(CruzNo == cruise, species == paste0(&#39;SP&#39;,spp)) %&gt;% select(5, 34, 26, 29, 33, 3) %&gt;% mutate(use_sit = EffortSeg != 0) %&gt;% select(-EffortSeg) %&gt;% arrange(desc(use_sit)) %&gt;% print abund %&gt;% filter(CruzNo == 1631) %&gt;% pull(species) %&gt;% table message(&#39;\\nLTabundR:&#39;) ltabundr %&gt;% filter(Cruise == cruise, species == spp) %&gt;% select(6, 2, 13, 74, 70, 5, 9, 40, 87) %&gt;% rename(use_sit = included, use_effort = use) %&gt;% arrange(desc(use_effort)) %&gt;% tibble %&gt;% print } Discrepancies Cruise 1203, Species 015 In this case, LTabundR has a non-systematic sighting that ABUND has ignored. sight_compare(abund, ltabundr, 1203, &#39;015&#39;) ABUND: # A tibble: 1 × 6 CruzNo DateTime Beauf Mixed best use_sit &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; 1 1203 2012-5-16 11:58 4 &quot; F&quot; 74.9 TRUE LTabundR: # A tibble: 2 × 9 Cruise DateTime Bft mixed best OnEffort EffType use_effort &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;lgl&gt; 1 1203 2012-05-13 13:19:44 5 TRUE 1 TRUE N TRUE 2 1203 2012-05-16 11:58:30 4 FALSE 74.9 TRUE N TRUE # ℹ 1 more variable: use_sit &lt;lgl&gt; Looking at the sighting details from LTabundR … (ltabundr %&gt;% filter(Cruise == 1203, species == &#39;015&#39;))[1,] Event DateTime Lat Lon OnEffort Cruise Mode OffsetGMT 1 S 2012-05-13 13:19:44 11.96733 -161.1727 TRUE 1203 C -10 EffType ESWsides Course SpdKt Bft SwellHght WindSpdKt RainFog HorizSun 1 N 2 18 9.2 5 6 21 5 12 VertSun Glare Vis ObsL Rec ObsR ObsInd EffortDot EventNum 1 12 FALSE 6 073 235 280 &lt;NA&gt; TRUE 268 file_das line_num stratum_HI_EEZ stratum_OtherCNP 1 CenPac1986-2020_Final_alb_edited.das 493027 FALSE TRUE stratum_WHICEAS year month day yday km_int km_cum ship stratum use 1 FALSE 2012 5 13 134 0 171168.7 OES OtherCNP TRUE eff_bloc seg_id SightNo Subgroup SightNoDaily Obs ObsStd Bearing Reticle 1 37-2 1416 069 &lt;NA&gt; 20120513_123 235 TRUE 90 NA DistNm Cue Method Photos Birds CalibSchool PhotosAerial Biopsy CourseSchool 1 0.1 3 1 Y Y N N N NA TurtleSp TurtleGs TurtleJFR TurtleAge TurtleCapt PinnipedSp PinnipedGs 1 &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA BoatType BoatGs PerpDistKm species best low high prob mixed ss_tot lnsstot 1 &lt;NA&gt; NA 0.1852 015 1 NaN NaN FALSE TRUE 6.956522 1.93968 ss_percent n_sp n_obs n_best n_low n_high calibr ss_valid mixed_max spp_max 1 NaN 2 1 1 1 1 TRUE FALSE FALSE &lt;NA&gt; included 1 TRUE According to ABUND, this sighting is not mixed-species, but LTabundR says it is. Looking at the raw DAS… das_file &lt;- &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39; das &lt;- das_readtext(das_file) i &lt;- which(substr(das$das, 6, 18) == &#39;131944 051312&#39;) das$das[i] [1] &quot;268S.131944 051312 N11:58.04 W161:10.36 069 235 3 1 090 0.10 N N N&quot; [2] &quot;269A.131944 051312 N11:58.04 W161:10.36 069 Y Y 033 015 &quot; [3] &quot;269C.131944 051312 N11:58.04 W161:10.36 Overall estimate for full group- never saw all at once. -EMO&quot; [4] &quot;268G.131944 051312 N11:58.04 W161:10.36 069 A 235 1 090 0.10 &quot; [5] &quot;269A.131944 051312 N11:58.04 W161:10.36 069 Y Y 033 015&quot; [6] &quot;269C.131944 051312 N11:58.04 W161:10.36 begin PC protool, first sighting is subgroup &#39;A&#39;, acoustics already tracking &quot; [7] &quot;269C.131944 051312 N11:58.04 W161:10.36 photos taken during sighting indicate Steno (015) present. Not seen or estimated during sighting. -EMO&quot; We see that this was a sighting of false killer whales during which species 015 was picked up during photo-ID. In the absence of a percent composition estimate for this sighting, it was ignored by ABUND. Cruise 1203, Species 049 In this case, LTabundR has a non-systematic sighting that ABUND has ignored. sight_compare(abund, ltabundr, 1203, &#39;049&#39;) ABUND: # A tibble: 1 × 6 CruzNo DateTime Beauf Mixed best use_sit &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; 1 1203 2012-5-3 14:21 6 &quot; F&quot; 1.16 TRUE LTabundR: # A tibble: 2 × 9 Cruise DateTime Bft mixed best OnEffort EffType use_effort &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;lgl&gt; 1 1203 2012-05-03 14:21:40 6 FALSE 1.16 TRUE S TRUE 2 1203 2012-05-07 10:54:09 7 FALSE 1 TRUE N FALSE # ℹ 1 more variable: use_sit &lt;lgl&gt; Looking at the sighting details from LTabundR … (ltabundr %&gt;% filter(Cruise == 1203, species == &#39;049&#39;))[2,] Event DateTime Lat Lon OnEffort Cruise Mode OffsetGMT 2 S 2012-05-07 10:54:09 5.960333 -162.1255 TRUE 1203 C -10 EffType ESWsides Course SpdKt Bft SwellHght WindSpdKt RainFog HorizSun 2 N 2 269 8.8 7 9 30 5 5 VertSun Glare Vis ObsL Rec ObsR ObsInd EffortDot EventNum 2 1 FALSE 4.5 238 328 073 &lt;NA&gt; TRUE 145 file_das line_num stratum_HI_EEZ stratum_OtherCNP 2 CenPac1986-2020_Final_alb_edited.das 489960 FALSE TRUE stratum_WHICEAS year month day yday km_int km_cum ship stratum use 2 FALSE 2012 5 7 128 0 170067.7 OES OtherCNP FALSE eff_bloc seg_id SightNo Subgroup SightNoDaily Obs ObsStd Bearing Reticle 2 36-0 1409 052 &lt;NA&gt; 20120507_103 238 TRUE 310 10 DistNm Cue Method Photos Birds CalibSchool PhotosAerial Biopsy CourseSchool 2 0.52 3 4 N N N N N NA TurtleSp TurtleGs TurtleJFR TurtleAge TurtleCapt PinnipedSp PinnipedGs 2 &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA BoatType BoatGs PerpDistKm species best low high prob mixed ss_tot lnsstot 2 &lt;NA&gt; NA 0.7377314 049 1 1 1 FALSE FALSE 1 0 ss_percent n_sp n_obs n_best n_low n_high calibr ss_valid mixed_max spp_max 2 1 1 1 1 1 1 TRUE TRUE TRUE 049 included 2 FALSE This is a sighting of a single Ziphiid whale. It appears to be within the geostratum: cruzi &lt;- filter_cruz(cruz, spp=&#39;049&#39;, years = 1988) map_cruz(cruzi) Loooking at the raw DAS data … i &lt;- which(substr(das$das, 6, 18) == &#39;105409 050712&#39;) das$das[(i[1] - 10):(i[1] + 3)] [1] &quot;135V.104143 050712 N05:57.61 W162:05.66 7 09 070 30.0&quot; [2] &quot;136N.104143 050712 N05:57.61 W162:05.66 272 09.3&quot; [3] &quot;137W.104143 050712 N05:57.61 W162:05.66 5 05 01 050 4.5&quot; [4] &quot;138*.104234 050712 N05:57.62 W162:05.79&quot; [5] &quot;139*.104434 050712 N05:57.62 W162:06.09&quot; [6] &quot;140*.104634 050712 N05:57.63 W162:06.39&quot; [7] &quot;141N.104713 050712 N05:57.63 W162:06.49 269 08.8&quot; [8] &quot;142*.104834 050712 N05:57.63 W162:06.69&quot; [9] &quot;143*.105034 050712 N05:57.63 W162:06.99&quot; [10] &quot;144*.105234 050712 N05:57.63 W162:07.29&quot; [11] &quot;145S.105409 050712 N05:57.62 W162:07.53 052 238 3 4 310 10.0 0.52 N N N&quot; [12] &quot;146A.105409 050712 N05:57.62 W162:07.53 052 N N 049 &quot; [13] &quot; 1 238 1 1 1 100&quot; [14] &quot;147*.105434 050712 N05:57.62 W162:07.59&quot; It is not clear why ABUND did not include this sighting as non-systematic. Perhaps the ABUND geostratum calculations placed this sighting outside of the study area. Cruise 1165, Species 047 In this case, LTabundR has a systematic sighting of a pygmy sperm whale that ABUND has ignored. sight_compare(abund, ltabundr, 1165, &#39;047&#39;) ABUND: # A tibble: 0 × 6 # ℹ 6 variables: CruzNo &lt;int&gt;, DateTime &lt;chr&gt;, Beauf &lt;int&gt;, Mixed &lt;chr&gt;, # best &lt;dbl&gt;, use_sit &lt;lgl&gt; LTabundR: # A tibble: 1 × 9 Cruise DateTime Bft mixed best OnEffort EffType use_effort &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;lgl&gt; 1 1165 1988-07-30 19:28:00 0 FALSE 1.16 TRUE S TRUE # ℹ 1 more variable: use_sit &lt;lgl&gt; (ltabundr %&gt;% filter(Cruise == 1165, species == &#39;047&#39;))[1,] Event DateTime Lat Lon OnEffort Cruise Mode OffsetGMT 1 S 1988-07-30 19:28:00 26.3 -121.1167 TRUE 1165 C NA EffType ESWsides Course SpdKt Bft SwellHght WindSpdKt RainFog HorizSun 1 S 2 163 10.5 0 NA NA 1 NA VertSun Glare Vis ObsL Rec ObsR ObsInd EffortDot EventNum 1 NA NA NA 038 068 051 &lt;NA&gt; TRUE 92 file_das line_num stratum_HI_EEZ stratum_OtherCNP 1 CenPac1986-2020_Final_alb_edited.das 50173 FALSE TRUE stratum_WHICEAS year month day yday km_int km_cum ship stratum use 1 FALSE 1988 7 30 212 0 13618.39 MAC OtherCNP TRUE eff_bloc seg_id SightNo Subgroup SightNoDaily Obs ObsStd Bearing Reticle 1 33-0 1377 07 &lt;NA&gt; 19880730_9 051 TRUE 45 3.38 DistNm Cue Method Photos Birds CalibSchool PhotosAerial Biopsy CourseSchool 1 0.9 3 4 &lt;NA&gt; N &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA TurtleSp TurtleGs TurtleJFR TurtleAge TurtleCapt PinnipedSp PinnipedGs 1 &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA BoatType BoatGs PerpDistKm species best low high prob mixed ss_tot 1 &lt;NA&gt; NA 1.178606 047 1.15942 1 1.259921 FALSE FALSE 1.15942 lnsstot ss_percent n_sp n_obs n_best n_low n_high calibr ss_valid mixed_max 1 0.1479201 1 1 3 3 3 3 TRUE TRUE TRUE spp_max included 1 047 TRUE To investigate this sighting, we can filter our cruz object and take a look at a map of this sighting: cruzi &lt;- filter_cruz(cruz, spp=&#39;047&#39;, years = 2012) map_cruz(cruzi) Using that map we see that this sighting occurred just inside of the OtherCNP geostratum. It is likely that the point-in-polygon subroutines inside ABUND9 decided that this sighting was out of the study area, and therefore excluded it. The subroutines used by LTabundR, which are based in the R package sf, should not be wrong in this case. Cruise 1621-073 In this case, both LTabundR and ABUND logged the same total number of sightings, but ABUND determined that that one was not valid for for density estimation, whereas LTabundR determined that two of them were not valid. sight_compare(abund, ltabundr, 1621, &#39;073&#39;) ABUND: # A tibble: 5 × 6 CruzNo DateTime Beauf Mixed best use_sit &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; 1 1621 2002-11-8 14:19 3 &quot; F&quot; 3.48 TRUE 2 1621 2002-11-20 16:2 4 &quot; F&quot; 3.97 TRUE 3 1621 2002-11-22 10:27 4 &quot; F&quot; 4.96 TRUE 4 1621 2002-11-22 12:20 4 &quot; F&quot; 1.16 TRUE 5 1621 2002-11-29 13:48 5 &quot; F&quot; 2.32 TRUE LTabundR: # A tibble: 7 × 9 Cruise DateTime Bft mixed best OnEffort EffType use_effort &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;lgl&gt; 1 1621 2002-11-08 14:19:04 3 FALSE 3.48 TRUE S TRUE 2 1621 2002-11-20 16:02:58 4 FALSE 3.97 TRUE S TRUE 3 1621 2002-11-22 10:27:28 4 FALSE 4.96 TRUE S TRUE 4 1621 2002-11-22 12:20:33 4 FALSE 1.16 TRUE S TRUE 5 1621 2002-11-29 13:48:49 5 FALSE 2.32 TRUE N TRUE 6 1621 2002-11-20 12:59:22 4 FALSE 1.16 FALSE S FALSE 7 1621 2002-11-21 11:57:55 4 FALSE 1.16 FALSE S FALSE # ℹ 1 more variable: use_sit &lt;lgl&gt; The discrepancy is in the November 8, 2002 sighting at 14:19:04. Looking at the sighting details from LTabundR… (ltabundr %&gt;% filter(Cruise == 1621, species == &#39;073&#39;))[1,] Event DateTime Lat Lon OnEffort Cruise Mode OffsetGMT 1 S 2002-11-08 14:19:04 22.539 -171.7638 TRUE 1621 C 10 EffType ESWsides Course SpdKt Bft SwellHght WindSpdKt RainFog HorizSun 1 S 2 108 9.4 3 5 10 1 3 VertSun Glare Vis ObsL Rec ObsR ObsInd EffortDot EventNum 1 2 FALSE 6 200 073 208 &lt;NA&gt; TRUE 129 file_das line_num stratum_HI_EEZ stratum_OtherCNP 1 CenPac1986-2020_Final_alb_edited.das 273857 TRUE TRUE stratum_WHICEAS year month day yday km_int km_cum ship stratum use 1 FALSE 2002 11 8 312 0 80534.72 DSJ HI_EEZ TRUE eff_bloc seg_id SightNo Subgroup SightNoDaily Obs ObsStd Bearing Reticle 1 102-7 97 275 &lt;NA&gt; 20021108_37 208 TRUE 93 0.5 DistNm Cue Method Photos Birds CalibSchool PhotosAerial Biopsy CourseSchool 1 2.75 6 4 Y N &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA TurtleSp TurtleGs TurtleJFR TurtleAge TurtleCapt PinnipedSp PinnipedGs 1 &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA BoatType BoatGs PerpDistKm species best low high prob mixed ss_tot 1 &lt;NA&gt; NA 5.08602 073 3.478261 3 3 FALSE FALSE 3.478261 lnsstot ss_percent n_sp n_obs n_best n_low n_high calibr ss_valid mixed_max 1 1.246532 1 1 6 6 6 6 TRUE TRUE TRUE spp_max included 1 073 FALSE LTabundR was correct to exclude this sighting because the bearing was past the beam (93 degrees). The bearing of 93 is also given in the raw DAS data… das_file &lt;- &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39; das &lt;- das_readtext(das_file) i &lt;- which(substr(das$das, 6, 18) == &#39;141904 110802&#39;) das$das[i] [1] &quot;129S.141904 110802 N22:32.34 W171:45.83 275 208 6 4 093 0.5 2.75&quot; [2] &quot;130A.141904 110802 N22:32.34 W171:45.83 275 Y N 073&quot; … so it is not clear why ABUND did not exclude this sighting as invalid as well. It may be that ABUND was not expecting bearings above 90 degrees in the on-effort data. Cruise 1631, Species 003 In this case, there was a non-systematic sighting of species 003 that was found by LTabundR but not by ABUND. sight_compare(abund, ltabundr, 1631, &#39;003&#39;) ABUND: # A tibble: 0 × 6 # ℹ 6 variables: CruzNo &lt;int&gt;, DateTime &lt;chr&gt;, Beauf &lt;int&gt;, Mixed &lt;chr&gt;, # best &lt;dbl&gt;, use_sit &lt;lgl&gt; LTabundR: # A tibble: 1 × 9 Cruise DateTime Bft mixed best OnEffort EffType use_effort &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;lgl&gt; 1 1631 2006-09-02 07:35:14 5 TRUE 1 TRUE N TRUE # ℹ 1 more variable: use_sit &lt;lgl&gt; (ltabundr %&gt;% filter(Cruise == 1631, species == &#39;003&#39;)) Event DateTime Lat Lon OnEffort Cruise Mode OffsetGMT 1 S 2006-09-02 07:35:14 19.28883 -156.8227 TRUE 1631 P 10 EffType ESWsides Course SpdKt Bft SwellHght WindSpdKt RainFog HorizSun 1 N 2 155 10 5 4 18 1 10 VertSun Glare Vis ObsL Rec ObsR ObsInd EffortDot EventNum 1 2 FALSE 7 073 196 197 &lt;NA&gt; TRUE 026 file_das line_num stratum_HI_EEZ stratum_OtherCNP 1 CenPac1986-2020_Final_alb_edited.das 405222 TRUE TRUE stratum_WHICEAS year month day yday km_int km_cum ship stratum use 1 TRUE 2006 9 2 245 0 124035.2 Mc2 WHICEAS TRUE eff_bloc seg_id SightNo Subgroup SightNoDaily Obs ObsStd Bearing Reticle 1 141-1 543 090 &lt;NA&gt; 20060902_38 197 TRUE 59 0.3 DistNm Cue Method Photos Birds CalibSchool PhotosAerial Biopsy CourseSchool 1 4.12 2 4 N N &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA TurtleSp TurtleGs TurtleJFR TurtleAge TurtleCapt PinnipedSp PinnipedGs 1 &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA BoatType BoatGs PerpDistKm species best low high prob mixed ss_tot 1 &lt;NA&gt; NA 6.540392 003 1 NaN NA FALSE TRUE 3.478261 lnsstot ss_percent n_sp n_obs n_best n_low n_high calibr ss_valid mixed_max 1 1.246532 NaN 2 1 0 1 0 TRUE FALSE FALSE spp_max included 1 &lt;NA&gt; TRUE The map indicates that this is not a geostratum boundary issue: cruzi &lt;- filter_cruz(cruz, spp=&#39;003&#39;, years = 2006) map_cruz(cruzi) Looking at the raw DAS data … das$das[405190:405220] [1] &quot; C 120000 090106 In transit to study area Sep 1. A.J. 10/18/06.&quot; [2] &quot;001B.061813 090206 N19:28.97 W156:55.51 1631 p 10 Y&quot; [3] &quot;002R.061813 090206 N19:28.97 W156:55.51 N&quot; [4] &quot;003P.061813 090206 N19:28.97 W156:55.51 197 007 277&quot; [5] &quot;004V.061813 090206 N19:28.97 W156:55.51 5 04 150 18.0&quot; [6] &quot;005N.061813 090206 N19:28.97 W156:55.51 154 10.4&quot; [7] &quot;006W.061813 090206 N19:28.97 W156:55.51 1 10 03 032 7.0&quot; [8] &quot;007C.061823 090206 N19:28.94 W156:55.50&quot; [9] &quot;008*.062019 090206 N19:28.63 W156:55.33&quot; [10] &quot;009*.063019 090206 N19:27.12 W156:54.53&quot; [11] &quot;010P.063950 090206 N19:25.68 W156:53.75 196 197 007&quot; [12] &quot;011C.063950 090206 N19:25.68 W156:53.75&quot; [13] &quot;012V.063950 090206 N19:25.68 W156:53.75 5 04 150 18.0&quot; [14] &quot;013N.063950 090206 N19:25.68 W156:53.75 151 10.1&quot; [15] &quot;014W.063950 090206 N19:25.68 W156:53.75 1 10 03 032 7.0&quot; [16] &quot;015*.064019 090206 N19:25.61 W156:53.71&quot; [17] &quot;016N.064253 090206 N19:25.22 W156:53.50 153 10.3&quot; [18] &quot;017*.065019 090206 N19:24.11 W156:52.90&quot; [19] &quot;018*.070019 090206 N19:22.62 W156:52.11&quot; [20] &quot;019*.071019 090206 N19:21.10 W156:51.30&quot; [21] &quot;020*.072019 090206 N19:19.59 W156:50.53&quot; [22] &quot;021P.072113 090206 N19:19.46 W156:50.45 073 196 197&quot; [23] &quot;022V.072113 090206 N19:19.46 W156:50.45 5 04 150 18.0&quot; [24] &quot;023N.072113 090206 N19:19.46 W156:50.45 155 10.0&quot; [25] &quot;024W.072113 090206 N19:19.46 W156:50.45 1 10 02 032 7.0&quot; [26] &quot;025*.073019 090206 N19:18.09 W156:49.75&quot; [27] &quot;026S.073514 090206 N19:17.33 W156:49.36 090 197 2 4 059 0.3 4.12 013&quot; [28] &quot;027A.073514 090206 N19:17.33 W156:49.36 090 N N 177 003&quot; [29] &quot; 1 197 3&quot; [30] &quot;028*.074019 090206 N19:16.53 W156:48.94&quot; [31] &quot;029V.074337 090206 N19:16.01 W156:48.67 4 04 150 12.0&quot; It appears that this was a multi-species sighting, but no species percentages were provided. In the absence of a percent composition estimate for this sighting, it was ignored by ABUND. Group sizes # Format ABUND abund &lt;- SIGHTS %&gt;% tidyr::pivot_longer(cols = 31:101, names_to = &#39;species&#39;, values_to = &#39;best&#39;) %&gt;% mutate(Region = gsub(&#39; &#39;,&#39;&#39;,Region)) %&gt;% filter(best &gt; 0, ! Region %in% c(&#39;NONE&#39;, &#39;Off-Transect&#39;), EffortSeg &gt; 0) %&gt;% select(Cruise = CruzNo, TotSS, LnTotSS, species, best) %&gt;% mutate(Software=&#39;ABUND9&#39;) # Format LTabundR ltabundr &lt;- cruz$cohorts$all$sightings %&gt;% filter(OnEffort == TRUE, included == TRUE, EffType %in% c(&#39;S&#39;, &#39;F&#39;)) %&gt;% select(Cruise, TotSS = ss_tot, LnTotSS = lnsstot, species, best) %&gt;% mutate(Software=&#39;LTabundR&#39;) # Combine the datasets ss &lt;- rbind(abund, ltabundr) # Plot the datasets ggplot(ss, aes(x=best, y=factor(Cruise), col=Software, pch=Software)) + geom_point(position=ggstance::position_dodgev(height=0.5), alpha=.6) + scale_x_continuous(trans=&#39;log&#39;, breaks=c(1,2, 5,10,25,50,100,500,1000,2500,5000)) + xlab(&#39;log Estimated School Size&#39;) + ylab(&#39;Cruise&#39;) + theme_light() Effort # Format ABUND abund &lt;- EFFORT %&gt;% mutate(Region = gsub(&#39; &#39;,&#39;&#39;,Region)) %&gt;% mutate(dt = paste0(Yr, stringr::str_pad(Mo, width=2, pad=&#39;0&#39;, side=&#39;left&#39;), stringr::str_pad(Da, width=2, pad=&#39;0&#39;, side=&#39;left&#39;))) %&gt;% filter(! Region %in% c(&#39;NONE&#39;, &#39;Off-Transect&#39;), EffortSeg &gt; 0) %&gt;% select(Cruise = CruzNo, Date = dt, km = length) %&gt;% mutate(software = &#39;ABUND9&#39;) # Format LTabundR ltabundr &lt;- cruz$cohorts$all$segments %&gt;% mutate(dt = gsub(&#39;-&#39;,&#39;&#39;,substr(DateTime1, 1,10))) %&gt;% filter(OnEffort == TRUE, use == TRUE, EffType %in% c(&#39;S&#39;, &#39;F&#39;)) %&gt;% select(Cruise, Date = dt, km = dist) %&gt;% mutate(software = &#39;LTabundR&#39;) # Join the datasets eff &lt;- rbind(abund, ltabundr) %&gt;% group_by(Cruise) %&gt;% mutate(km = round(km)) %&gt;% summarize(km_abund = sum(km[software == &#39;ABUND9&#39;]), km_ltabundr = sum(km[software == &#39;LTabundR&#39;])) # Plot p &lt;- ggplot(eff, aes(x = km_abund, y = km_ltabundr, col=factor(Cruise))) + geom_abline(slope=1, intercept=0, lty=3) + geom_point(alpha=.6) + ylab(&#39;LTabundR&#39;) + xlab(&#39;ABUND9&#39;) + scale_x_continuous(breaks = seq(0, 15000, by=2500)) + scale_y_continuous(breaks = seq(0, 15000, by=2500)) + labs(title=&#39;Systematic effort per cruise&#39;, col=&#39;Cruise&#39;) + theme_light() # Make it interactive ggplotly(p) "],["maps.html", " 4 Maps Publishable maps Interactive maps Interactive dashboard", " 4 Maps To build a flexible system for mapping cruise data, we have the following functions: Publishable maps Base maps Begin with a basic map, including EEZ borders: m &lt;- map_base(region=&#39;cnp&#39;) m We also have a base map for the California Current … m &lt;- map_base(region=&#39;ccs&#39;) m And the ETP: m &lt;- map_base(region=&#39;etp&#39;) m Add strata Add your research strata to your map: m &lt;- map_base(region=&#39;cnp&#39;) m1 &lt;- map_strata(m, cruz_1720$settings, region=&#39;cnp&#39;) Add survey tracks m1 &lt;- map_effort(m, cruz_1720) m1 The defaults of map_effort() assume, for simplicity, that you want to see the segments to be included in density estimation for the first cohort specified in your settings. You can adjust this and other defaults using the function arguments. Customizing effort Inputs This map changes survey track thickness and color. m1 &lt;- map_effort(m, cruz_1720, effort_color=&#39;firebrick&#39;, effort_stroke=2.5, effort_linetype=1,) Color-code conditions Your second customization option is to add format variables to the segments slot of the cohort of interest in the cruz object. This gives you full control of line color, thickness, and line-type according to whatever specifications you wish to set, e.g., color-coding by effort type or Beaufort sea state. This is possible because the function map_effort() looks for the variables col (line color), lwd (line thickness or stroke), and lty (line type) in the columns of cruz$segments. If these columns exist, the values therein will be used instead of the function defaults. For example, color-code by Beaufort scale: # Save copy of segments to modify cruzi &lt;- cruz_1720 segments &lt;- cruzi$cohorts$all$segments # Add column `col`: color code by BFT sea state bft_colors &lt;- c(&#39;steelblue4&#39;,&#39;steelblue2&#39;,&#39;cadetblue1&#39;,&#39;grey&#39;) segments$col &lt;- bft_colors[4] segments$col[ segments$avgBft &lt;= 7 ] &lt;- bft_colors[3] # bft 5 + segments$col[ segments$avgBft &lt;= 4 ] &lt;- bft_colors[2] # bft 3 - 4 segments$col[ segments$avgBft &lt;= 2 ] &lt;- bft_colors[1] # bft 0 -2 # Update sub_segments slot in `cruz` object cruzi$cohorts$all$segments &lt;- segments # Update map m_custom2 &lt;- map_effort(m, cruzi) # Add legend using native functions from mapping package `tmap` m_custom2 &lt;- m_custom2 + tmap::tm_add_legend(&#39;line&#39;, col = bft_colors, lwd = 3, labels = c(&#39; 0 - 2&#39;, &#39; 3 - 4&#39;, &#39; 5 +&#39;, &#39; no data&#39;), title=&quot;Beaufort sea state&quot;) + tmap::tm_layout(legend.position=c(&#39;left&#39;,&#39;bottom&#39;)) # Show map m_custom2 Add sightings Use the function map_sightings() to add sightings to your map: m1 &lt;- map_sightings(m, cruz_1720) Customizing sightings To demonstrate some of the customization options, consider this map that shows sightings of false killer whales with custom dot color, shape, and size: m1 &lt;- map_sightings(m, cruz_1720, include_species = &#39;033&#39;, color_base = &#39;purple&#39;, shape_base = 18, size_base = 1) Next is a map of humpback whales and sperm whales, color-coded by species and shape-coded by whether or not the sighting will be included in the analysis: m1 &lt;- map_sightings(m, cruz_1720, include_species = c(&#39;076&#39;,&#39;046&#39;), color_code = TRUE, shape_code = TRUE) Overview Here is an overview of the steps needed to map strata, survey tracks, and sightings all together: m &lt;- map_base(&#39;cnp&#39;) m &lt;- map_strata(m, cruz_1720$settings) m &lt;- map_effort(m, cruz_1720) m &lt;- map_sightings(m, cruz_1720, size_base=.4) m Interactive maps LTabundR also has an interactive map function, which maps survey data using the leaflet package. map_cruz(cruz_1720, cohort=1, eez_show=FALSE, strata_show=FALSE, effort_show=TRUE, effort_resolution=1, sightings_show=TRUE, sightings_color = &#39;firebrick&#39;, verbose=FALSE) Note that you can also click on sightings and tracklines to see their details. Refer to the documentation for this function (?map_cruz) to see all the options available for stylizing these maps. Interactive dashboard Finally, note that LTabundR comes with an interactive data explorer app (a Shiny app) for filtering survey data according to effort scenario and species code, toggling map_cruz() settings, and reviewing summary tables of effort and sightings (including inspection of truncation distances). cruz_explorer(cruz) Screenshots from this app:           "],["filter.html", " 5 Filter &amp; combine surveys Filter Combine", " 5 Filter &amp; combine surveys You may soon encounter the need to filter a processed cruz object to only certain years, regions, or cruise numbers. You may also need to combine one processed cruz object with another. Here we will continue with the cruz object we created on the previous page. As a reminder, here is the data structure of that object: cruz_structure(cruz) &quot;cruz&quot; list structure ======================== $settings $strata --- with 11 polygon coordinate sets $survey --- with 11 input arguments $cohorts --- with 3 cohorts specified, each with 19 input arguments $strata ... containing a summary dataframe of 11 geostrata and their spatial areas ... geostratum names: HI_EEZ, OtherCNP, MHI, WHICEAS, Spotted_OU, Spotted_FI, Spotted_BI, Bottlenose_KaNi, Bottlenose_OUFI, Bottlenose_BI, NWHI $cohorts $all geostrata: WHICEAS, HI_EEZ, OtherCNP $segments --- with 1900 segments (median = 149.2 km) $das --- with 329638 data rows $sightings --- with 3934 detections $subgroups --- with 255 subgroups, 62 sightings, and 389 events $bottlenose geostrata: WHICEAS, HI_EEZ, OtherCNP, Bottlenose_BI, Bottlenose_OUFI, Bottlenose_KaNi $segments --- with 2063 segments (median = 148.5 km) $das --- with 329638 data rows $sightings --- with 523 detections $spotted geostrata: WHICEAS, HI_EEZ, OtherCNP, Spotted_OU, Spotted_FI, Spotted_BI $segments --- with 2072 segments (median = 148.4 km) $das --- with 329638 data rows $sightings --- with 527 detections Filter LTabundR lets you filter a cruz object using the function filter_cruz(). For example, in our WHICEAS case study, we processed surveys from 1986 - 2020, which we needed to do to model our detection functions, but our interest for mapping is specifically valid effort in 2017 and 2020 only, and only within the \"WHICEAS\" geostratum. cruz_1720 &lt;- filter_cruz(cruz, analysis_only = TRUE, years = c(2017, 2020), regions = &#39;WHICEAS&#39;) We will use this filtered cruz object for mapping &amp; sightings summaries downstream. save(cruz_1720,file=&#39;whiceas_cruz_1720.RData&#39;) Note that filter_cruz() has many other filter options. See ?filter_cruz() for details. Combine Say you have two processed cruz objects: one containing survey effort from the Hawaiian EEZ (HI_EEZ) geostratum area only, and one containing survey effort from everywhere else that does not include HI_EEZ effort. Let’s make those fake datasets right now, using filter_cruz(): Hawaiian EEZ-only data: cruz_hi &lt;- filter_cruz(cruz, regions = &#39;HI_EEZ&#39;, verbose = FALSE) Pelagic Hawaiian EEZ - only data: cruz_other &lt;- filter_cruz(cruz, not_regions = &#39;HI_EEZ&#39;, verbose = FALSE) Say you want to combine these datasets together in order to reconstruct the equivalent of our original cruz object. You can do this with the LTabundR function, cruz_combine(). # Make a list of cruz objects cruzes &lt;- list(cruz_hi, cruz_other) # Now combine cruz_demo &lt;- cruz_combine(cruzes) Re-constituted data structure: cruz_structure(cruz_demo) &quot;cruz&quot; list structure ======================== $settings $strata --- with 11 polygon coordinate sets $survey --- with 11 input arguments $cohorts --- with 3 cohorts specified, each with 19 input arguments $strata ... containing a summary dataframe of 11 geostrata and their spatial areas ... geostratum names: HI_EEZ, OtherCNP, MHI, WHICEAS, Spotted_OU, Spotted_FI, Spotted_BI, Bottlenose_KaNi, Bottlenose_OUFI, Bottlenose_BI, NWHI $cohorts $all geostrata: WHICEAS, HI_EEZ, OtherCNP $segments --- with 1854 segments (median = 149.2 km) $das --- with 326623 data rows $sightings --- with 3930 detections $subgroups --- with 237 subgroups, 54 sightings, and 345 events $bottlenose geostrata: WHICEAS, HI_EEZ, OtherCNP, Bottlenose_BI, Bottlenose_OUFI, Bottlenose_KaNi $segments --- with 2017 segments (median = 148.5 km) $das --- with 326623 data rows $sightings --- with 523 detections $spotted geostrata: WHICEAS, HI_EEZ, OtherCNP, Spotted_OU, Spotted_FI, Spotted_BI $segments --- with 2027 segments (median = 148.5 km) $das --- with 326623 data rows $sightings --- with 527 detections "],["summarize.html", " 6 Summarize survey Summarize effort Summarize by Beaufort Summarize sightings Summarize certain species cruz_explorer()", " 6 Summarize survey Here we will summarize the 2017 &amp; 2020 survey data we prepared on the previous page. load(&#39;whiceas_cruz_1720.RData&#39;) Summarize effort The summarize_effort() functions builds tables with total kilometers and days surveyed. effort &lt;- summarize_effort(cruz_1720, cohort=1) This function summarizes effort in several default tables: effort %&gt;% names() [1] &quot;total&quot; &quot;total_by_cruise&quot; &quot;total_by_year&quot; &quot;total_by_effort&quot; [5] &quot;total_by_stratum&quot; Total surveyed The slot $total provides the grand total distance and unique dates surveyed: library(DT) effort$total %&gt;% DT::datatable(options=list(initComplete = htmlwidgets::JS( &quot;function(settings, json) {$(this.api().table().container()).css({&#39;font-size&#39;: &#39;9pt&#39;});}&quot;) )) Total surveyed by effort The slot $total_by_effort provides the total distance and days surveyed, grouped by segments that will be included in the analysis and those that won’t: Total surveyed by stratum The slot $total_by_stratum provides the total distance and days surveyed within each stratum, again grouped by segments that will be included in the analysis and those that won’t: Summarize by Beaufort bft &lt;- summarize_bft(cruz_1720, cohort=1) This function summarizes effort by Beaufort in four default tables: bft %&gt;% names() [1] &quot;overall&quot; &quot;by_year&quot; &quot;by_stratum&quot; &quot;details&quot; Simple overall breakdown The slot $overall provides the total effort – and proportion of effort – occurring in each Beaufort state: Breakdown by year The slot $by_year provides the above for each year separately: Breakdown by stratum The slot $by_stratum provides the above for each geostratum separately: Detailed breakdown The slot $details provides the above for each cruise-year-study area-geostratum combination within the data: Summarize sightings The summarize_sightings() function builds tables summarizing the sightings within each cohort-analysis. (Eventually, we may want to include an option to merge all sightings from all cohort-analyses into a single table.) sightings &lt;- summarize_sightings(cruz_1720, cohort=1) This function summarizes sightings in four default tables: sightings %&gt;% names() [1] &quot;simple_totals&quot; &quot;analysis_totals&quot; [3] &quot;stratum_simple_totals&quot; &quot;stratum_analysis_totals&quot; Simple species totals The slot $simple_totals includes all sightings, even if they will not be inluded in analysis: Analysis totals The slot $analysis_totals only includes sightings that meet all inclusion criteria for the analysis: Simple totals for each stratum The slot $stratum_simple_totals splits the first table (simple species totals) so that sightings are tallied for each geo-stratum separately: Analysis totals for each stratum The slot $stratum_analysis_totals splits the second table (analysis totals for each species) so that sightings are tallied for each geo-stratum separately: Summarize certain species To deep-dive into details for a ceratin species (or group of species), use the function summarize_species(). species &lt;- summarize_species(spp=&#39;046&#39;, cruz_1720) This functions a list with a variety of summaries: species %&gt;% names [1] &quot;species&quot; &quot;n_total&quot; &quot;n_analysis&quot; [4] &quot;school_size&quot; &quot;yearly_total&quot; &quot;yearly_analysis&quot; [7] &quot;regional_total&quot; &quot;regional_analysis&quot; &quot;detection_distances&quot; [10] &quot;sightings&quot; The slots $n_total and $n_analysis provide the total number of sightings and the number eligible for inclusion in the analysis: species$n_total [1] 14 species$n_analysis [1] 14 School size details This table only includes the sightings eligible for analysis: Annual summaries (all sightings) Annual summaries (analysis only) Regional summaries (all sightings) Regional summaries (analysis only) Detection distances This table can be used to determine the best truncation distance to use, based on the percent truncation you wish and the number of sightings available at each option. All sightings data Finally, this last slot holds a dataframe of all sightings data for the specified species: cruz_explorer() Note that all of these summary tables can be viewed interactively using the function cruz_explorer(), which allows you to efficiently subset the data according to various filters. cruz_explorer(cruz_1720) "],["g0.html", " 7 Estimating g(0) Relative g(0) Weighted average g(0)", " 7 Estimating g(0) Detection function models assume g(0) is 1.0. In distance sampling, a “detection function” is fit to the your sighting distances to reflect the fact that animals farther out are more difficult to detect. That detection function is a model of how the probability of detection declines with increasing distance from your survey trackline. The equations for detection function models are all constructed to assume that the probability of detecting an animal on your trackline (distance = 0 km) is 1.0 – you never miss an animal on your trackline. This trackline detection probability is referred to as g(0). In reality, though, it almost never is – and it greatly impacts results. When searching for marine mammals at sea, even some of those occurring directly on your survey trackline will be missed. Real g(0) is actually less than 1.0. This technicality makes a big difference: if g(0) is actually 0.5, the assumption that g(0) is 1.0 will underestimate animal abundance by 50%. Some animals, such as pygmy and dwarf sperm whales (Genus Kogia), are very cryptic and easily missed, and thus likely have a true g(0) below 0.1. This means that estimates assuming their g(0) is still 1.0 will underestimate true abundance by 90%! Moreover, all species – whether you are a pygmy sperm whale or a blue whale – become easier to miss when sighting conditions deteriorate (e.g., Beaufort sea states 4 - 6). Therefore, g(0) must be estimated then used to scale the detection function. So g(0) matters, and the assumption of most detection functions that g(0) = 1.0 is nearly always wrong. Luckily, there is a way to handle this that avoids constructing new detection function equations or estimating even more parameters during the detection function fitting process: we use an estimate of g(0) to scale the detection function. Say the detection function predicts that the probability of detection is 1.0, 0.8, and 0.6 at distances 0 km, 1 km, and 2km, respectively. If g(0) is actually 0.5, then we can scale the detection function so that those respective predictions are now 0.5, 0.4, and 0.3. Estimating g(0) for a survey generally involves four steps: First, you estimate g(0) in perfect conditions (i.e., Beaufort sea state 0). Second, you scale that estimate downward to approximate g(0) when conditions are less than ideal (i.e., a separate g(0) estimate for each Beaufort sea state from 1 to 6). This is known as the relative trackline probability, or relative g(0), or most simply: Rg(0). Third, you determine the weighted average value of g(0) for your particular survey, based on the proportional distribution of effort in each sea state. You pass this single value to your line-transect analysis functions. Fourth, you need to determine the CV of your weighted estimate of g(0). This isn’t straightforward, because you first need to simulate a new distribution for the weighted g(0) estimate, from which you then calculate the CV. The first of these steps is typically the biggest lift analytically, and very few studies provide absolute estimates of g(0) for their species of interest. Doing so involves Bayesian simulations and special field methods (see this example from Jay Barlow, NOAA-NFMFS Southwest Fisheries Science Center). Instead, most studies assume that the absolute g(0) is in fact 1.0 – even though it’s not – and proceed directly to the second step – relative trackline probability, Rg(0). This is more common because it can be estimated directly from the survey data, thanks to an approached developed in Barlow (2015), “Inferring trackline detection probabilities, g(0), for cetaceans from apparent densities in different survey conditions” (Marine Mammal Science). Relative g(0) Archived Rg(0) estimates As of 2022, most northeast Pacific NOAA-NMFS studies still use the Rg(0) estimates from Barlow (2015), which are based on NOAA-NMFS cruises from 1986 to 2010. LTabundR includes Barlow’s results in a built-in dataset. data(barlow_2015) Here is the top of this dataset, in which each row is a Rg(0) estimate for a single species - Beaufort sea state scenario. barlow_2015 %&gt;% head(12) title scientific spp truncation 1 Delphinus spp Delphinus 005-016-017 5.5 2 Delphinus spp Delphinus 005-016-017 5.5 3 Delphinus spp Delphinus 005-016-017 5.5 4 Delphinus spp Delphinus 005-016-017 5.5 5 Delphinus spp Delphinus 005-016-017 5.5 6 Delphinus spp Delphinus 005-016-017 5.5 7 Delphinus spp Delphinus 005-016-017 5.5 8 Stenella attenuata ssp Stenella attenuata ssp 002-006-089,-090 5.5 9 Stenella attenuata ssp Stenella attenuata ssp 002-006-089,-090 5.5 10 Stenella attenuata ssp Stenella attenuata ssp 002-006-089,-090 5.5 11 Stenella attenuata ssp Stenella attenuata ssp 002-006-089,-090 5.5 12 Stenella attenuata ssp Stenella attenuata ssp 002-006-089,-090 5.5 pooling regions bft Rg0 Rg0_CV 1 none none 0 1.000 0.00 2 none none 1 1.000 0.00 3 none none 2 0.940 0.25 4 none none 3 0.722 0.25 5 none none 4 0.485 0.14 6 none none 5 0.394 0.20 7 none none 6 0.404 0.50 8 none none 0 1.000 0.00 9 none none 1 0.728 0.03 10 none none 2 0.531 0.06 11 none none 3 0.386 0.09 12 none none 4 0.282 0.12 This dataset includes Rg(0) estimates for the following species: barlow_2015$title %&gt;% unique [1] &quot;Delphinus spp&quot; &quot;Stenella attenuata ssp&quot; [3] &quot;Stenella longirostris ssp&quot; &quot;Striped dolphin&quot; [5] &quot;Rough-toothed dolphin&quot; &quot;Bottlenose dolphin&quot; [7] &quot;Risso&#39;s dolphin&quot; &quot;Short-finned pilot whale&quot; [9] &quot;Killer whale&quot; &quot;Sperm whale&quot; [11] &quot;Kogia spp&quot; &quot;Cuvier&#39;s beaked whale&quot; [13] &quot;Mesoplodon spp&quot; &quot;Dall&#39;s porpoise&quot; [15] &quot;Minke whale&quot; &quot;Sei/Bryde&#39;s&quot; [17] &quot;Fin whale&quot; &quot;Blue whale&quot; [19] &quot;Humpback whale&quot; &quot;Unidentified dolphin&quot; [21] &quot;Unidentified cetacean&quot; &quot;Pacific white-sided dolphin&quot; [23] &quot;Pygmy killer whale&quot; If your study species – or one with similar detectability – can be found on this list, then you can take this data.frame of Rg(0) values and move on to the next step. Note that if you do not have a large survey dataset, this may be the only option available to you. Estimating new Rg(0) values requires a large number of sightings (i.e., hundreds) across many Beaufort states. New Rg(0) estimates LTabundR includes a function, g0_model(), which you can use to apply the Barlow (2015) modeling methods to generate new estimates of Rg(0) based on your own survey data. To do this, you first need a cruz object in which effort has been split into short segments (5 - 10 km). If you want to work with NOAA-NMFS WinCruz data from the Pacific, you can use a built-in dataset of 1986 - 2020 surveys that is processed specifically for use in Rg(0) estimation: data(&quot;noaa_10km_1986_2020&quot;) To use this dataset in R(0) estimation, we first filter it to systematic effort within sea states 0 - 6: cruzi &lt;- filter_cruz(noaa_10km_1986_2020, analysis_only = TRUE, eff_types = &#39;S&#39;, bft_range = 0:6, on_off = TRUE) You can then estimate Rg(0) for each Beaufort sea state using the function g0_model(). For example, the code for striped dolphin (Stenella coeruleoalba) is as follows: rg0 &lt;- g0_model(spp = &#39;013&#39;, truncation_distance = 5.5, cruz = cruzi, jackknife_fraction = .1) The jackknife_fraction input indicates that standard error and CV will be estimated using an iterative jackknife procedure in which 10% of the data is removed in each iteration. Find more details on this process using the function documentation, ?g0_model(). The input pool_bft provides a way to specify that low Beaufort sea states, which are typically rare in open-ocean surveys, should be pooled. This step may be needed in order to achieve a monotonic decline in the g(0) ~ Bft relationship for some species, but the default is NULL, i.e., no pooling. If pool_bft is the character string \"01\", Beaufort states 1 will be pooled into state 0. If pool_bft is the character string \"012\", Beaufort states 1 and 2 will be pooled into state 0. We recommend beginning with NULL then modifying this if needed, based on the output. The chief result of this function is a $summary table: rg0$summary bft Rg0 ESW n Rg0_SE Rg0_CV ESW_SE 1 0 1.0000000 3.848931 10 0.00000000 0.0000000 0.16223439 2 1 0.8662628 3.625921 10 0.13414951 0.1548601 0.12500298 3 2 0.6877623 3.388426 10 0.18187254 0.2644410 0.08256461 4 3 0.4426797 3.140401 10 0.12199113 0.2755743 0.05271386 5 4 0.2732751 2.890996 10 0.06475668 0.2369652 0.06789456 6 5 0.2217223 2.648658 10 0.05541759 0.2499415 0.10786919 7 6 0.2205977 2.422439 10 0.07542736 0.3419226 0.14742487 The model predicts that Rg(0) declines rapidly with deteriorating sea state: plot(Rg0 ~ bft, data = rg0$summary, ylim=c(0,1), type=&#39;o&#39;, pch=16) abline(h=seq(0,1,by=.1), col=&#39;grey85&#39;, lty=3) In addition to the summary above, this function returns various details, including the details of the Generalized Additive Model (GAM) (for both the estimate and the jackknifed datasets), and the raw sightings and segments used in the model. [1] &quot;Rg0&quot; &quot;gam&quot; &quot;jackknife&quot; &quot;summary&quot; &quot;sightings&quot; &quot;segments&quot; To produce these estimates efficiently for many species, you can use the wrapper function g0_table(), as follows. First you build a list of parameters for each species/species group: species &lt;- list( list(spp = c(&#39;005&#39;, &#39;016&#39;, &#39;017&#39;), title = &#39;Delphinus spp&#39;, constrain_shape = TRUE, truncation = 5.5), list(spp = &#39;021&#39;, title = &quot;Risso&#39;s dolphin&quot;, truncation = 5.5, constrain_shape = TRUE), list(spp = &#39;046&#39;, title = &#39;Sperm whale&#39;, truncation = 5.5), list(spp = c(&#39;047&#39;, &#39;048&#39;, &#39;080&#39;), title = &#39;Kogia spp&#39;, truncation = 4.0), list(spp = &#39;061&#39;, title = &quot;Cuvier&#39;s beaked whale&quot;, truncation = 4.0), list(spp = &#39;074&#39;, title = &#39;Fin whale&#39;, truncation = 5.5, regions = &#39;CCS&#39;)) Note that we used shorter truncation distances for cryptic species. Note also that we had to pool Beaufort sea states 0-2 for Risso’s dolphins in order to maintain a monotonic decline in the Rg(0) ~ Beaufort curve. Note also that we limited the geostrata used to model the fin whale Rg(0) curve to the California Current System (‘CCS’, after Barlow 2015), so that zero-inflated segments did not confound the model. We then pass this species list to g0_table(). In this example, we are only estimating the Rg(0) relationship, without conducting jackknife estimation: rg0s &lt;- g0_table(cruzi, species, eff_types = &#39;S&#39;, jackknife_fraction = NULL) Now plot the result using a dedicated LTabundR function: g0_plot(rg0s, panes=1) Weighted average g(0) Since g(0) clearly depends upon survey conditions, and since each survey is carried out in a specific sequence of conditions, a unique, weighted g(0) value must be estimated for each species in each geostratum and year of interest. This will be done automatically by the line-transect analysis functions coming up (see LTabundR::lta()), meaning you only need a table of Rg(0) values in order to proceed to the next step. But you can also calculate weighted g(0) values separately as an isolated analysis, which we show below. Let’s say we want to estimate the average g(0) for striped dolphins during the WHICEAS survey years of 2017 and 2020. From above, we have an estimate of the Rg(0) and its CV for each Beaufort state: rg0$summary %&gt;% select(bft, Rg0, Rg0_CV) bft Rg0 Rg0_CV 1 0 1.0000000 0.0000000 2 1 0.8662628 0.1548601 3 2 0.6877623 0.2644410 4 3 0.4426797 0.2755743 5 4 0.2732751 0.2369652 6 5 0.2217223 0.2499415 7 6 0.2205977 0.3419226 We also have a processed cruz object with 2017 data: load(&#39;whiceas_cruz_1720.RData&#39;) cruz_17 &lt;- filter_cruz(cruz_1720, years = 2017, verbose=FALSE) To view the distribution of effort in this WHICEAS 2017 across sea states, we can use the function summarize_bft(): summarize_bft(cruz_17)$overall # A tibble: 6 × 3 bftr km prop &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 130. 0.0200 2 2 583. 0.0897 3 3 901. 0.139 4 4 2140. 0.329 5 5 1873. 0.288 6 6 875. 0.135 We then use the function g0_weighted_var() to compute the weighted Rg(0) for our survey as well as its CV. This function carries out an automated optimization routine to simulate a new distribution for the weighted g(0), which is then used to estimate the weighted CV. weighted_g0_2017 &lt;- g0_weighted(Rg0 = rg0$summary$Rg0, Rg0_cv = rg0$summary$Rg0_CV, cruz = cruz_17) The result: weighted_g0_2017$g0[1:2] weighted_g0 wt.mean 1 0.3238287 0.333 Now let’s do the same for WHICEAS 2020 and compare the weighted g(0) estimate: # Filter to 2020 cruz_20 &lt;- filter_cruz(cruz_1720, years = 2020, verbose=FALSE) # Summarize Bft effort summarize_bft(cruz_20)$overall # A tibble: 6 × 3 bftr km prop &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 88.2 0.0165 2 2 262. 0.0490 3 3 434. 0.0813 4 4 1482. 0.278 5 5 2004. 0.376 6 6 1067. 0.200 Note that conditions were a bit worse in 2020 compared to 2017. We therefore expect the weighted g(0) estimate to be lower: weighted_g0_2020 &lt;- g0_weighted(Rg0 = rg0$summary$Rg0, Rg0_cv = rg0$summary$Rg0_CV, cruz = cruz_20) The result: weighted_g0_2020$g0[1:2] weighted_g0 wt.mean 1 0.2872902 0.295 Confirmed: weighted g(0) for 2020 is slightly lower than in 2017, due to generally worse survey conditions. These year-specific estimates should prevent those different conditions from impacting their respective abundance estimates. "],["lta.html", " 8 Line-transect analysis Key inputs Variance estimation Other inputs Output Unusual estimate scenarios Behind the scenes", " 8 Line-transect analysis We use line-transect analysis to produce estimates of animal density and/or abundance based upon surveys. The main LTabundR function for conducting line-transect analysis is lta(), which calls for four primary arguments in addition to your cruz object: lta(cruz, Rg0, fit_filters, df_settings, estimates) Below we explain each of these inputs, discuss other optional inputs, and explore the results produced by lta(). Key inputs cruz This is the cruz object you have generated with process_surveys(). Before running lta(), ensure that this cruz object is filtered only to the years, regions, and sighting conditions you would like to use for detection function fitting. Filter your cruz object with full flexibility using LTabundR::filter_cruz(). Note that filtering for detection function fitting is typically less stringent than filtering for downstream steps for abundance estimation, since as many sightings are included as possible to combat low sample sizes, as long as sightings were observed using standard methods in an unbiased search pattern, and as long as you do not expect detectability to vary across years and regions. Here we will work with a version of the 1986-2020 Central North Pacific survey data we processed a few pages back. This version is included as a built-in dataset within LTabundR: load(&#39;whiceas_cruz.RData&#39;) As it is provided, this dataset does not need any filtering. We will use these data to estimate the abundance of striped dolphins (Stenella coeruleoalba), Fraser’s dolphins (Lagenodelphis hosei), and Melon-headed whales (Preponocephala electra) within the WHICEAS study area in 2017 and 2020. We will group these three species into a ‘species pool’ in order to gain a sufficient sample size for fitting a detection function. We will then use “Species” as a covariate within the detection function model, along with other variables including Beaufort Sea State, ship name, and log-transformed school size. Rg0 The result of LTabundR::g0_model(), which is a data.frame with Relative trackline detection probabilities, Rg(0), for each species in each Beaufort sea state. See LTabundR dataset data(\"g0_results\"), used below, as an example. This is an optional input. If not provided, g(0) will be assumed to 1.0, and its CV will be assumed to be 0. Alternatively, you can manually specify values for g(0) and its CV in the estimates argument below. Here we will use a data.frame of Rg(0) estimates based on the same survey years, 1986 - 2020, which has been provided as a built-in dataset: data(&quot;g0_results&quot;) Rg0 &lt;- g0_results This dataset looks as follows. Each row is a Rg(0) estimate for a species group in a given Beaufort state, with details on the data used to generate that estimate and the CV of the estimate. For the lta() routine, the critical columns are spp, bft, Rg0, and Rg0_CV. Rg0 %&gt;% head title spp bft Rg0 ESW n Rg0_SE Rg0_CV 1 Delphinus spp 005-016-017 0 1.0000000 4.085421 10 0.00000000 0.00000000 2 Delphinus spp 005-016-017 1 0.7790195 3.827873 10 0.02901978 0.03725168 3 Delphinus spp 005-016-017 2 0.6068839 3.539560 10 0.04515935 0.07441184 4 Delphinus spp 005-016-017 3 0.4727958 3.231096 10 0.05271389 0.11149401 5 Delphinus spp 005-016-017 4 0.3683443 2.917720 10 0.05470342 0.14851164 6 Delphinus spp 005-016-017 5 0.2869780 2.613723 10 0.05322816 0.18547819 ESW_SE sits sits_p segs segs_p 1 0.21020267 38 0.01747126 193 0.004131082 2 0.16060474 188 0.08643678 1441 0.030843982 3 0.09893266 483 0.22206897 4434 0.094907853 4 0.05637934 574 0.26390805 8899 0.190479248 5 0.10050755 606 0.27862069 17912 0.383398617 6 0.16828702 270 0.12413793 12473 0.266979173 fit_filters The fit_filters input specifies how to filter the data before fitting the detection function. It accepts a named list, which in our example will look like this: fit_filters = list(spp = c(&#39;013&#39;, &#39;026&#39;, &#39;031&#39;), pool = &#39;Multi-species pool 1&#39;, cohort = &#39;all&#39;, truncation_distance = 5, other_species = &#39;remove&#39;) spp: A character vector of species codes. Using multiple species codes may be useful when you have low sample sizes for a cohort of similar species. cohort: The cohort containing these species, provided as a name or a number indicating which slot in cruz$cohorts should be referenced. truncation_distance: The truncation distance to apply during model fitting. The remaining inputs are optional (i.e., they all have defaults): pool: A character string, providing a title for this species pool. If not specified, the species codes used will be concatenated to produce a title automatically. other_species: A character vector with four recognized values: If \"apply\" (the default if not specified), the species code will be changed to \"Other\" for sightings in which the species was in a mixed-species school but was not the species with the largest percentage of the total school size. In those cases, the species was not as relevant to the detection of the school as the other species were, which may bias the detection function. This creates a factor level for the detection function to use (when \"species\" is a covariate) to distinguish between cue-relevant species that are within the specified pool and those that are not. The second option for other_species is \"ignore\", which does not reassign species codes to \"Other\", and ignores whether the species of interest held the plurality for a mixed species detection. The third option is \"remove\": any species re-assigned to \"Other\" will be removed before the detection function is fit; this can be useful if only a small number of species are re-assigned to \"Other\", which would then obviate species as a viable covariate (since the sample size of all species levels would be unlikely to exceed df_settings$covariates_n_per_level – see below). The fourth and final option is coerce, which forces all species codes to \"Other\" for the purposes of detection function fitting and abundance estimation. This is effectively the same as removing ‘species’ from the list of covariates, but this option can be a convenience if you want to quickly toggle the use of species as a covariate for a specific species pool, and/or produce abundance estimates for unidentified taxa (e.g., an ‘Unidentified dolphins’ species pool that includes multiple species codes).   df_settings The df_settings input specifies how to fit a detection function to the filtered data. It accepts a named list, which in our example will look like this: df_settings = list(covariates = c(&#39;bft&#39;,&#39;lnsstot&#39;,&#39;cruise&#39;,&#39;year&#39;,&#39;ship&#39;,&#39;species&#39;), covariates_factor = c(FALSE, FALSE, TRUE, TRUE, TRUE, TRUE), covariates_levels = 2, covariates_n_per_level = 10, detection_function_base = &#39;hn&#39;, base_model = &#39;~1&#39;, delta_aic = 2) (Note that all of these inputs have defaults, and are therefore optional.) covariates Covariates you wish to include as candidates in detection function models, provided as a character vector. The covariates must match columns existing within cruz$cohorts$&lt;cohort_name&gt;$sightings. Note that the function will ignore case, coercing all covariates to lowercase. Default: no covariates. covariates_factor A Boolean vector, which must be the same length as covariates, indicating whether each covariate should be treated as a factor instead of a numeric. Default: NULL. covariates_levels The minimum number of levels a factor covariate must have in order to be included as an eligible covariate. Default: 2. covariates_n_per_level The minimum number of observations within each level of a factor covariate. If this condition is not met, the covariate is excluded from the candidates. Default: 10. detection_function_base The base key for the detection function, provided as a character vector. Accepted values are \"hn\" (half-normal key, the default, which exhibits greater stability when fitting to cetacean survey data; Gerrogette and Forcada 2005), \"hr\" (hazard-rate), or c(\"hn\", \"hr), which will loop through both keys and attempt model fitting. base_model The initial model formula, upon which to build using candidate covariates. If not provided by the user, the default is \"~ 1\". delta_aic The AIC difference between the model yielding the lowest AIC and other candidate models, used to define the best-fitting models. Typically, AIC differences of less than 2 (the default) indicate effectively equal model performance. If this value is not zero, then model averaging will be done: if multiple models are within delta_aic of the model with the lowest AIC, all “best” models will be used in subsequent steps and their results will be averaged. See Details below. estimates The estimates input specifies which estimates of density and abundance to produce based on the fitted detection function. This input accepts a list of sub-lists, which in our example will look something like this: estimates &lt;- list(list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2017, regions = &#39;WHICEAS&#39;)) This example shows only a single sub-list, specifying how to generate a density/abundance estimate for striped dolphins (species code 013) within the “WHICEAS” geostratum for 2017. spp: A character vector of species codes. title: A title for this abundance estimate, given as a character vector, ’ e.g., \"Striped dolphin - pelagic\". If left blank, the species code(s) will be concatenated to use as a title. years: A numeric vector of years, used to filter data to include only effort/sightings from these years. regions: A character vector of geostratum names, used to filter the data. Any segment or sighting occurring within any (but not necessarily all) of the provided regions will be returned. This holds true for nested regions: for example, in analyses from the Central North Pacific, in which the Hawaii EEZ geostratum (\"HI-EEZ\") is nested within the larger geostratum representing the entire CNP study area (\"OtherCNP\"), an input of regions = \"OtherCNP\" will return segments/sightings both inside the Hawaii EEZ and outside of it. You also have the option of manually specifying other filters &amp; arguments. Each of these sub-lists accepts the following named slots: cruises: An optional numeric vector of cruise numbers, used to filter data to include effort/sighting from only certain cruises. Ignored if NULL. regions_remove: A character vector of geostratum names, similar to above. Any segment or sighting occurring within any of these not_regions will not be returned. Using the example above, if regions = \"OtherCNP\" and not_regions = \"HI-EEZ\", only segments occuring within OtherCNP and outside of HI-EEZ will be returned. This can be particularly useful for abundance estimates for pelagic stock that exclude nested insular stocks. g0: If left as the default NULL, the lta() function will automatically estimate the weighted trackline detection probability (g0) according to the distribution of Beaufort sea states contained within the survey years/regions for which density/abundance is being estimated (this is done using the LTabundR function g0_weighted()). This will only be done if the Rg0 input above is not NULL; if it is and you do not provide g(0) values here, g0 will be coerced to equal 1. To coerce g(0) to a certain value of your own choosing, you can provide a numeric vector of length 1 or 2. If length 1, this value represents g(0) for all schools regardless of size. If length 2, these values represent g(0) for small and large school sizes, as defined by g0_threshold below. g0_cv: Similar to g0 above: if left NULL, the CV of the g(0) estimate will be automatically estimated based on weighted survey conditions. Alternatively, you can manually specify a CV here, using a numeric vector of length 1 or 2. If you do not specify a value and Rg0 input is NULL, g0_cv will be coerced to equal 0. g0_threshold: The school size threshold between small and large groups. alt_g0_spp: An alternate species code to use to draw Relative g(0) values from the Rg0 input. This is useful in the event that Rg(0) was not estimated for the species whose density/abundance you are estimating, but there is a similarly detectable species whose Rg(0) parameters have been estimated. combine_g0: A Boolean, with default FALSE. If TRUE, weighted g0 estimates will be produced separately for each species code provided (specifically, for each unique row in the Rg0 table that is found after filtering by the species codes you provide in this estimate), THEN average those estimates together. This can be useful when you do not have a Rg(0) estimates for a certain species, but you can approximate g0 by averaging together estimates from multiple species (e.g., averaging together weighted g(0) from across rorqual species in order to get a weighted g(0) estimate for ‘Unidentified rorquals’). region_title: An optional character vector indicating the title you would like to give to the region pertaining to this estimate. This can be useful if you have a complicated assemblage of regions you are combining and/or removing. If not supplied, the function will automatically generate a region_title based on regions and regions_remove. forced_effort: If this is a single numeric value instead of NULL (NULL is the default), this value will be used as the survey effort, in km, in a brute-force method; this same value will be used for every year and region. This is only helpful if you are looking for a relatively easy way to compare results from your own analysis to another (e.g., comparing LTabundR results to reports from NOAA reports prior to 2021, in which effort was calculated slightly differently). area: If this is a single numeric value instead of NULL (NULL is the default), this value will be used as the area of the region in which abundance is being estimated, in square km, in a brute-force approach. If left NULL, the function will calculate the final area of the survey area resulting from the regions and regions_remove filters above. remove_land: A Boolean, with default TRUE, indicating whether or not land area should be removed from the survey area before calculating its area for abundance estimation. This term is only referenced if area is not specified manually. Here is the full estimates list for all the species-year-geostratum combinations for which we want to estimate density/abundance: estimates &lt;- list( list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2017, regions = &#39;WHICEAS&#39;), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2020, regions = &#39;WHICEAS&#39;), list(spp = &#39;026&#39;, title = &#39;Frasers dolphin&#39;, years = 2017, regions = &#39;WHICEAS&#39;), list(spp = &#39;026&#39;, title = &#39;Frasers dolphin&#39;, years = 2020, regions = &#39;WHICEAS&#39;), list(spp = &#39;031&#39;, title = &#39;Melon-headed whale&#39;, years = 2017, regions = &#39;WHICEAS&#39;), list(spp = &#39;031&#39;, title = &#39;Melon-headed whale&#39;, years = 2020, regions = &#39;WHICEAS&#39;)) Each of these sub-lists specifies the details for a single estimate of density/abundance, making it possible to produce multiple estimates from the same detection function model. Generally, there needs to be a sub-list for each species-region-year combination of interest. You can imagine that building up these sub-lists can get tedious. It can also introduce the possibility of error or inconsistencies across estimates of multiple species. To address that issue, LTabundR includes the function lta_estimates(), which makes your code for preparing an estimates object much more efficient. That function is demonstrated in the Case Studies chapters. Quickly review results: results$estimate title species Region Area year segments km 1 Striped dolphin 013 (WHICEAS) 402948.7 2017 29 3040.009 2 Striped dolphin 013 (WHICEAS) 402948.7 2020 34 4585.386 3 Frasers dolphin 026 (WHICEAS) 402948.7 2017 29 3040.009 4 Frasers dolphin 026 (WHICEAS) 402948.7 2020 34 4585.386 5 Melon-headed whale 031 (WHICEAS) 402948.7 2017 29 3040.009 6 Melon-headed whale 031 (WHICEAS) 402948.7 2020 34 4585.386 Area_covered ESW_mean n g0_est ER_clusters D_clusters N_clusters 1 11425.53 3.758386 3 0.2319146 0.0009868393 0.00056981695 229.60702 2 17620.38 3.842726 3 0.2034128 0.0006542525 0.00042079040 169.55696 3 NA NA 0 0.4766764 0.0000000000 0.00000000000 0.00000 4 18348.83 4.001588 1 0.4469301 0.0002180842 0.00006097084 24.56812 5 10292.25 3.385600 2 0.5250732 0.0006578929 0.00018588382 74.90165 6 16061.18 3.502688 2 0.4966077 0.0004361683 0.00012572342 50.66009 size_mean size_sd ER D N 1 31.95079 0.5780968 0.03153029 0.01822835 7345.092 2 53.95865 2.7402089 0.03530258 0.02269332 9144.245 3 NA NA 0.00000000 0.00000000 0.000 4 261.32841 NA 0.05699159 0.01593341 6420.348 5 185.98103 96.4369707 0.12235559 0.03371770 13586.505 6 324.68110 58.0826568 0.14161562 0.04054800 16338.766 More details on the lta() output are provided below. Variance estimation By default, the lta() function produces a single estimate of the detection function and a single estimate of density/abundance estimate for each sub-list within estimates(). However, you can obtain the coefficient of variation (CV) of those estimates by activating the function’s bootstrap variance estimation feature. To do this, add bootstraps as an input specifying a large number of iterations (1,000 iterations is standard, but we suggest first testing your code with 5 - 10 bootstraps before committing; the function typically requires ~1 hour per 100 bootstraps.). For the purposes of example only, we will just use 10 bootstrap iterations here: results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, bootstraps = 10) This command will first produce official estimates of the detection function and density/abundance, then it will repeat the analysis for the number of iterations you have specified. In each iteration, survey segments are re-sampled according to standard bootstrap variance estimation methods (see more details below, in “Behind the Scenes”). Other inputs There are a few other optional input arguments that lend further control over the lta() procedure. lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, ss_correction = 1, bootstraps = 10 toplot = TRUE, verbose = TRUE,) use_g0: A Boolean, with default TRUE, indicating whether or not to use custom g(0) value(s). If FALSE, the assumed g(0) value will be 1. This is an easy way to toggle on-and-off automated g(0) estimation and/or ignore manually supplied g(0) values. ss_correction: Should a correction be applied to school sizes? School sizes will be scaled by this number. The default, 1, means no changes will occur. toplot: A Boolean, with default TRUE, indicating whether detection function plots (Distance::plot.ds()) should be displayed as the candidate models are tested. verbose: A Boolean, with default TRUE, indicating whether or not updates should be printed to the Console. Output During processing While lta() is running, it will print things to the Console (if verbose is TRUE), plot diagnostic plots of how the study area is being calculated (if toplot is TRUE), and plot detection function model candidates (if toplot is TRUE). To demonstrate this, we will run the estimate for striped dolphins in 2017 only, without variance bootstrapping. To expedite processing, we will manually supply g(0) values from Bradford et al. (2021) (this saves about 3 minutes per estimate): new_estimates &lt;- list( list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2017, regions = &#39;WHICEAS&#39;, g0 = 0.35, g0_cv = 0.19)) # Run it: demo &lt;- lta(cruz, Rg0, fit_filters, df_settings, new_estimates, bootstraps = 0) Additionally, windows will appear showing details for the detection function models and details of the density/abundance estimate. Outputs The lta() function returns a list of objects. To demonstrate this output, we will pull back in the dataset representing the result of the analysis above, for all three species in both years (with 5 bootstrap iterations): This list of results has five slots: names(results) [1] &quot;pool&quot; &quot;inputs&quot; &quot;estimate&quot; &quot;df&quot; &quot;bootstrap&quot; pool: The species pool pertaining to these estimates. inputs: A list of the inputs used to produce these estimates. estimate: A table of density/abundance estimates for each species/region/year combination specified in the estimates input. results$estimate title species Region Area year segments km 1 Striped dolphin 013 (WHICEAS) 402948.7 2017 29 3040.009 2 Striped dolphin 013 (WHICEAS) 402948.7 2020 34 4585.386 3 Frasers dolphin 026 (WHICEAS) 402948.7 2017 29 3040.009 4 Frasers dolphin 026 (WHICEAS) 402948.7 2020 34 4585.386 5 Melon-headed whale 031 (WHICEAS) 402948.7 2017 29 3040.009 6 Melon-headed whale 031 (WHICEAS) 402948.7 2020 34 4585.386 Area_covered ESW_mean n g0_est ER_clusters D_clusters N_clusters 1 11425.53 3.758386 3 0.2319146 0.0009868393 0.00056981695 229.60702 2 17620.38 3.842726 3 0.2034128 0.0006542525 0.00042079040 169.55696 3 NA NA 0 0.4766764 0.0000000000 0.00000000000 0.00000 4 18348.83 4.001588 1 0.4469301 0.0002180842 0.00006097084 24.56812 5 10292.25 3.385600 2 0.5250732 0.0006578929 0.00018588382 74.90165 6 16061.18 3.502688 2 0.4966077 0.0004361683 0.00012572342 50.66009 size_mean size_sd ER D N 1 31.95079 0.5780968 0.03153029 0.01822835 7345.092 2 53.95865 2.7402089 0.03530258 0.02269332 9144.245 3 NA NA 0.00000000 0.00000000 0.000 4 261.32841 NA 0.05699159 0.01593341 6420.348 5 185.98103 96.4369707 0.12235559 0.03371770 13586.505 6 324.68110 58.0826568 0.14161562 0.04054800 16338.766 df: A named list with details for the detection function. results$df %&gt;% names [1] &quot;best_models&quot; &quot;all_models&quot; &quot;best_objects&quot; &quot;sightings&quot; &quot;sample_size&quot; [6] &quot;curve&quot; results$df$best_models Model Key_function Formula Pmean AIC 1 9 hn ~1 + ship + bft + lnsstot 0.5609598 939.916 2 11 hn ~1 + ship + bft + lnsstot + species 0.5552113 940.489 3 6 hn ~1 + ship + bft 0.5658167 941.858 $\\\\Delta$AIC Covariates tested pool 1 0.000 bft, lnsstot, ship, species Multi-species pool 1 2 0.573 bft, lnsstot, ship, species Multi-species pool 1 3 1.942 bft, lnsstot, ship, species Multi-species pool 1 bootstrap: If bootstrap variance estimation was carried out, the output would also include bootstrap, a named list with results from the bootstrap process, only returned if the bootstraps input is greater than 1. results$bootstrap %&gt;% names [1] &quot;summary&quot; &quot;details&quot; &quot;df&quot; results$bootstrap$summary %&gt;% head # A tibble: 6 × 18 # Groups: title, Region [3] title Region year species iterations ESW_mean g0_mean g0_cv km ER &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Frasers … (WHIC… 2017 026 10 NaN 0.527 0.706 3025. 0 2 Frasers … (WHIC… 2020 026 10 4.10 0.421 0.878 4354. 0.0562 3 Melon-he… (WHIC… 2017 031 10 3.30 0.546 0.682 3025. 0.165 4 Melon-he… (WHIC… 2020 031 10 3.33 0.338 0.747 4354. 0.109 5 Striped … (WHIC… 2017 013 10 3.78 0.238 0.136 3025. 0.0331 6 Striped … (WHIC… 2020 013 10 3.87 0.205 0.0971 4354. 0.0362 # ℹ 8 more variables: D &lt;dbl&gt;, size &lt;dbl&gt;, Nmean &lt;dbl&gt;, Nmedian &lt;dbl&gt;, # Nsd &lt;dbl&gt;, CV &lt;dbl&gt;, L95 &lt;dbl&gt;, U95 &lt;dbl&gt; Unusual estimate scenarios Most line-transect estimates in most areas are relatively straightforward: you want an estimate for a single species in a single year, and your geostrata do not overlap, nor do they need to be stratified or combined in funny ways. But there will also be unusual and slightly more complicated scenarios. We outline some of those below and demonstrate how they can be handled within the LTabundR framework. Species combinations &amp; g(0) Multi-species schools Multi-species schools can confound detection function model fitting, since your species of interest may not be the predominant species in the group, which means that the other species present may be having a greater influence over the detection function. You can decide how to account for this using the other_species slot in your fit_filters list. See the details on this discussed above. Species pools When you don’t have enough sightings of individual species to model a detection function effectively, it can be useful to pool sightings from multiple species who have similar detection characteristics. This is a common tactic in the Central North Pacific. When you do this, you typically need to make the following changes to a “normal” lta() call: In your df_settings list, consider adding species as a covariate, and ensure that you specify that it should be treated as a factor. This may improve detection function model fit. In your fit_filters list, specify multiple species codes and name your species pool accordingly (e.g., “Multi-species pool 1”). In your fit_filters list, specify how to handle “Other” species (see above). In your estimates list, add a sub-list for each species-region-year for which you want a density/abundance estimate. We took all of these steps in the example above with striped dolphins, Fraser’s dolphins, and melon-headed whales. Use that code as a guide. Pooling similar species Species that can be confused with one another may need to be pooled together for abundance estimation. For example, in the northeast Pacific, sei whales, Bryde’s whales, and fin whales can co-occur but they are difficult to distinguish in the field. They are also relatively uncommon, and may need to be pooled with other species in order to obtain a sound detection function model. To account for this, we want to estimate the density/abundance in the WHICEAS study area of all detections of sei, Bryde’s, fin whales together. To handle this, we will follow all the steps taken for a multi-species pool, as discussed above. Additionally, in our estimates sub-list(s), we will specify (1) that multiple species should be included in the estimate, and (2) that the weighted g(0) estimates for each of the individual species should be averaged together: estimates &lt;- list( list(spp = c(&#39;072&#39;, &#39;073&#39;, &#39;099&#39;, &#39;074&#39;), title = &quot;Sei/Bryde&#39;s/Fin whale&quot;, years = 2017, regions = &#39;WHICEAS&#39;, combine_g0 = TRUE)) Rare unidentified taxa A similar problem occur when you have species codes for unidentified taxa that have been identified down to a family- or genus-level. For example, “Unidenfitied Mesoplodon” is a species code (the code is \"050\") for any beaked whale that is definitely in the genus Mesoplodon. There are plenty of these sightings, which means g(0) and its CV can be estimated just fine without referring to other species codes. Other unidentified taxa, however, are less common. For example, in Hawaiian studies, density/abundance is estimated for “Unidentified rorquals”. In the field there is a species code for this group, \"070\", but it is rarely used – there are enough sightings to model the detection function, but not nearly enough sightings to estimate g(0) or its CV. In this case, we need to combine g(0) from more common species codes in order to estimate the unidentified rorqual’s g(0). To do this, we fit a detection function using species code \"070\" … fit_filters &lt;- list(spp = c(&#39;070&#39;), pool = &#39;Unidentified rorqual&#39;, cohort = &#39;all&#39;, truncation_distance = 5.5) … then, in our estimates list, we specify some alternate g(0) species designations. estimates &lt;- list( list(spp = &#39;070&#39;, title = &#39;Unidentified rorqual&#39;, years = 2017, regions = &#39;WHICEAS&#39;, alt_g0_spp = c(&#39;071&#39;,&#39;099&#39;,&#39;074&#39;,&#39;075&#39;), combine_g0 = TRUE)) Geostratum combinations In your estimates sub-lists, the regions and regions_remove slots give you control of the geographic scope of (1) the weighted g(0) and CV used in density estimation, (2) the effort and sightings used to estimate density, (3) and the area used to calculate abundance. A note on cohort geostrata Recall that, when processing your survey data to create a cruz object, you provide a list of geostrata as an argument in your process_surveys() call. You also have the option to specify a subset of those geostrata for each species cohort (see load_cohort_settings()), which is an option that you should almost always use. Selecting a subset of geostrata is important because that subset is used to “segmentize” your survey data – i.e., break effort into discrete sections that can be bootstrap-sampling during the lta() variance estimation routine – and the segmentizing procedure always breaks segments when a survey passes from one geostratum to another. This matters because the lta() bootstrapping routine will re-sample survey segments in a way that preserves the proportion of segments occurring in each geostratum, to ensure that all geostrata are represented in the same proportion as the original estimate. When segments are unncessarily broken into small segments by irrelevant geostrata that have been included in the analysis, the bootstrap estimate of the CV is likely to be too large. For example: in the Central North Pacific, there are about 11 geostrata commonly used. These include the Hawaiian EEZ geostratum, the Main Hawaiian Islands geostratum, and the larger CNP geostratum that represents the maximum range of the study area. These three geostrata are typically all you need for most density estimates for most species. However, a few species – e.g., bottlenose dolphin, pantropical spotted dolphin, and false killer whale – have special geostrata that represent insular stock boundaries and/or pelagic stock boundaries. If those insular geostrata are used in density estimates for which they do not apply, they will confound the bootstrap estimate of density/abundance CV. Punchline: be sure to specify only the relevant geostrata in each cohort’s settings. Combining disparate geostrata For example, in Hawaii bottlenose dolphins belong to a pelagic stock as well as several insular stocks. If you wished to estimate the abundance of all insular stocks together, you simply provide their respective geostratum names in the regions slot of your estimates sub-list: estimates &lt;- list(list(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;, years = 2017, regions = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;))) Removing insular geostrata Conversely, you may wish to estimate density/abundance for pelagic bottlenose dolphins only, ignoring the insular stocks. You can substract geostrata using the regions_remove slot: estimates &lt;- list(list(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;, years = 2017, regions = &#39;HI_EEZ&#39;, regions_remove = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;))) Combine partially overlapping geostrata Say you want to estimate the density/abundance for a set of geostrata that partially overlap. An example of this is that the Northwest Hawaiian Islands geostratum overlaps slightly with the Main Hawaiian Islands geostratum. This is not an issue; when study area is calculated within lta() (actually, that function calls another function, strata_area(), to do this. That function is demonstrated below), overlap among strata is accounted for. estimates &lt;- list(list(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;, years = 2017, regions = c(&#39;MHI&#39;,&#39;NWHI&#39;))) Regionally stratified analysis Field surveys are sometimes stratified such that trackline design and/or density can differ substantially across regions. Also, analysts may sometimes wish to estimate density/abundance for individual regions separately, regardless of design stratification. In lta(), you can accommodate a stratified analysis by providing an estimates sub-list for each geostratum. For example, in 2002 the Hawaiian EEZ was surveyed with different effort intensity in the Main Hawaiian Islands region compared to pelagic waters. For that reason, density/abundance estimates ought to be stratified by region: estimates &lt;- list( list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2002, regions = &#39;MHI&#39;), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2002, regions = &#39;HI_EEZ&#39;, regions_remove = &#39;MHI&#39;)) Here we have one 2002 estimate for the Main Hawaiian Islands, and a second for the pelagic Hawiian EEZ, achieved by subtracting the \"MHI\" stratum from the \"HI_EEZ\" stratum. Once lta() processing is complete, you can summarize and plot the results for each study area separately. The next step is to combine the stratified estimates to generate a grand estimate for the entire EEZ. This is achieved using the LTabundR functions lta_enlist() and lta_destratify(). We discuss this further in a later chapter. Subgroup-based analyses After 2010, Pacific Islands Fisheries Science Center (PIFSC) began a sub-group protocol referred to as the “PC Protocol” after the scientific name for false killer whales, Pseudorca crassidens, which was the species for which the protocol was designed. False killer whales are rare and occur in dispersed subgroups, which complicates conventional distance sampling approaches to line-transect analysis. To handle this, a separate, subgroup-based analytical approach was developed in 2014 - 2017. This approach could theoretically be used for other species that occur in subgroups. We cover this on a separate page. Other scenarios Study periods that span years, such as a December - January survey. This is not yet handled well within the LTabundR framework. More will go here. Behind the scenes Step-by-step operations The following list is a summary of each step of the lta() function, in order of operations. Function inputs are checked for valid formatting, and any optional inputs that are not user-provided are given default values. For each sublist in your estimates input, the following actions are taken: Sublist inputs are checked for valid formatting, and any optional inputs that are not user-provided are given default values. A polygon is created based on the sublist’s slots for regions (strata to include in abundance estimation) and region_remove (strata to exclude). This process is detailed in the Area Estimation section below. The spatial area, in square km, is calculated for that polygon. Weighted g(0) values are estimated based on the survey effort related to this sublist: Determine the species code(s) to be used to filter the Rg0 dataset. Usually this is the species code(s) in the spp slot, but if the sublist has an alt_g0 slot, the code(s) in that slot is used to filter the Rg(0) dataset. If multiple codes are provided, and the sublist has a slot named combine_g0 with the value TRUE, then weighted g(0) values will be estimated for each species then those values will be averaged together a few steps down. (If multiple species codes are provided but combine_g0 is not provided or is FALSE, then only the first species code will be used.) If the species code(s) are not found in the Rg0 dataset, weighted g(0) will be coerced to 1.0 with CV of 0. Survey effort is filtered down according to the relevant slots in the sublist – years, regions (the strata to include), and, optionally, regions_remove (the strata to exclude) — as well as the lta() inputs abund_eff_types (which EffortType values are usable for abundance estimation) and abund_bft_range (which Beaufort sea states are usable). That survey effort is used to estimate weighted g(0) and its CV for each species code using the LTabundR function g0_weighted(). That function does the following: Determines the proportion of survey effort occurring in each Beaufort state (this is done using the LTabundR function summarize_bft()). Uses those sea state proportions as weights to calculate the weighted mean g(0) based on the values in Rg(0). For example, if the Rg(0) values for species code \"046\" (sperm whale) in Beaufort states 0 to 6 are 1.00, 0.87, 0.75, 0.65, 0.57, 0.49, and 0.42, respectively, and the proportion of effort in those respective sea states are 0.01, 0.03, 0.07, 0.19, 0.35, 0.24, and 0.11, then the weighted g(0) is 0.679. This weighted g(0) is the value provided in the final results table for the function. To estimate the CV of this weighted g(0), a MCMC routine is used that first approximates the distribution of g(0) … (details needed here) … This is essentially a direct adaptation of the code developed by Jeff Moore and Amanda Bradford, and described in Bradford et al. (2021). If multiple species codes were provided and therefore multiple estimates of weighted g(0) were produced, those estimates are averaged using the LTabundR function g0_combine(). That function uses the same equations as the Excel spreadsheet used in the Bradford et al. (2022) WHICEAS study. The sublist is updated with these resulting values: the study area polygon, the estimated of weighted g(0), its CV, and the proportional effort in each Beaufort state. A reference list is built containing details for the g(0) distributions for each sublist in estimates. This is done in order to facilitate easy recall during the bootstrapping phase later on in the function. Each weighted g(0) and its CV are passed to the LTabundR function g0_optimize(), which returns the parameters needed to generate a distribution from which to draw bootstrap values of g(0). That function is a near-exact replica of the code developed by Jay Barlow to produce bootstrapped estimates in Barlow (2006). It involves an optimization routine which can take several seconds to complete, so this step is conducted once here rather than repeatedly during bootstrap iterations. The cruz object is filtered to survey data for the specified cohort, to segments for which the column use is TRUE, and to sightings for which the column included is TRUE. The sightings are filtered further to include only the species specified in the input fit_filters$spp. The resulting datasets will be used for modeling the detection function, and a subset of the datasets will be used for estimating abundance. Handling “Other” species designations: if the input fit_filters has a slot for other_species, certain species codes in mixed-species sightings may be modified if they do not constitute the plurality of the group size. See details in Fit filters above. If any of the covariates requested within the input df_settings$covariates have missing values (i.e., NA), those sightings will be removed from the dataset used for fitting detection functions and estimating abundance. Similarly, any sightings with invalid group size estimates (the column ss_valid == FALSE) are removed. The sample sizes used in fitting the detection function are stored. If the input df_settings$simplify_cue exists and is TRUE, cue codes (column Cue in the sightings dataset) are simplified to aid in overcoming factor level sample size limitations. If the input df_settings$simplify_bino exists and is TRUE, sighting method codes (column Method in the sightings dataset) are simplified to aid in overcoming factor level sample size limitations. If any of the covariates to be used in modeling the detection function should be treated as a factor (this is specified in the input df$covariates_factor), then the covariates columns are updated as such. All covariates specified as a factor are first tested for eligibility. Only factors with at least two levels (or whatever you specified with df_settings$covariates_levels) and 10 observations in each level (or whatever you specified with df_settings$covariates_n_per_level) are eligible for inclusion in the final set of covariates. The final results object is staged; this is a list with slots for data regarding the species pool, the inputs used, the detection function model, and the resulting estimates. A large loop is then initiated, with the first iteration being the formal estimate. Subsequent iterations are part of the bootstrapping process for estimating CV and confidence intervals. The detection function is modelled based on the sightings, covariates and truncation distance provided. This is done by calling the LTabundR function df_fit(), which depends upon the detection function modeling function mrds() from the package Distance. See further details in the section below on “Fitting a detection function”. Detailed results of the best-fit model are stored, its detection curve is generated using the LTabundR function df_curve(), and the Effective Strip Half-Width estimates for each sighting is added to the sightings data. Segments and sightings are filtered to only data appropriate for estimating density/abundance. This means OnEffort is TRUE, EffortType is one of the variables in the lta() input abund_efftypes (the defaults is just \"S\"), Bft can be rounded to one of the values provided in the input abund_bft_range (the default is 0:6), and sightings occur within the truncation distance. Loop through each sublist in the lta() input estimates: Segments and sightings are filtered to the species, years, cruises, and regions specified. Abundance is estimated using the LTabundR() function abundance(), based upon the filtered data and the g(0) value for this sublist. The resulting Effective Strip Half-Width (ESW) is based on the mean ESW of the sightings used to estimate abundance. The resulting g(0) is based on the mean weighted g(0) value for each sighting, since in some use cases a different g(0) value is supplied for small vs. large schools. Results are stored. If a filepath was supplied for the lta() input results_file, an RData object with all of the results up to this point is saved to file. The loop is repeated for bootstrap iterations if the lta() input bootstraps is not NULL and is a number greater than 0. The differences in these bootstrap iterations from the original estimate routine are as follows: Before the detection function is modeled, bootstrapped versions of the segments and sightings are generated using the LTabundR function prep_bootstrap_datasets(). This function (1) determines the number of segments occurring within each stratum; (2) resamples the segment IDs within each stratum (with replacement), to yield the same number of segments (though some segments may be replicated) and preserve the relative distribution of effort across strata; then (3) gathers the sightings associated with each of those bootstrapped segments. If a segment is counted twice, two copies of its respective sightings are used. These new versions of the segments and sightings are used to model the detection function, and are then filtered to estimate density/abundance. Before abundance is estimated, a new g(0) value is drawn randomly from a distribution that is based on the parameters stored in the reference table described in step 3 above. The code for doing this is copied directly from the routine written by Jay Barlow’s for his 2006 report. After abundance is estimated, details of the bootstrap iteration are saved and the results_file is updated. At the end of all bootstrap iterations, a summary of the bootstrap iteration process is generated and the results_file is updated. The CV of g(0) in this summary table is based on the bootstrapped values of g(0). The confidence intervals for density/abundance in this summary table are generated using the LTabundR function lta_ci(). Other details Area estimation {-} Unless you manually specify the study area in your estimates list, lta() will calculate your study area for you based on the geostrata you provide. It does so by calling the LTabundR function strata_area(), which you can use on your own to explore geostratum combination options. This function was designed using the sf package to handle complex polygon combinations, and it uses Natural Earth datasets to remove land within your study area (this is a feature you can turn off, if you want). Here are some examples of how strata_area() handles complex scenarios. Say you want to estimate abundance in the ‘WHCEAS’ study area, but you want to make sure the study area estimate is accurately removing land: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;WHICEAS&#39;), verbose = FALSE) Say you want to estimate abundance in the pelagic Hawaiian EEZ, ignoring effort and sightings within the Main Hawaiian Islands stratum and accurately removing small islands in northwest Hawaii: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;HI_EEZ&#39;), strata_remove = c(&#39;MHI&#39;), verbose = FALSE) Say you want to estimate abundance of pelagic bottlenose dolphins within the WHICEAS study area, ignoring the insular stocks: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;WHICEAS&#39;), strata_remove = c(&#39;Bottlenose_KaNi&#39;,&#39;Bottlenose_OUFI&#39;,&#39;Bottlenose_BI&#39;), verbose = FALSE) Say you want to estimate abundance for only the insular bottlenose stocks: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;Bottlenose_KaNi&#39;,&#39;Bottlenose_OUFI&#39;,&#39;Bottlenose_BI&#39;), verbose = FALSE) Say you want to estimate abundance for false killer whales within the Northwest Hawaiian Islands and Main Hawaiian Islands study areas combined, but those geostrata partially overlap: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;MHI&#39;,&#39;NWHI&#39;), verbose = FALSE) Say you want to estimate abundance for the Hawaiian EEZ outside of those partially overlapping geostrata: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = &#39;HI_EEZ&#39;, strata_remove = c(&#39;MHI&#39;,&#39;NWHI&#39;), verbose = FALSE) g(0) estimation If you want lta() to calculate a weighted g(0) estimate (and associated CV) that is specific to the conditions associated with your estimates sub-list parameters, all you need to do is provide the Rg0 input. When you do this, the lta() function will find the Rg0 values associated with the species code(s) in your estimates sub-list, then calculate weighted g(0) and its CV using the LTabundR function, g0_weighted(), which we discussed and demonstrated on the previous page. If lta() can’t find your species code in the Rg0 table you provide, it will give up and assume that g(0) is 1.0 and that g0_cv is 0.0. If your estimates sub-list has a alt_g0_spp slot, lta() will use that species code instead to filter the Rg0 table. If your estimates sub-list has a combine_g0 slot that is TRUE, lta() will filter the Rg0 table using all species codes you provide. If that filtration results in multiple Rg0 species being found, weighted g(0) will be calculated for each of those species separately, then those g(0) estimates will be combined using a geometric mean (using the LTabundR function g0_combine()). If combine_g0 is FALSE, only the first species code provided in your estimates sub-list will be used to filter Rg0. If you want to supply a weighted g(0) estimate and its CV yourself, you can add the g0 and g0_cv slots to your estlimates sublist, as explained above. If you want to coerce g(0) to be assumed to be 1.0 (with CV = 0.0), you can either (1) not supply the Rg0 input, or (2) manually specify the g0 and g0_cv slots in your estimates sub-list accordingly. Fitting a detection function {-} The detection function is estimated using functions in the package mrds, primarily the main function mrds::ddf(), which uses a Horvitz-Thompson-like estimator to predict the probability of detection for each sighting. If multiple base key functions (e.g., half-normal or hazard-rate) are provided, and/or if covariates are specified, model fitting is done in a forward stepwise procedure: In the first round, the base model (no covariates, i.e., \"~1\") is fit first. In the second round, each covariate is added one at a time; at the end of the round, the covariate, if any, that produces the lowest AIC below the AIC from the previous round is added to the formula. This process is repeated in subsequent rounds, adding a new covariate term in each round, until the AIC no longer improves. If a second base key is provided, the process is repeated for that second key. All models within delta_aic of the model with the lowest AIC qualify as best-fitting models. The best-fitting model(s) is(are) then used to estimate the Effective Strip half-Width (ESW) based on the covariates associated with each sighting. If multiple best-fitting models occur, we will find the average ESW for each sighting across all models, using a weighted mean approach in which we weight according to model AIC. To turn off this model averaging step, set delta_aic to 0 to avoid passing multiple models to the abundance estimation stage. Note that if LnSsTot is included as a covariate, the function will (1) check to see if the sightings dataframe has a column named ss_valid (all cruz objects do), then, if so, (2) filter sightings only to rows where ss_valid is TRUE, meaning the school size estimate for that sighting is a valid estimate. This stage of the lta() command is executed within a backend function, LTabundR::fit_df(), which has its own documentation for your reference. Estimating density &amp; abundance Estimates are produced for various combinations of species, regions, and years, according to the arguments specified in your estimates list(s). Before these estimates are produced, we filter the data used to fit the detection function to strictly systematic (design-based) effort (i.e., EffType = \"S\"), in which standard protocols are in use (i.e., OnEffort = TRUE) and the Beaufort sea state is less than 7 (though these controls can be modified using the lta() inputs abund_eff_types and abund_bft_range (see above). Note that if sightings has a column named ss_valid (all standard cruz objects do) and any of the rows in that column are FALSE, those rows will have their best school size estimate (which will be NA or 1, since they are invalid) replaced by the mean best estimate for their respective species. This stage of the lta() command is executed within a back-end function, LTabundR::abundance(), which has its own documentation for your reference. Bootstrap variance estimation If the bootstraps input value is greater than 1, bootstrap variance estimation will be attempted. In each bootstrap iteration, survey segments are re-sampled with replacement before fitting the detection function and estimating density/abundance. Re-sampling is done in a routine that preserves the proportion of segments from each geostratum. Note that the entire process is repeated in each bootstrap: step-wise fitting of the detection function, averaging of the best-fitting models, and density/abundance estimation for all species/region/year combinations specified in your estimates input. At the end of the bootstrap process, results are summarized for each species/region/year combination. 95% confidence intervals are calculated using the BCA method (package coxed, function bca()). g(0) values during bootstrapping When conducting the non-parametric bootstrap routine to estimate the CV of density and abundance, uncertainty is incorporated into the g(0) value in each iteration using a parametric bootstrapping subroutine: First, a logit-transformed distribution is modeled based upon the mean and CV of g(0) provided by the user in the estimates input (see documentation for LTabundR::g0_optimize() for details on this step). This modeled distribution is used to randomly draw a g(0) value for each iteration of the density/abundance bootstrap routine. In this way, the uncertainty in g(0) is propagated into uncertainty in density/abundance. Returned estimates for weighted g(0) Note that the results object returned by lta() (we will refer to this object as lta_result here) contains various g(0) estimates in various places. Here is a key: In lta_result$estimate, you will find the columns g0_small (the weighted g(0) estimate for small group sizes) and g0_large (the weighted g(0) estimate for large group sizes). These values may differ if you manually provided different weighted g(0) estimates for small amnd large schools. In most cases, however, the two values will be the same. You will also find the column g0_est, which is the average g(0) of the detections used for the point estimate. This can differ from g0_small and g0_large since some schools may be large and some may be small. You will also find g0_cv_small and g0_cv_large which are the CV’s of g0_small and g0_large, respectively, that are estimated using a Monte Carlo Markov Chain routine described in the Step-by-Step outline provided above. If small vs large estimates of g(0) do not differ, the CVs will be the same. These four values (g0_small, g0_large, g0_cv_small, and g0_cv_large) are the weighted g(0) estimates reported by the LTabundR functions provided for summarizing your results (e.g., lta_diagnostics() and lta_report()). In lta_result$bootstrap$summary, you will find the columns g0_mean and g0_cv, which report the mean and CV, respectively, of the parametric bootstrap values of g(0) used in each bootstrap iteration. These should be similar to the g(0) values you find in lta_result$estimate but will maybe not exactly equal. In lta_result$bootstrap$details, you will find a column g0_est which provides the bootstrapped g(0) value used in each iteration of the bootstrap routine. "],["destratify.html", " 9 Stratified analysis", " 9 Stratified analysis Field surveys are sometimes stratified such that trackline design and/or density can differ substantially across regions. Also, analysts may sometimes wish to estimate density/abundance for individual regions separately, regardless of design stratification. On the previous page, we demonstrated how to accommodate a stratified analysis by providing an estimates sub-list for each geostratum. For example, in 2002 the Hawaiian EEZ was surveyed with different effort intensity in the Main Hawaiian Islands region compared to pelagic waters. Here is the code that generates density/abundance estimates of striped dolphins in 2002 (stratified) and 2010 (unstratified), with only 10 bootstrap iterations: # Survey data data(&quot;cnp_150km_1986_2020&quot;) cruz &lt;- cnp_150km_1986_2020 # Rg0 table data(&quot;g0_results&quot;) Rg0 &lt;- g0_results # Detection function filters fit_filters &lt;- list(spp = c(&#39;013&#39;, &#39;026&#39;, &#39;031&#39;), pool = &#39;Multi-species pool 1&#39;, cohort = &#39;all&#39;, truncation_distance = 5, other_species = &#39;remove&#39;) # Detection function settings df_settings &lt;- list(covariates = c(&#39;bft&#39;,&#39;lnsstot&#39;,&#39;cruise&#39;,&#39;year&#39;,&#39;ship&#39;,&#39;species&#39;), covariates_factor = c(FALSE, FALSE, TRUE, TRUE, TRUE, TRUE), covariates_levels = 2, covariates_n_per_level = 10, detection_function_base = &#39;hn&#39;, base_model = &#39;~1&#39;, delta_aic = 2) # Estimates estimates &lt;- list( list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2002, regions = &#39;MHI&#39;), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2002, regions = &#39;HI_EEZ&#39;, regions_remove = &#39;MHI&#39;), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2010, regions = &#39;HI_EEZ&#39;)) # Run it results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, bootstraps = 10) # Save it locally saveRDS(results, file=&#39;lta/multispecies_pool_1.RData&#39;) Let’s read these results back in using the LTabundR function lta_enlist(), which stores LTA results in a flexible list structure. ltas &lt;- lta_enlist(&#39;lta/&#39;) As these results stand, 2002 estimates are stratified into 2 separate regions: (ltas %&gt;% lta_report(verbose = FALSE))$table4 Error: ! Names repair functions can&#39;t return `NA` values. Now let’s process these LTA results through an LTabundR function, lta_destratify(), which will combine the separate regional estimates from 2002 into a single estimate for the year. ltas_2a &lt;- lta_destratify(ltas, years = 2002, combine_method = &#39;arithmetic&#39;, new_region = &#39;(HI_EEZ)&#39;) The new_region argument specifies how to refer to the combined region. In this case we want the 2002 study area to be named the same as the unstratified 2010 study area, hence \"(HI_EEZ)\". The combine_method argument is explained below. Now let’s re-check the summary table: (ltas_2a %&gt;% lta_report(verbose=FALSE))$table4 Error: ! Names repair functions can&#39;t return `NA` values. There is now only one set of columns for 2002, and the values therein are combinations of the stratified regions. The “de-stratification” routine within lta_destratify() sums abundance estimates across regions to get combined abundance. To estimate density in the combined regions, the function uses weighted averaging in which the area of a geostratum serves as its weight. To estimate CV and confidence interval of the combined result, the function uses one of two combine_methods (this is an argument in the function). The default method is \"arithmetic\", which uses classic formulae to estimate the combined variance and the corresponding confidence interval. This is done in a way that allows multiple geostrata to be combined, not just two. The second option, \"bootstrap\", uses an iterative method that draws bootstrap samples, with replacement, from the bootstrap estimate of density within each stratified region. ltas_2b &lt;- lta_destratify(ltas, years = 2002, combine_method = &#39;bootstrap&#39;, new_region = &#39;(HI_EEZ)&#39;) (ltas_2b %&gt;% lta_report(verbose=FALSE))$table4 Error: ! Names repair functions can&#39;t return `NA` values. Note that use of lta_destratify() only makes sense if the stratified regions have zero overlap. "],["trend-analysis.html", " 10 Trend analysis", " 10 Trend analysis When a species’ density appears to change dramatically from one survey year to the next, it could be due to several factors: the species’ abundance may have changed; its range may have shifted; or the timing of its migratory movements may have shifted. This apparent change could also be due solely to random chance: you can sample the exact same population in two different surveys, and you are liable to produce different abundance estimates due simply to random variation in how often you encounter your target species. In other words, random variation in the encounter rate may lead you to estimate a change in abundance, when in fact there is no change. For this reason, whenever you suspect that abundance has changed between years – i.e., whenever the confidence intervals for two years do not overlap – it is good practice to carry out follow-up tests. One such test was developed in (Bradford et al. 2020 and Bradford et al. (2021). That test has been provided in LTabundR with the function er_simulator(), which refers to a simulation-based test of random variation in the encounter rate (ER). This function uses randomization simulations to test for the probability that year-to-year changes observed in a species’ encounter rate are due to random sampling variation (and not actual change in the encounter rate). More specifcially, this function uses bootstrap sampling of survey segments to see if random variation in sampling could possibly produce an apparent but immaterial change in encounter rate across years. You will find full analytical details in the Appendix to Bradford et al. (2020) for analytical details, but briefly: in each bootstrap iteration, survey segments are resampled in a way that preserves the proportion of effort occurring within each geostratum in the data. The resampled data are used to calculate the overall ER across all survey years, since the null hypothesis is that the ER does not change across years. This overall ER is used to predict the number of sightings in each year, based on the distance covered by the resampled segments in each year. This process is repeated (typically hundreds to thousands of times) to produce a distribution of predicted sighting counts in each year. This distribution reflects the range of ERs that could be possible due simply to random variation and not to underlying changes in abundance. These distributions are compared to the actual number of sightings observed in their respective year. The fraction of simulated sightings counts that are more extreme than the observed count reflects the probability that the observed count is due to random sample variation alone. For example, Bradford et al. (2021) found non-overlapping confidence intervals in their estimates of Bryde’s whale abundance in 2002, 2010, and 2017. To test for the significance of these trends, they carried out the “ER simulator” routine described above. In LTabundR, we would carry out the same analysis as follows: Take your processed data: data(cnp_150km_1986_2020) cruz &lt;- cnp_150km_1986_2020 Filter it to systematic effort in the years of interest: cruzi &lt;- filter_cruz(cruz, analysis_only = TRUE, years = c(2002, 2010, 2017), regions = &#39;HI_EEZ&#39;, eff_types = &#39;S&#39;, bft_range = 0:6) Conduct the ER simulation, passing the species code for the Bryde’s whale (\"072\"). For the purposes of this example, we will only use 100 iterations. er_results &lt;- er_simulator(spp = &#39;072&#39;, cruz = cruzi, iterations = 100) This routine provides a list with two slots: er_results %&gt;% names [1] &quot;summary&quot; &quot;details&quot; The summary slot returns the p-value for each year, i.e., the chances that the observed number of sightings was due purely to random variation in the encounter rate. er_results$summary years observed p 1 2002 158 0.96 2 2010 210 0.00 3 2017 146 0.97 In this example, the encounter rates observed in 2002 and 2010 are very likely due to some process other than random variation in the encounter rate, such as range shifts, seasonal movement timing shifts, and/or changes in abundance. However, the observed encounter rate in 2017 could easily be explained by random variation in the ER. The details slot returns the simulation predictions for each year, which can be used to replicate the histograms that are printed when the function is run. A dataframe with a row for each year. Columns provide the number of observations of the species of interest during systematic effort, and the p-value of the test. The p-value represents the fraction of simulated encounter rates that exceed the observed encounter rate. "],["subgroups2.html", " 11 Subgroup-based analysis Inputs Outputs Behind the scenes", " 11 Subgroup-based analysis False killer whales (Pseudorca crassidens) are rare and occur in dispersed subgroups, which complicates conventional distance sampling approaches to line-transect analysis. To better estimate their abundance, in 2010 the Pacific Islands Fisheries Science Center (PIFSC) initiated a sub-group protocol referred to as the “PC Protocol”, a reference to the species’ scientific name. To conduct line-transect analysis with this sub-group-based protocol, a method was developed in 2014 - 2017 To handle this, a separate, subgroup-based analytical approach was developed in 2014 - 2017, then updated in 2020 (Bradford et al. 2020). An additional complication is that false killer whales in Hawaiian waters belong to two discrete populations – the Northwest Hawaiian Islands (NWHI) population and a pelagic population – whose ranges partially overlap, which means that population assignment cannot always be based simply on the geographic location of sightings. When geographic inference of population is not possible, biopsy-sampled genetics, photo-identification, and acoustics are used to assign each sighting to a population post-hoc. To accommodate these special circumstances with an appropriate balance of flexibility and efficiency, LTabundR includes a function named lta_subgroup(), whose use will look something like this: lta_subgroup(df_sits, truncation_distance, ss, density_segments, density_das, density_sightings, Rg0= NULL, cruz10 = NULL, g0_spp = NULL, g0_truncation = NULL, g0_constrain_shape = FALSE, g0_jackknife_fraction = 0.1, abundance_area = NULL, iterations = 5000, density_bootstraps = 10000, output_dir = NULL, toplot = TRUE, verbose = TRUE Note that there are several required inputs (without any defaults) as well as many optional inputs (with defaults provided). We will step through each of these inputs below, using a case study in which we estimate false killer whale abundance in the Hawaiian EEZ for 2017. Before proceeding, we recommend that you review the details of how subgroup data are processed within the LTabundR framework. Inputs df_sits This is a data.frame of sightings you want to use to fit the detection function model. For false killer whales in Bradford et al. (2020), this is a combination of systematic sightings prior to 2010 and Phase 1 sightings from 2010 onwards (using the PC protocol). No filtering will be applied to these sightings within this function, so make sure you provide the data pre-filtered. Bradford et al. (2020) used a single detection function for all populations of false killer whale. LTabundR has a built-in dataset for processed Central North Pacific surveys, 1986-2020, using 150-km segments. We will use that here: data(&quot;cnp_150km_1986_2020&quot;) cruz &lt;- cnp_150km_1986_2020 The code used to generate this dataset can be seen by pulling up the help documentation: ?noaa_10km_1986_2020. As mentioned above, for 1986 - 2010, all detections are assumed to be ‘Phase 1’ sightings, and therefore usable in detection function estimation. Here we draw those sightings from the above cruz object, filtering as needed (the species code for false killer whales is \"033\"), and to simplify we will select only a few key columns. sits1 &lt;- cruz$cohorts$pseudorca$sightings %&gt;% filter(OnEffort == TRUE, year &lt; 2011, Lat &gt;= 5, Lat &lt;= 40, Lon &gt;= -185, Lon &lt;= -120, species == &#39;033&#39;, ObsStd == TRUE, Bearing &lt;= 90, mixed == FALSE) %&gt;% select(DateTime, Lat, Lon, Cruise, PerpDistKm) sits1 %&gt;% nrow [1] 18 sits1 %&gt;% head DateTime Lat Lon Cruise PerpDistKm 1 1986-11-13 09:43:00 10.466667 -139.2833 990 1.17493742 2 1989-09-10 17:18:00 7.350000 -129.5333 1268 0.07751339 3 1990-09-09 10:55:00 7.750000 -128.3333 1370 0.00000000 4 1998-09-03 09:51:53 13.386167 -153.5658 1611 1.20590444 5 1999-08-18 13:54:24 7.031167 -145.1092 1614 1.21766870 6 2005-08-29 13:58:46 5.629667 -163.8967 1629 4.32970626 For 2011 onwards, we will use Phase 1 subgroup detections from the PC protocol, making sure that the column holding detection distances is named PerpDistKm: sits2 &lt;- cruz$cohorts$pseudorca$subgroups$subgroups %&gt;% filter(OnEffort == TRUE, lubridate::year(DateTime) &gt;= 2011, Lat &gt;= 5, Lat &lt;= 40, Lon &gt;= -185, Lon &lt;= -120, ObsStd == TRUE, Angle &lt;= 90, Species == &#39;033&#39;, Phase == 1) %&gt;% select(DateTime, Lat, Lon, Cruise, PerpDistKm = PerpDist) sits2 %&gt;% nrow [1] 65 sits2 %&gt;% head DateTime Lat Lon Cruise PerpDistKm 1 2011-10-26 13:16:06 7.240500 -164.9347 1108 1.4714466 2 2011-10-26 13:31:17 7.207333 -164.9572 1108 2.1211999 3 2011-10-26 13:48:26 7.169667 -164.9830 1108 0.9876483 4 2011-10-26 14:02:29 7.139000 -165.0043 1108 1.5731017 5 2013-05-13 07:09:54 24.302333 -168.3195 1303 1.6787670 6 2013-05-13 07:24:54 24.343500 -168.3315 1303 0.9470481 To create df_sits for detection function fitting, we combine these datasets together: df_sits &lt;- rbind(sits1, sits2) df_sits %&gt;% nrow [1] 83 truncation_distance The truncation distance, in km, will be applied during detection function model fitting. Typically the farthest 5 - 10% of sightings are truncated, but this needs to be balanced by sample size considerations. Get candidate distances: truncation_options &lt;- quantile(df_sits$PerpDistKm, c(0.90,0.91,0.92,0.93,0.94,.95)) truncation_options 90% 91% 92% 93% 94% 95% 4.376699 4.434760 4.515456 4.622182 4.782686 5.256604 Plot these options: hist(df_sits$PerpDistKm, main=&#39;Detection distances&#39;) abline(v=truncation_options, col=&#39;red&#39;, lty=3) Get sample size remaining for each candidate distance: data.frame(km = truncation_options, n = sapply(truncation_options, function(x){length(which(df_sits$PerpDistKm &lt;= x))})) km n 90% 4.376699 74 91% 4.434760 75 92% 4.515456 76 93% 4.622182 77 94% 4.782686 78 95% 5.256604 78 Based on these results, we will choose a truncation distance of 4.5 km. truncation_distance &lt;- 4.5 ss This is a numeric vector of subgroup school sizes. The function will find this vector’s geometric mean and bootstrapped CV. In Bradford et al. (2020), school size data come from all Phase 1 and Phase 2 estimates of subgroup sizes from 2010 onwards. In the processed cruz object, each of those estimates is the geometric mean of repeat estimates from separate observers. ss &lt;- cruz$cohort$all$subgroups$subgroups %&gt;% filter(lubridate::year(DateTime) &gt;= 2011, Lat &gt;= 5, Lat &lt;= 40, Lon &gt;= -185, Lon &lt;= -120, GSBest_geom_valid == TRUE, # only use valid &quot;Best&quot; estimates Species == &#39;033&#39;) %&gt;% pull(GSBest_geom) ss %&gt;% length [1] 180 ss [1] 5.00 7.00 1.00 1.00 1.26 2.00 2.63 2.62 1.00 5.75 1.00 1.73 [13] 17.94 1.00 4.00 4.76 1.73 1.00 3.78 4.47 2.45 2.00 2.00 1.00 [25] 1.00 5.00 1.00 4.00 1.00 1.00 1.00 1.00 1.00 3.63 2.00 3.00 [37] 2.00 8.12 8.00 4.00 2.00 1.00 1.00 4.00 4.00 2.00 1.00 2.00 [49] 6.00 2.00 2.00 1.00 2.00 2.38 1.00 4.00 7.55 2.00 4.12 7.09 [61] 2.83 2.00 1.00 1.00 1.00 1.00 1.00 2.00 2.00 1.00 1.00 1.00 [73] 1.00 1.00 1.00 1.00 2.00 1.00 2.00 2.00 1.73 1.00 4.00 7.48 [85] 1.41 2.00 1.00 1.00 1.00 1.00 2.00 1.00 2.00 2.83 3.00 2.00 [97] 3.00 1.41 1.00 1.00 2.83 2.00 4.00 1.00 3.00 2.88 3.46 4.24 [109] 2.00 1.00 1.00 2.00 1.00 1.00 1.00 1.00 1.00 2.00 1.00 1.00 [121] 1.00 1.86 2.47 1.00 2.00 1.41 2.52 1.26 2.00 6.00 4.16 2.00 [133] 1.00 3.00 5.00 5.67 4.00 1.00 2.00 2.00 4.00 4.47 2.00 2.00 [145] 2.00 4.47 2.00 1.00 2.00 1.00 1.00 2.00 2.00 1.41 4.47 1.00 [157] 1.00 3.17 2.88 2.00 2.00 6.00 5.00 1.00 2.00 2.00 4.00 2.00 [169] 2.00 1.00 1.00 2.83 1.00 2.88 2.45 2.00 2.00 1.00 1.00 1.00 Rg0 This is a data.frame with estimates of Relative g(0) and its CV at each Beaufort sea state. If this input is left NULL, then these estimates will be produced by the function using the subsequent g0_ inputs below. If this input is not supplied and any of the subsequent g0_ inputs are missing, then g(0) will be assumed to be 1.0 with CV of 0.0. When you do supply this Rg0 input, the data.frame has three required columns: bft (Beaufort sea state, numbers between 0 and 7), Rg0 (Rg(0) estimates for each Beaufort state), Rg0_CV (the CV of the Rg(0) estimate in each Beaufort state). Other columns are allowed but will be ignored. Here is an example of a valid Rg0 input based on the values reported for false killer whales in (Bradford et al. (2020)). Rg0 &lt;- data.frame(bft = 0:6, Rg0 = c(1, 1, .72, .51, .37, .26, .19), Rg0_CV = c(0, 0, 0.11, 0.22, 0.34, 0.46, 0.59)) Rg0 bft Rg0 Rg0_CV 1 0 1.00 0.00 2 1 1.00 0.00 3 2 0.72 0.11 4 3 0.51 0.22 5 4 0.37 0.34 6 5 0.26 0.46 7 6 0.19 0.59 Again: if you supply this input, then the following g0_ inputs will be ignored. cruz10 This is a processed cruz object with short segment lengths, ideally 10 km or less (hence the 10 in the input name). This cruz object will be used to estimate Rg(0), i.e., the relative trackline detection probability (see its chapter), using the following g0_ inputs. LTabundR comes with a built-in dataset we can use for this purpose: data(&quot;noaa_10km_1986_2020&quot;) cruz10 &lt;- noaa_10km_1986_2020 The code used to generate this dataset can be seen by pulling up the help documentation: ?noaa_10km_1986_2020. g0_spp This and the following g0_ inputs will be used to model Relative g(0) estimates and their CV in various Beaufort sea states. If the previous input, Rg0 is provided, then these g0_ inputs will be ignored and no Rg(0) modeling will take place. Furthermore, if any of these g0_ inputs are not provided, Rg(0) will be coerced to 1.0 with a CV of 0.0 for all sea states. This input, g0_spp, is a character vector of species code(s) to use to estimate Rg(0). In most cases this will be a single species, e.g., \"033\" for false killer whales. g0_spp &lt;- &#39;033&#39; g0_truncation The truncation distance to use when estimating Rg(0). In Bradford et al. (2020) this is 5.5 km. g0_truncation &lt;- 5.5 g0_constrain_shape Some Rg(0) curves will not decline monotonically due to sample size issues at low Bft (0-2) or high Bft (5-6) states. To coerce monotonic decline, set this input to TRUE, and the function will use a shape-constrained GAM (scam() from package scam) instead of a classic mgcv::gam(). g0_constrain_shape = FALSE g0_jackknife_fraction The proportion of data to leave out within each jackknife permutation. The default is 0.1 (i.e., 10% of the data, yielding 10 jackknife loops), after Barlow (2015). g0_jackknife_fraction = 0.1 density_segments The survey segments to be used in density/abundance estimation. For example, Bradford et al. (2020) used 150-km segments to estimate false killer whale density in the Hawaiian EEZ in 2017. For this we can use the 1986-2020 dataset we loaded above. Note that no filtering will be applied to these segments by the lta_subgroup() function, so w need to filter them ourselves first: we want only systematic segments for the Hawaiian EEZ in 2017 (specially, just cruises 1705 and 1706). cruzi &lt;- filter_cruz(cruz = cruz, analysis_only = TRUE, years = 2017, cruises = c(1705, 1706), regions = &#39;HI_EEZ&#39;, bft_range = 0:6, eff_types = &#39;S&#39;, on_off = TRUE) From this filtered cruz object, we will isolate the segments data: density_segments &lt;- cruzi$cohorts$all$segments Since we do not want to stratify our analysis by smaller geostrata, such as the Main Hawaiian Islands, we will go ahead and coerce all stratum assignments to the Hawaiian EEZ geostratum: density_segments$stratum &lt;- &#39;HI_EEZ&#39; density_das This is the complete survey data corresponding to the above segments. These data will be used to determine the proportion of survey effort occurring in each Beaufort sea state during estimation of Relative g(0). density_das &lt;- cruzi$cohorts$all$das density_sightings These are the encounters to use in density/abundance estimation. In Bradford et al. (20120), these were the Phase 1 detections of false killer whale subgroups within the population-region-year of interest, e.g., Northwest Hawaiian Island population sightings within the Hawaiian EEZ in 2017. No filtering is applied to these sightings within lta_subgroups(), so make sure only the sightings you wish to use are included and nothing more. In this example, since we do not have population information on hand, we will not filter detections to a specific population. Instead, we will estimate abundance of all false killer whales within the Hawaiian EEZ: density_sightings &lt;- cruz$cohorts$all$subgroups$subgroups %&gt;% filter(lubridate::year(DateTime) == 2017, EffType == &#39;S&#39;, OnEffort == TRUE, PerpDist &lt;= truncation_distance, Angle &lt;= 90, ObsStd == TRUE, Species == &#39;033&#39;, Phase == 1) density_sightings %&gt;% nrow [1] 23 density_sightings %&gt;% head Cruise Date DateTime Lat Lon OnEffort EffType Bft 1 1706 2017-07-23 2017-07-23 07:06:44 25.19550 -172.9102 TRUE S 3 2 1706 2017-07-23 2017-07-23 07:13:57 25.20117 -172.9320 TRUE S 3 3 1706 2017-07-23 2017-07-23 07:25:44 25.21000 -172.9673 TRUE S 3 4 1706 2017-07-23 2017-07-23 07:28:54 25.21233 -172.9770 TRUE S 3 5 1706 2017-07-27 2017-07-27 17:17:00 21.20933 -163.3107 TRUE S 4 6 1706 2017-07-27 2017-07-27 17:44:19 21.19050 -163.2342 TRUE S 4 SwellHght RainFog HorizSun VertSun Glare Vis ObsL Rec ObsR ObsInd SightNo 1 4 1 5 3 FALSE 7.5 125 280 307 &lt;NA&gt; 033 2 4 1 5 3 FALSE 7.5 125 280 307 &lt;NA&gt; 033 3 4 1 5 3 FALSE 7.5 125 280 307 &lt;NA&gt; 033 4 4 1 5 3 FALSE 7.5 125 280 307 &lt;NA&gt; 033 5 8 5 6 1 FALSE 7.0 387 125 280 099 039 6 8 5 5 2 FALSE 7.0 238 387 125 &lt;NA&gt; 039 Obs_Sight Species SubGrp Angle RadDist seg_id PerpDist GSBest GSH GSL 1 125 033 A 65 4.3706941 895 3.96119412 8 14 4.0 2 125 033 B 60 2.0371879 895 1.76425651 4 8 2.0 3 307 033 D 10 0.2037188 895 0.03537540 1 1 1.0 4 307 033 E 80 0.5926365 895 0.58363301 1 1 1.0 5 387 033 A 2 2.5001852 892 0.08725521 4 5 3.0 6 125 033 F 23 1.1667531 892 0.45588675 2 4 1.5 GSBest_geom GSH_geom GSL_geom GSBest_valid GSBest_geom_valid ObsStd 1 8 14 4.00 TRUE TRUE TRUE 2 4 8 2.00 TRUE TRUE TRUE 3 1 1 1.00 TRUE TRUE TRUE 4 1 1 1.00 TRUE TRUE TRUE 5 4 5 3.00 TRUE TRUE TRUE 6 2 4 1.41 TRUE TRUE TRUE stratum_HI_EEZ stratum_OtherCNP stratum_MHI stratum Phase 1 TRUE TRUE FALSE HI_EEZ 1 2 TRUE TRUE FALSE HI_EEZ 1 3 TRUE TRUE FALSE HI_EEZ 1 4 TRUE TRUE FALSE HI_EEZ 1 5 TRUE TRUE FALSE HI_EEZ 1 6 TRUE TRUE FALSE HI_EEZ 1 As above, let’s make sure the geostratum assignments for these sightings are simple: density_segments$stratum &lt;- &#39;HI_EEZ&#39; abundance_area This is the area, in square km, of the region of interest. The density estimate will be scaled by this area. We have two options for finding this area. The first is to draw the area from our cohort$strata slot: cruz$strata$area[cruz$strata$stratum == &#39;HI_EEZ&#39;] 2474596 [km^2] The second is to calculate it ourselves using the LTabundR function strata_area(). This second option will be useful if your study area is a complicated combination/substraction of multiple geostrata. data(strata_cnp) abundance_area &lt;- strata_area(strata_all = strata_cnp, strata_keep = &#39;HI_EEZ&#39;)$km2 abundance_area [1] 2474596 Remaining inputs iterations: Number of iterations to use in the various CV bootstrapping procedures occurring throughout this function, specifically: Effective Strip Half-Width CV estimation, school size CV estimation, weighted g(0) CV estimation, and encounter rate estimation. output_dir: The path in which results RData files should be stored. If left ““, the current working directory will be used. toplot: A Boolean, with default FALSE, indicating whether to plot various aspects of the analysis. verbose: A Boolean, with default TRUE, indicating whether to print status updates to the Console. density_bootstraps: Number of bootstrap iterations to use for the CV estimate of density and abundance specifically. This input allows this final step to use a different (typically larger) iteration size than the iterations input above. Here we call the lta_subgroup() function without re-modeling the Relative g(0) parameters, with only 200 iterations: results_subgroup &lt;- lta_subgroup(df_sits = df_sits, truncation_distance = truncation_distance, ss = ss, density_segments = density_segments, density_das = density_das, density_sightings = density_sightings, Rg0 = Rg0, abundance_area = abundance_area, iterations = 200, output_dir = &#39;subgroup/&#39;, toplot = TRUE, verbose = TRUE, density_bootstraps = 10000) save(results_subgroup, file=&#39;results_subgroup.RData&#39;) load(&#39;results_subgroup.RData&#39;) Outputs The function returns a list with many slots, including estimates of density and abundance – along with estimates of intermediate parameters – with a CV derived from a bootstrapping routine. To demonstrate this output, we will use results from a call with only 10 bootstrap iterations. results_subgroup %&gt;% names [1] &quot;D&quot; &quot;D_CV&quot; &quot;D_L95&quot; [4] &quot;D_U95&quot; &quot;N&quot; &quot;N_CV&quot; [7] &quot;N_L95&quot; &quot;N_U95&quot; &quot;ER&quot; [10] &quot;ss&quot; &quot;n&quot; &quot;L&quot; [13] &quot;n_segments&quot; &quot;g0&quot; &quot;g0_cv&quot; [16] &quot;g0_details&quot; &quot;df&quot; &quot;bootstraps&quot; [19] &quot;iterations&quot; &quot;density_bootstraps&quot; Most of these slots hold best-estimates of parameters or sample size details: results_subgroup[c(1:15, 19:20)] $D 1 0.001657904 $D_CV 1 0.6426318 $D_L95 [1] 0.0006649133 $D_U95 [1] 0.006223288 $N [1] 4103 $N_CV [1] 0.6425695 $N_L95 [1] 1645 $N_U95 [1] 15400 $ER [1] 0.001332624 $ss [1] 2.397167 $n [1] 23 $L [1] 17259.18 $n_segments [1] 136 $g0 [1] 0.359 $g0_cv [1] 0.324 $iterations [1] 200 $density_bootstraps [1] 10000 The g0_details slot includes the results from the g0_model() and g0_weighted() functions called internally by lta_subgroup(). See those functions’ documentation pages for details. results_subgroup$g0_details %&gt;% names [1] &quot;summary&quot; The df slot includes details of the detection function fit. See the documentation for df_plot() for details. results_subgroup$df %&gt;% names [1] &quot;best_models&quot; &quot;all_models&quot; &quot;best_objects&quot; &quot;sightings&quot; The bootstraps slot has the bootstrapped values for various parameters, in case they are useful for troubleshooting, subsequent analyses, and/or plotting: results_subgroup$bootstraps %&gt;% names [1] &quot;esw&quot; &quot;ss&quot; &quot;g0&quot; &quot;er&quot; &quot;D&quot; &quot;N&quot; Some examples: Behind the scenes This function performs the following operations: Fits a detection function to df_sits without covariates, using the LTabundR function df_fit(), in order to estimate the effective strip half-width (ESW). Conducts bootstrap re-sampling of the detection function fitting routine in order to estimate the CV of ESW. Estimates the geometric mean of subgroup school size based on the ss input. Creates a bootstrap-resampled distribution of subgroup school sizes, with which CV is estimated. Models the Relative g(0) in different survey conditions using the LTabundR function g0_model(). This function also estimates the CV of the Rg(0) estimate in each Beaufort sea state using jackknife resampling. Estimates the encounter rate (subgroup detections / trackline surveyed). Creates a bootstrap-resampled distribution of encounter rate estimates. Calculates a weighted g(0) estimate according to the proportion of effort occurring in each Beaufort sea state, then uses an automated parameter optimization routine (see details in LTabundR function g0_weighted()) to estimate the CV of the weighted g(0) estimate. Creates a bootstrap-resampled distribution of the weighted g(0) estimate. Estimates density using the best estimates of effective strip half-width, school size, g(0), and the encounter rate. Estimates abundance by scaling the density estimate by the provided abundance_area. Creates a bootstrap-resampled distribution of the density estimate by iteratively drawing values from the resampled distributions of the constituent parameters of the density equation. Creates a bootstrap-resampled distribtion of the abundance estimate by scaling the density distribution by abundance_area. Note that this approach could theoretically be used for other species that occur in subgroups. "],["diagnostic-plots.html", " 12 Diagnostic plots", " 12 Diagnostic plots To demonstrate how LTA results can explored quickly and reviewed for QA/QC using diagnostic plots, we will use a built-in LTabundR dataset, which has density/abundance estimates for the Hawaiian EEZ in 2010 and 2017 for striped dolphins, Fraser’s dolphins, and melon-headed whales, ran with only 100 iterations: data(lta_result) We created these LTA results using the following built-in processed dataset: data(cnp_150km_1986_2020) The function lta_diagnostics() can be used to review the object returned by the LTabundR function lta(), which is the primary function in this package for line-transect analysis. The typical way to use this function is simply: lta_diagnostics(lta_result) When you run this, the function will step through many diagnostic outputs (there are currently 8), some of which are tables and some of which are plots. Between each output, the function will wait for the user to press &lt;Enter&gt;. To turn that waiting feature off, you can add the input wait = FALSE. To see which outputs are currently available from this function, use the following code: lta_diagnostics(lta_result, options = c(), describe_options = TRUE) List of options for outputs to provide: =============== (use numbers in the input `options`) 1 - Point estimate (encounter rate, density, abundance, g(0), etc.) 2 - Summary of bootstrap iterations, including CV of density/abundance 3 - Plot of detection function 4 - Histogram of bootstrapped detection counts 5 - Histogram of bootstrapped g(0) values 6 - Histogram of bootstrapped abundance estimates 7 - Scatterplot of abundance ~ g(0) relationship in boostraps 8 - Time series of point estimate CV as bootstraps accumulate ====================================================== To call specific outputs and not others, use the options input. We demonstrate this be stepping through each output below. Option 1: The point estimate lta_diagnostics(lta_result, options = 1) title species Region Area year segments km Area_covered 1 Striped dolphin 013 (HI_EEZ) 2474596 2010 134 18102 64377 2 Striped dolphin 013 (HI_EEZ) 2474596 2017 137 17273 61571 3 Fraser&#39;s dolphin 026 (HI_EEZ) 2474596 2010 134 18102 63021 4 Fraser&#39;s dolphin 026 (HI_EEZ) 2474596 2017 137 17273 50516 5 Melon-headed whale 031 (HI_EEZ) 2474596 2010 134 18102 NA 6 Melon-headed whale 031 (HI_EEZ) 2474596 2017 137 17273 57626 ESW_mean n g0_est ER_clusters D_clusters N_clusters size_mean size_sd ER 1 3.56 19 0.33 0.0010 0.0005 1129.4 51.4 47.2 0.0539 2 3.56 17 0.32 0.0010 0.0004 1104.8 35.4 18.1 0.0348 3 3.48 3 0.33 0.0002 0.0001 179.4 236.2 129.0 0.0391 4 2.92 2 0.32 0.0001 0.0001 154.0 355.6 91.4 0.0412 5 NA 0 0.33 0.0000 0.0000 0.0 NA NA 0.0000 6 3.34 3 0.32 0.0002 0.0001 202.0 189.2 68.4 0.0329 D N g0_small g0_large g0_cv_small g0_cv_large 1 0.0223 55218 0.33 0.33 0.20 0.21 2 0.0148 36678 0.32 0.32 0.21 0.21 3 0.0175 43253 0.33 0.33 0.20 0.21 4 0.0218 53999 0.32 0.32 0.21 0.21 5 0.0000 0 0.33 0.33 0.20 0.21 6 0.0152 37615 0.32 0.32 0.21 0.21 Option 2: Summary of bootstrap iterations lta_diagnostics(lta_result, options = 2) title Region year species iterations ESW_mean g0_mean 1 Fraser&#39;s dolphin (HI_EEZ) 2010 026 100 3.720370 0.3265753 2 Fraser&#39;s dolphin (HI_EEZ) 2017 026 100 3.277146 0.3124772 3 Melon-headed whale (HI_EEZ) 2010 031 100 NaN 0.3209042 4 Melon-headed whale (HI_EEZ) 2017 031 100 3.151928 0.3201805 5 Striped dolphin (HI_EEZ) 2010 013 100 3.681439 0.3263969 6 Striped dolphin (HI_EEZ) 2017 013 100 3.643254 0.3225009 g0_cv km ER D size Nmean Nmedian Nsd 1 0.2045909 18175.25 0.04191221 0.01934134 239.58838 47862.01 40236.80 39619.30 2 0.1837095 17420.51 0.04363659 0.02255462 353.61848 55813.57 51795.71 36926.13 3 0.1874003 18175.25 0.00000000 0.00000000 NaN 0.00 0.00 0.00 4 0.2058941 17420.51 0.03278565 0.01879579 197.22189 46511.97 33793.75 39478.17 5 0.2041014 18175.25 0.05339105 0.02281395 52.21862 56455.29 51375.72 23096.04 6 0.1897529 17420.51 0.03389859 0.01490678 34.71461 36888.24 34647.24 12654.75 CV L95 U95 1 0.8277818 11965.50 191448.02 2 0.6615977 18604.52 167440.72 3 NaN NaN NaN 4 0.8487744 11627.99 186047.88 5 0.4091033 28227.65 112910.59 6 0.3430566 18444.12 73776.49 Option 3: Plot of detection function lta_diagnostics(lta_result, options = 3) Option 4: Histogram of bootstrapped detection counts lta_diagnostics(lta_result, options = 4) Option 5: Histogram of bootstrapped g(0) values lta_diagnostics(lta_result, options = 5) Option 6: Histogram of bootstrapped abundance estimates lta_diagnostics(lta_result, options = 6) Option 7: Relationship between bootstrap g(0) and abudance lta_diagnostics(lta_result, options = 7) Option 8: Running calculation of CV during bootstrap process lta_diagnostics(lta_result, options = 8) "],["tables.html", " 13 Tables Summary tables Appendix tables", " 13 Tables To demonstrate how LTA results can be summarized, tabularized, and plotted, we will use the same built-in dataset as the previous chapter: density/abundance estimates for the Hawaiian EEZ in 2010 and 2017 for striped dolphins, Fraser’s dolphins, and melon-headed whales, ran with only 100 iterations: data(lta_result) We created these LTA results using the following built-in processed dataset: data(cnp_150km_1986_2020) Summary tables To summarize lta() results using the standard table format provided in recent NOAA stock assessment reports, use the function lta_report(). tables &lt;- lta_report(lta_result, cruz = cnp_150km_1986_2020, verbose = TRUE) Providing the cruz object is not required, but if it is not provided, one of the five summary tables ($table1a below) will not be returned. tables %&gt;% names [1] &quot;table1a&quot; &quot;table1b&quot; &quot;table2&quot; &quot;table3&quot; &quot;table4&quot; &quot;tableA1&quot; &quot;tableA2&quot; Table 1 in reports: Sample sizes Table 1a If cruz was provided, $table1a 1a will include total sighting counts for all species in the years from lta_result, broken down by region. The Ntot column holds all sightings, regardless of effort status or Beaufort sea state. Nsys holds counts of systematic-only sightings (i.e., EffType = “S” and Bft &lt;= 6), which may still include sightings that are beyond the species-specific truncation distance and were therefore excluded from density/abundance estimation. These counts are provided separately from the $table1b slot below, since those counts are based on the lta_result object, and will not include sightings for species that did not have a specific LTA estimate specified when it was made. We also include this separately so as to give the user full flexibility in how they summarize sighting counts by region/population/stock. tables$table1a %&gt;% DT::datatable(options=list(initComplete = htmlwidgets::JS( &quot;function(settings, json) {$(this.api().table().container()).css({&#39;font-size&#39;: &#39;9pt&#39;});}&quot;) )) Table 1b This table holds sighting counts used in estimates of density/abundance. The rows match the rows for Tables 3 and 4. In this table, columns are still prepared for total sightings (Ntot) and systematic sightings (Nsys), but they are left blank, since it is not clear how sightings from multiple regions in $table1a would be concatenated for this table. The user can fill in those gaps accordingly. tables$table1b %&gt;% DT::datatable(options=list(initComplete = htmlwidgets::JS( &quot;function(settings, json) {$(this.api().table().container()).css({&#39;font-size&#39;: &#39;9pt&#39;});}&quot;) )) Table 2 in reports: Detection functions Table 3: Parameter estimates Table 4: Density/abundance Appendix tables Table A1: Study areas Table A2: Effort totals (parsed by Beaufort sea state) tables$tableA2 $`2010` # A tibble: 1 × 10 Species Stratum Effort B0 B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 all HI_EEZ 16166 0.00124 0.0127 0.0383 0.117 0.480 0.303 0.0474 $`2017` # A tibble: 1 × 10 Species Stratum Effort B0 B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 all HI_EEZ 15913 0.000828 0.0102 0.0370 0.117 0.312 0.350 0.173 "],["plots.html", " 14 Plots Abundance plots Detection function plots", " 14 Plots Abundance plots To plot the abundance estimate with error bars representing the 95% confidence interval, use the function lta_plot(): lta_plot(lta_result, nrows=1) Detection function plots To plot the best-fit detection model for a species pool, use the df_plot() function. df_plot(lta_result) This function provides various stylization options, including the option to show multiple best-fitting models atop a single histogram of detections: df_plot(lta_result, model_colors=RColorBrewer::brewer.pal(n = 4, name = &quot;Dark2&quot;), model_pch = 16, pt_show=2, pt_alpha=.3, bootstrap_show = FALSE, legend_show=TRUE, legend_x=2.8) "],["whiceas.html", " 15 WHICEAS Data processing Data exploration Rg0 Density &amp; abundance Results Validation", " 15 WHICEAS Here we demonstrate code that reproduces the Bradford et al. (2022) WHICEAS report within the new LTabundR framework. This study estimates cetacean abundance for Hawaiian WHICEAS study area for 2017 and 2020. Here we use survey data from 1986 to 2020 to estimate Relative g(0) and detection functions. Currently, coefficients of variation (CV) of density and abundance are estimated using only 100 bootstrap iterations (the publication uses 1,000) to reduce processing time. library(dplyr) library(LTabundR) library(ggplot2) Data processing Settings Survey-wide settings data(species_codes) data(ships) data(group_size_coefficients) edits &lt;- readRDS(&#39;cnp_1986_2020_edits.RData&#39;) survey &lt;- load_survey_settings( out_handling = &#39;remove&#39;, max_row_interval = Inf, segment_method = &quot;equallength&quot;, segment_target_km = 150, segment_max_interval = 24, segment_remainder_handling = c(&quot;segment&quot;), ship_list = ships, species_codes = species_codes, group_size_coefficients = group_size_coefficients, smear_angles = FALSE ) Geostrata data(strata_cnp) Cohort-specific settings Cohort 1: all species all_species &lt;- load_cohort_settings( id = &quot;all&quot;, # * species = NULL, strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;), # * probable_species = FALSE, sighting_method = 0, cue_range = 0:7, school_size_range = c(0, 10000), school_size_calibrate = TRUE, calibration_floor = 0, use_low_if_na = TRUE, io_sightings = 0, geometric_mean_group = TRUE, truncation_km = 7.5, # * beaufort_range = 0:6, abeam_sightings = TRUE, strata_overlap_handling = c(&quot;smallest&quot;), distance_types = c(&#39;S&#39;,&#39;F&#39;,&#39;N&#39;), distance_modes = c(&#39;P&#39;,&#39;C&#39;), distance_on_off = TRUE ) Cohort 2: bottlenose dolphins bottlenose &lt;- load_cohort_settings( id = &quot;bottlenose&quot;, species = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;, &#39;Bottlenose_BI&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_KaNi&#39;), truncation_km = 7.5) Cohort 3: pantropical spotted dolphins spotted &lt;- load_cohort_settings( id = &quot;spotted&quot;, species = &#39;002&#39;, strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;, &#39;Spotted_OU&#39;,&#39;Spotted_FI&#39;,&#39;Spotted_BI&#39;), truncation_km = 7.5) Process settings &lt;- load_settings(strata = strata_cnp, survey = survey, cohorts = list(all_species, bottlenose, spotted)) cruz &lt;- process_surveys(das_file = &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39;, settings = settings, edits = edits) save(cruz, file=&#39;whiceas/whiceas_cruz.RData&#39;) load(&#39;whiceas/whiceas_cruz.RData&#39;) Data exploration Processed data structure cruz_structure(cruz) Map of sightings For the entire cruz object: map_cruz(cruz, sightings_color = &#39;firebrick&#39;) For just the surveys of interest: # Filter cruz1720 &lt;- filter_cruz(cruz, years = c(2017, 2020), regions = &#39;WHICEAS&#39;, eff_types = &#39;S&#39;, bft_range = 0:6) # Map, including survey tracks map_cruz(cruz1720, sightings_color = &#39;firebrick&#39;, effort_show = TRUE, effort_resolution = 3, effort_weight = 4, effort_opacity = .4) Interactive dashboard Use this to determine truncation distances. Note them in your lta() code below: cruz_explorer(cruz) Rg0 We can use the built-in dataset, data(g0_results), which has Beaufort-specific Relative g(0) estimates for most species based on 1986-2020 surveys. data(&quot;g0_results&quot;) Rg0 &lt;- g0_results # Plot the results: g0_plot(Rg0, panes = 3) To explore the effects of LTabundR’s g(0) estimation routines on our abundance estimates, we will be running the analyses below with three different g(0) scenarios. In scenario 1, we will manually specify the weighted g(0) and its CV for each species in 2020; these results should be nearly exact replicates of those from Bradford et al. (2021), and we can be sure that any discrepancy between the two sets of results are not attributable the g(0) aspect of the analysis. Second, we will allow LTabundR to calculate the weighted g(0) and its CV using the same Relative g(0) values that were used in Bradford et al. (2021) (i.e., the results of Barlow (2015), which used surveys from 1986 to 2010 and is provided in LTabundR as a built-in dataset), such that any difference between scenarios 1 and 2 are likely due to the weighted g(0) subroutines. Finally, in scenario 3, we will use the Relative g(0) esitmates produced by LTabundR using a more extensive survey dataset (1986-2020). For each g(0) scenario, our Rg0 object and results_path will differ: # Specify scenario number here g0_scenario &lt;- 2 # Set Rg0 source and results path if(g0_scenario == 1){ # Using manually-specified g0 and its CV Rg0 &lt;- NULL results_path &lt;- &#39;whiceas/lta_manual/&#39; } if(g0_scenario == 2){ # Using Rg0 estimates from Barlow 2015 data(barlow_2015) Rg0 &lt;- barlow_2015 results_path &lt;- &#39;whiceas/lta_barlow/&#39; } if(g0_scenario == 3){ # New LTabundR estimates of Rg0 data(g0_results) Rg0 &lt;- g0_results results_path &lt;- &#39;whiceas/lta/&#39; } Density &amp; abundance First we can define common values that will be constant across all estimates we produce: bootstraps &lt;- 200 years &lt;- 1986:2020 fit_regions &lt;- NULL fit_not_regions &lt;- NULL toplot = TRUE verbose = TRUE df_settings &lt;- list(covariates = c(&#39;bft&#39;,&#39;lnsstot&#39;,&#39;cruise&#39;,&#39;year&#39;,&#39;ship&#39;,&#39;species&#39;), covariates_factor = c(FALSE, FALSE, TRUE, TRUE, TRUE, TRUE), covariates_levels = 2, covariates_n_per_level = 10, simplify_cue = TRUE, simplify_bino = TRUE, detection_function_base = &#39;hn&#39;, base_model = &#39;~1&#39;, delta_aic = 2) For most species, we want to estimate density/abundance for the same set of year-region scenarios. To reduce code redundancy, as well as the risk of typing errors (and our work!), we can use the LTabundR function lta_estimates() to economize how we prepare our estimates input. For most species, these are the year-region scenarios for which we want estimates: scenarios &lt;- list(list(years = 2017, regions = &#39;WHICEAS&#39;), list(years = 2020, regions = &#39;WHICEAS&#39;)) The lta_estimates() function will generate a custom function that makes it easy to create a set of estimates sub-lists for each species of interest: estimator &lt;- lta_estimates(scenarios) That result, estimator, is actually a function. Here’s an example of how this function will work, using the first species pool as an example: estimates &lt;- c(estimator(spp = &#39;013&#39;, title = &quot;Striped dolphin&quot;), estimator(spp = &#39;026&#39;, title = &quot;Fraser&#39;s dolphin&quot;, alt_g0_spp = &#39;013&#39;), estimator(spp = &#39;031&#39;, title = &quot;Melon-headed whale&quot;, alt_g0_spp = &#39;013&#39;)) estimates [[1]] [[1]]$years [1] 2017 [[1]]$regions [1] &quot;WHICEAS&quot; [[1]]$spp [1] &quot;013&quot; [[1]]$title [1] &quot;Striped dolphin&quot; [[2]] [[2]]$years [1] 2020 [[2]]$regions [1] &quot;WHICEAS&quot; [[2]]$spp [1] &quot;013&quot; [[2]]$title [1] &quot;Striped dolphin&quot; [[3]] [[3]]$years [1] 2017 [[3]]$regions [1] &quot;WHICEAS&quot; [[3]]$spp [1] &quot;026&quot; [[3]]$title [1] &quot;Fraser&#39;s dolphin&quot; [[3]]$alt_g0_spp [1] &quot;013&quot; [[4]] [[4]]$years [1] 2020 [[4]]$regions [1] &quot;WHICEAS&quot; [[4]]$spp [1] &quot;026&quot; [[4]]$title [1] &quot;Fraser&#39;s dolphin&quot; [[4]]$alt_g0_spp [1] &quot;013&quot; [[5]] [[5]]$years [1] 2017 [[5]]$regions [1] &quot;WHICEAS&quot; [[5]]$spp [1] &quot;031&quot; [[5]]$title [1] &quot;Melon-headed whale&quot; [[5]]$alt_g0_spp [1] &quot;013&quot; [[6]] [[6]]$years [1] 2020 [[6]]$regions [1] &quot;WHICEAS&quot; [[6]]$spp [1] &quot;031&quot; [[6]]$title [1] &quot;Melon-headed whale&quot; [[6]]$alt_g0_spp [1] &quot;013&quot; The output of estimator() is a list of sub-lists specifying a set of density/abundance estimates you want to produce based on the detection function for a single species pool. Here is the full code for producing those estimates for all species from Bradford et al. (2021): Multi-species pool 1 # Striped dolphin (013), Fraser&#39;s dolphin (026), Melon-headed whale (031) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;013&#39;, &#39;026&#39;, &#39;031&#39;), pool = &#39;Multi-species pool 1&#39;, cohort = &#39;all&#39;, truncation_distance = 5, other_species = &#39;remove&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;013&#39;, title = &quot;Striped dolphin&quot;), estimator(spp = &#39;026&#39;, title = &quot;Fraser&#39;s dolphin&quot;, alt_g0_spp = &#39;013&#39;), estimator(spp = &#39;031&#39;, title = &quot;Melon-headed whale&quot;, alt_g0_spp = &#39;013&#39;)) estimates # Manually specify g0 and its CV -- only for g0_scenario 1 # Two specifications per species, one for 2017 and one for 2020 if(g0_scenario==1){ # Striped estimates[[1]]$g0 &lt;- 0.35; estimates[[1]]$g0_cv &lt;- 0.19 estimates[[2]]$g0 &lt;- 0.31; estimates[[2]]$g0_cv &lt;- 0.22 # Fraser&#39;s estimates[[3]]$g0 &lt;- 0.35; estimates[[3]]$g0_cv &lt;- 0.10 estimates[[4]]$g0 &lt;- 0.31; estimates[[4]]$g0_cv &lt;- 0.22 # Melon-headed estimates[[5]]$g0 &lt;- 0.35; estimates[[5]]$g0_cv &lt;- 0.19 estimates[[6]]$g0 &lt;- 0.31; estimates[[6]]$g0_cv &lt;- 0.22 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 2 # Rough-toothed dolphin (15), Common bottlenose dolphin (18), Risso&#39;s (21), # Pygmy killer whale (32) # Notes # Bottlenose abundance is estimated in a separate cohort, but included here for DF fitting if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), pool = &#39;Multi-species pool 2&#39;, cohort = &#39;all&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;015&#39;, title = &quot;Rough-toothed dolphin&quot;), estimator(spp = &#39;021&#39;, title = &quot;Risso&#39;s dolphin&quot;), estimator(spp = &#39;032&#39;, title = &quot;Pygmy killer whale&quot;)) if(g0_scenario==1){ # Rough-toothed estimates[[1]]$g0 &lt;- 0.09; estimates[[1]]$g0_cv &lt;- 0.45 estimates[[2]]$g0 &lt;- 0.07; estimates[[2]]$g0_cv &lt;- 0.51 # Risso&#39;s estimates[[3]]$g0 &lt;- 0.57; estimates[[3]]$g0_cv &lt;- 0.18 estimates[[4]]$g0 &lt;- 0.52; estimates[[4]]$g0_cv &lt;- 021 # Pygmy killer estimates[[5]]$g0 &lt;- 0.14; estimates[[5]]$g0_cv &lt;- 0.25 estimates[[6]]$g0 &lt;- 0.11; estimates[[6]]$g0_cv &lt;- 0.28 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 3 # Short-finned pilot whale (036), Longman&#39;s beaked whale (065) # No Rg(0) available for Longman&#39;s -- will use SF pilot whale instead to estimate its weighted g0 if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;036&#39;, &#39;065&#39;), pool = &#39;Multi-species pool 3&#39;, cohort = &#39;all&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;036&#39;, title = &quot;Short-finned pilot whale&quot;), estimator(spp = &#39;065&#39;, title = &quot;Longman&#39;s beaked whale&quot;, alt_g0_spp = &#39;036&#39;)) if(g0_scenario==1){ # Short-finned estimates[[1]]$g0 &lt;- 0.58; estimates[[1]]$g0_cv &lt;- 0.15 estimates[[2]]$g0 &lt;- 0.52; estimates[[2]]$g0_cv &lt;- 0.19 # Longman&#39;s estimates[[3]]$g0 &lt;- 0.58; estimates[[3]]$g0_cv &lt;- 0.15 estimates[[4]]$g0 &lt;- 0.52; estimates[[4]]$g0_cv &lt;- 0.19 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 4 # Killer whale (37), sperm whale (46) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;037&#39;, &#39;046&#39;), pool = &#39;Multi-species pool 4&#39;, cohort = &#39;all&#39;, truncation_distance = 5.5, other_species = &#39;remove&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;037&#39;, title = &quot;Killer whale&quot;), estimator(spp = &#39;046&#39;, title = &quot;Sperm whale&quot;)) if(g0_scenario==1){ # Killer (no sightings in ALB et al 2021) estimates[[1]]$g0 &lt;- 1; estimates[[1]]$g0_cv &lt;- 0 estimates[[2]]$g0 &lt;- 1; estimates[[2]]$g0_cv &lt;- 0 # Sperm estimates[[3]]$g0 &lt;- 0.63; estimates[[3]]$g0_cv &lt;- 0.34 estimates[[4]]$g0 &lt;- 0.61; estimates[[4]]$g0_cv &lt;- 0.37 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 5 # Pygmy sperm whale (47), dwarf sperm whale (48), UNID Kogia (80), # Blainville&#39;s beaked whale (59), Cuvier&#39;s beaked whale (61), # UNID Mesoplodon (51), UNID beaked whale (49), Minke whale (71) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;047&#39;, &#39;048&#39;, &#39;080&#39;, &#39;059&#39;, &#39;061&#39;, &#39;051&#39;, &#39;049&#39;, &#39;071&#39;), pool = &#39;Multi-species pool 5&#39;, cohort = &#39;all&#39;, truncation_distance = 4.5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;047&#39;, title = &quot;Pygmy sperm whale&quot;), estimator(spp = &#39;048&#39;, title = &quot;Dwarf sperm whale&quot;), estimator(spp = &#39;080&#39;, title = &quot;Unidentified Kogia&quot;), estimator(spp = &#39;059&#39;, title = &quot;Blainville&#39;s beaked whale&quot;), estimator(spp = &#39;061&#39;, title = &quot;Cuvier&#39;s beaked whale&quot;), estimator(spp = &#39;051&#39;, title = &quot;Unidentified Mesoplodon&quot;), estimator(spp = &#39;049&#39;, title = &quot;Unidentified beaked whale&quot;, alt_g0_spp = c(&#39;061&#39;,&#39;051&#39;), combine_g0 = TRUE), estimator(spp = &#39;071&#39;, title = &quot;Minke whale&quot;)) # Note Barlow2015 provides absolute estimates for Cuviers and UNID Mesop if(g0_scenario==1){ # Pygmy sperm (no sightings in ALB, using other kogia values) estimates[[1]]$g0 &lt;- 0.005; estimates[[1]]$g0_cv &lt;- 0.15 estimates[[2]]$g0 &lt;- 0.004; estimates[[2]]$g0_cv &lt;- 0.15 # Dwarf sperm (no ALB sightings in 2017, using 2020 g0) estimates[[3]]$g0 &lt;- 0.005; estimates[[3]]$g0_cv &lt;- 0.15 estimates[[4]]$g0 &lt;- 0.004; estimates[[4]]$g0_cv &lt;- 0.15 # UNID Kogia (no ALB sightings 2020, using 2017 g0) estimates[[5]]$g0 &lt;- 0.005; estimates[[5]]$g0_cv &lt;- 0.15 estimates[[6]]$g0 &lt;- 0.004; estimates[[6]]$g0_cv &lt;- 0.15 # Blainville&#39;s (no ALB sightings 2017, using 2020 g0) estimates[[7]]$g0 &lt;- 0.11; estimates[[7]]$g0_cv &lt;- 0.30 estimates[[8]]$g0 &lt;- 0.11; estimates[[6]]$g0_cv &lt;- 0.30 # Cuvier&#39;s (no sightings in ALB -- using unid beaked g0) estimates[[9]]$g0 &lt;- 0.13; estimates[[9]]$g0_cv &lt;- 0.20 estimates[[10]]$g0 &lt;- 0.11; estimates[[10]]$g0_cv &lt;- 0.21 # UNID Mesop (no ALB sightings 2017, using 2020 g0) estimates[[11]]$g0 &lt;- 0.11; estimates[[11]]$g0_cv &lt;- 0.30 estimates[[12]]$g0 &lt;- 0.11; estimates[[12]]$g0_cv &lt;- 0.30 # UNID beaked estimates[[13]]$g0 &lt;- 0.13; estimates[[13]]$g0_cv &lt;- 0.20 estimates[[14]]$g0 &lt;- 0.11; estimates[[14]]$g0_cv &lt;- 0.21 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 6 # Bryde&#39;s whale (72), Sei whale (73), Fin whale (74), Blue whale (75), # Sei/Bryde&#39;s (99), Fin/Sei/Bryde&#39;s (72, 73, 74, 99) # Bryde&#39;s, Sei&#39;s, and Sei/Bryde&#39;s all use same Rg0 (title = &quot;Sei/Bryde&#39;s&quot;) # Sei/Bryde&#39;s/Fin use an average of Fin and Sei/Bryde&#39;s. if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;072&#39;, &#39;073&#39;, &#39;074&#39;,&#39;075&#39;,&#39;099&#39;), pool = &#39;Multi-species pool 6&#39;, cohort = &#39;all&#39;, truncation_distance = 5.0, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;072&#39;, title = &quot;Bryde&#39;s whale&quot;), estimator(spp = &#39;073&#39;, title = &quot;Sei whale&quot;), estimator(spp = &#39;074&#39;, title = &quot;Fin whale&quot;), estimator(spp = &#39;075&#39;, title = &quot;Blue whale&quot;), estimator(spp = &#39;099&#39;, title = &quot;Sei/Bryde&#39;s whale&quot;), estimator(spp = c(&#39;072&#39;, &#39;073&#39;, &#39;099&#39;, &#39;074&#39;), title = &quot;Sei/Bryde&#39;s/Fin whale&quot;, combine_g0 = TRUE)) if(g0_scenario==1){ # Brydes (no ALB sightings -- using sei values) estimates[[1]]$g0 &lt;- 0.38; estimates[[1]]$g0_cv &lt;- 0.21 estimates[[2]]$g0 &lt;- 0.38; estimates[[2]]$g0_cv &lt;- 0.21 # Sei (no ALB sightings in 2017, using 2020 g0) estimates[[3]]$g0 &lt;- 0.38; estimates[[3]]$g0_cv &lt;- 0.21 estimates[[4]]$g0 &lt;- 0.38; estimates[[4]]$g0_cv &lt;- 0.21 # Fin (no ALB sightings in 2017, using 2020 g0) estimates[[5]]$g0 &lt;- 0.30; estimates[[5]]$g0_cv &lt;- 0.29 estimates[[6]]$g0 &lt;- 0.30; estimates[[6]]$g0_cv &lt;- 0.29 # Blue (no ALB sightings, using fin values) estimates[[7]]$g0 &lt;- 0.30; estimates[[7]]$g0_cv &lt;- 0.29 estimates[[8]]$g0 &lt;- 0.30; estimates[[8]]$g0_cv &lt;- 0.29 # Sei / Bryde&#39;s (no ALB sightings in 2017, using 2020 g0) estimates[[9]]$g0 &lt;- 0.38; estimates[[9]]$g0_cv &lt;- 0.21 estimates[[10]]$g0 &lt;- 0.38; estimates[[10]]$g0_cv &lt;- 0.21 # Sei / Bryde&#39;s / Fin (no ALB sightings in 2017, using 2020 g0) estimates[[11]]$g0 &lt;- 0.34; estimates[[11]]$g0_cv &lt;- 0.17 estimates[[12]]$g0 &lt;- 0.34; estimates[[12]]$g0_cv &lt;- 0.17 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Humpback whale if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;076&#39;), pool = &#39;Humpback whale&#39;, cohort = &#39;all&#39;, truncation_distance = 5.5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;-c(estimator(spp = &#39;076&#39;, title = &quot;Humpback whale&quot;)) if(g0_scenario==1){ # No ALB sightings in 2017, using 2020 g0 estimates[[1]]$g0 &lt;- 0.68; estimates[[1]]$g0_cv &lt;- 0.36 estimates[[2]]$g0 &lt;- 0.68; estimates[[2]]$g0_cv &lt;- 0.36 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Unidentified rorquals # UNID rorquals (70) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;070&#39;), pool = &#39;Unidentified rorqual&#39;, cohort = &#39;all&#39;, truncation_distance = 5.5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;070&#39;, title = &quot;Unidentified rorqual&quot;, alt_g0_spp = c(&#39;071&#39;,&#39;099&#39;,&#39;074&#39;,&#39;075&#39;), combine_g0 = TRUE)) if(g0_scenario==1){ estimates[[1]]$g0 &lt;- 0.35; estimates[[1]]$g0_cv &lt;- 0.18 estimates[[2]]$g0 &lt;- 0.32; estimates[[2]]$g0_cv &lt;- 0.20 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) results$bootstrap$summary %&gt;% as.data.frame # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Unidentified dolphins # UNID dolphin (177, 277, 377, 77) if(TRUE){ # toggle spp &lt;- c(&#39;177&#39;,&#39;277&#39;,&#39;377&#39;,&#39;077&#39;) pool_title &lt;- &#39;Unidentified dolphin&#39; # Detection function specifications fit_filters &lt;- list(spp = c(&#39;177&#39;,&#39;277&#39;,&#39;377&#39;,&#39;077&#39;), pool = pool_title, cohort = &#39;all&#39;, truncation_distance = 5.5, other_species = &#39;coerce&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- estimator(spp = spp, title = pool_title, alt_g0_spp = c(&#39;002&#39;,&#39;013&#39;,&#39;018&#39;,&#39;015&#39;, &#39;036&#39;, &#39;021&#39;), combine_g0 = TRUE) if(g0_scenario==1){ estimates[[1]]$g0 &lt;- 0.33; estimates[[1]]$g0_cv &lt;- 0.08 estimates[[2]]$g0 &lt;- 0.29; estimates[[2]]$g0_cv &lt;- 0.10 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Unidentified cetaceans # UNID cetacean (78, 79, 98, 96) if(TRUE){ # toggle spp &lt;- c(&#39;078&#39;,&#39;079&#39;,&#39;098&#39;,&#39;096&#39;) pool_title &lt;- &#39;Unidentified cetacean&#39; # Detection function specifications fit_filters &lt;- list(spp = spp, pool = pool_title, cohort = &#39;all&#39;, truncation_distance = 5.5, other_species = &#39;coerce&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- estimator(spp = spp, title = pool_title, g0=1.0, g0_cv = 0.0) if(g0_scenario==1){ estimates[[1]]$g0 &lt;- 1.0; estimates[[1]]$g0_cv &lt;- 0.0 estimates[[2]]$g0 &lt;- 1.0; estimates[[2]]$g0_cv &lt;- 0.0 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Bottlenose dolphin # Bottlenose dolphin (018) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), pool = &#39;Bottlenose dolphin&#39;, cohort = &#39;bottlenose&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan scenarios &lt;- list( list(years = 2017, regions = &#39;WHICEAS&#39;, regions_remove = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;), region_title = &#39;(WHICEAS)&#39;), list(years = 2020, regions = &#39;WHICEAS&#39;, regions_remove = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;), region_title = &#39;(WHICEAS)&#39;)) estimator &lt;- lta_estimates(scenarios) estimates &lt;- estimator(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;) if(g0_scenario==1){ # No ALB sightings in 2017, using 2020 g0 estimates[[1]]$g0 &lt;- 0.24; estimates[[1]]$g0_cv &lt;- 0.38 estimates[[2]]$g0 &lt;- 0.24; estimates[[2]]$g0_cv &lt;- 0.38 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Pantropical spotted dolphin # Pantropical spotted dolphin (002) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;002&#39;), pool = &#39;Pantropical spotted dolphin&#39;, cohort = &#39;spotted&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan scenarios &lt;- list( list(years = 2017, regions = &#39;WHICEAS&#39;, regions_remove = c(&#39;Spotted_OU&#39;, &#39;Spotted_FI&#39;, &#39;Spotted_BI&#39;), region_title = &#39;(WHICEAS)&#39;), list(years = 2020, regions = &#39;WHICEAS&#39;, regions_remove = c(&#39;Spotted_OU&#39;, &#39;Spotted_FI&#39;, &#39;Spotted_BI&#39;), region_title = &#39;(WHICEAS)&#39;)) estimator &lt;- lta_estimates(scenarios) estimates &lt;- estimator(spp = &#39;002&#39;, title = &#39;Pantropical spotted dolphin&#39;) if(g0_scenario==1){ estimates[[1]]$g0 &lt;- 0.28; estimates[[1]]$g0_cv &lt;- 0.11 estimates[[2]]$g0 &lt;- 0.25; estimates[[2]]$g0_cv &lt;- 0.13 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Results To review results, we will use g(0) scenario 3 (new auto-generated estimates of Relative g(0) from LTabundR). # Load results ltas &lt;- lta_enlist(&#39;whiceas/lta/&#39;) Tables Generate report: reporti &lt;- lta_report(ltas, cruz) Error: ! Names repair functions can&#39;t return `NA` values. Table 1. Sample sizes. The lta_report() function above attempts to generate sample size tables based on the cruz object and ltas results (see $table1a and $table1b outputs of lta_report()), but this is difficult to generalize into an automatic function, especially when cohort-specific geostrata are involved. To determine sample sizes with more control, we can write a quick helper function: sample_size &lt;- function(cruz, spp, cohort, years, td, in_region=NULL, region_remove = NULL){ suppressMessages({ sits &lt;- cruz$cohorts[[cohort]]$sightings %&gt;% filter(species %in% spp) %&gt;% filter(year %in% years) %&gt;% filter(stratum %in% in_region) %&gt;% filter(! stratum %in% region_remove) %&gt;% mutate(species = paste(spp, collapse=&#39;/&#39;)) %&gt;% group_by(species, year) %&gt;% summarize(ntot = n(), nsys = length(which(use == TRUE &amp; Bft &lt;= 6 &amp; EffType == &#39;S&#39;)), nest = length(which(use == TRUE &amp; included == TRUE &amp; Bft &lt;= 6 &amp; EffType == &#39;S&#39; &amp; PerpDistKm &lt;= td))) %&gt;% tidyr::pivot_longer(cols = ntot:nest) %&gt;% tidyr::pivot_wider(id_cols = species, names_from = year:name, values_from = value) }) return(sits) } Now we can use this function to generate sample size totals for each species/stock of interest: # Save years to re-use in the lines below years &lt;- c(2017, 2020) # Spotted dolphin sample_size(cruz, spp = &#39;002&#39;, cohort = 3, years, td = 5, in_region = &#39;WHICEAS&#39;, region_remove = c(&#39;Spotted_OU&#39;,&#39;Spotted_FI&#39;,&#39;Spotted_BI&#39;)) # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 002 10 7 6 6 4 # ℹ 1 more variable: `2020_nest` &lt;int&gt; # More efficient code for remaining species: sample_size(cruz, &#39;013&#39;, 1, years, 5, &#39;WHICEAS&#39;) # striped dolphin # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 013 4 3 3 8 3 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;015&#39;, 1, years, 5, &#39;WHICEAS&#39;) # rough-toothed dolphin # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 015 18 4 4 7 4 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;018&#39;, 2, years, 5, &#39;WHICEAS&#39;, # bottlenose dolphin c(&#39;Bottlenose_KaNi&#39;,&#39;Bottlenose_OUFI&#39;,&#39;Bottlenose_BI&#39;)) # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 018 1 0 0 3 3 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;021&#39;, 1, years, 5, &#39;WHICEAS&#39;) # risso&#39;s dolphin # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 021 5 2 2 5 4 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;026&#39;, 1, years, 5, &#39;WHICEAS&#39;) # fraser&#39;s dolphin # A tibble: 1 × 4 # Groups: species [1] species `2020_ntot` `2020_nsys` `2020_nest` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 026 3 2 2 sample_size(cruz, &#39;031&#39;, 1, years, 5, &#39;WHICEAS&#39;) # melon-headed whale # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 031 4 2 2 6 3 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;032&#39;, 1, years, 5, &#39;WHICEAS&#39;) # pygmy killer whale # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 032 2 1 1 3 3 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;036&#39;, 1, years, 5, &#39;WHICEAS&#39;) # short-finned pilot whale # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 036 27 1 1 6 5 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;037&#39;, 1, years, 5.5, &#39;WHICEAS&#39;) # killer whale # A tibble: 0 × 1 # Groups: species [0] # ℹ 1 variable: species &lt;chr&gt; sample_size(cruz, &#39;046&#39;, 1, years, 5.5, &#39;WHICEAS&#39;) # sperm whale # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 046 5 1 1 14 10 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;047&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # pygmy sperm whale # A tibble: 0 × 1 # Groups: species [0] # ℹ 1 variable: species &lt;chr&gt; sample_size(cruz, &#39;048&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # dwarf sperm whale # A tibble: 1 × 4 # Groups: species [1] species `2020_ntot` `2020_nsys` `2020_nest` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 048 1 1 1 sample_size(cruz, &#39;080&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # UNID Kogia # A tibble: 1 × 4 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 080 2 1 1 sample_size(cruz, &#39;049&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # UNID beaked whale # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 049 4 1 1 4 4 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;051&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # UNID Mesoplodon # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 051 1 0 0 3 2 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;059&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # Blainville&#39;s beaked whale # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 059 3 0 0 2 0 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;061&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # Cuvier&#39;s beaked whale # A tibble: 1 × 4 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 061 3 0 0 sample_size(cruz, &#39;065&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # Longman&#39;s beaked whale # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 065 3 2 2 1 1 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;070&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # UNID rorqual # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 070 2 1 1 15 4 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;071&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # Minke whale # A tibble: 1 × 4 # Groups: species [1] species `2020_ntot` `2020_nsys` `2020_nest` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 071 1 1 0 sample_size(cruz, c(&#39;199&#39;), 1, years, 5, &#39;WHICEAS&#39;) # Sei/Bryde&#39;s/Fin # A tibble: 1 × 4 # Groups: species [1] species `2020_ntot` `2020_nsys` `2020_nest` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 199 1 1 1 sample_size(cruz, &#39;073&#39;, 1, years, 5, &#39;WHICEAS&#39;) # Sei whale # A tibble: 1 × 4 # Groups: species [1] species `2020_ntot` `2020_nsys` `2020_nest` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 073 5 3 3 sample_size(cruz, &#39;074&#39;, 1, years, 5, &#39;WHICEAS&#39;) # Fin whale # A tibble: 1 × 4 # Groups: species [1] species `2020_ntot` `2020_nsys` `2020_nest` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 074 1 1 1 sample_size(cruz, &#39;076&#39;, 1, years, 5.5, &#39;WHICEAS&#39;) # Humpback whale # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 076 3 0 0 164 85 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;099&#39;, 1, years, 5, &#39;WHICEAS&#39;) # Sei/Brydes # A tibble: 1 × 4 # Groups: species [1] species `2020_ntot` `2020_nsys` `2020_nest` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 099 5 4 4 sample_size(cruz, c(&#39;177&#39;,&#39;277&#39;,&#39;377&#39;,&#39;077&#39;), 1, years, 5.5, &#39;WHICEAS&#39;) # UNID dolphin # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 177/277/377/077 18 4 3 23 11 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, c(&#39;078&#39;,&#39;079&#39;,&#39;098&#39;,&#39;096&#39;), 1, years, 5.5, &#39;WHICEAS&#39;) # UNID cetacean # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 078/079/098/096 6 1 1 23 13 # ℹ 1 more variable: `2020_nest` &lt;int&gt; To expedite building up this sample size table, consider copying and pasting the table produced by lta_report()$table1b, then filling in the blanks with values from above: Table 1b. Sample sizes of sightings used in density esitmation. Table 2. Detection functions for cetacean species and taxonomic categories. reporti$table2 Error in eval(expr, envir, enclos): object &#39;reporti&#39; not found Table 3. Estimates of line-transect parameters for cetacean species and taxonomic categories. reporti$table3 Error in eval(expr, envir, enclos): object &#39;reporti&#39; not found Table 4. Estimates of density (individuals per 1,000 km2) and abundance for cetacean species and taxonomic categories sighted while on systematic survey effort. reporti$table4 Error in eval(expr, envir, enclos): object &#39;reporti&#39; not found Table A1. Study areas. reporti$tableA1 Error in eval(expr, envir, enclos): object &#39;reporti&#39; not found Table A2. Effort totals and by Beaufort sea-state, in each survey year. reporti$tableA2 Error in eval(expr, envir, enclos): object &#39;reporti&#39; not found Plots lta_plot(species = NULL, lta_result = ltas, years = c(2017, 2020)) Validation To validate our results and the routines within LTabundR, we will compare the LTabundR WHICEAS results to those from A.L. Bradford et al. (2022). To do so, we first bring in their results, which we’ve staged in a GoogleSheet provided in the code chunk below: library(gsheet) url &lt;- &#39;https://docs.google.com/spreadsheets/d/1S94I9I0R589Z8PFP41lYOF8hM6yetap7RFlPdBeet1M/edit?usp=sharing&#39; alb &lt;- gsheet2tbl(url) Next we format these data to facilitate joining and analyzing alongside our new WHICEAS results within a tidyverse framework: library(dplyr) library(tidyr) alb &lt;- alb %&gt;% mutate(title=gsub(&quot;&#39;&quot;,&quot;&quot;,species)) %&gt;% tidyr::pivot_longer(3:ncol(alb), names_to=&#39;column&#39;, values_to=&#39;value&#39;, values_transform = as.character) %&gt;% mutate(source=&#39;ALB et al. (2022)&#39;, title = species, code = stringr::str_pad(code,width=3, side=&#39;left&#39;, pad=&#39;0&#39;), year = ifelse(grepl(&#39;17&#39;, column), 2017, 2020) %&gt;% as.numeric) %&gt;% select(-species) %&gt;% rename(species = code) %&gt;% mutate(column = gsub(&#39;17&#39;,&#39;&#39;,column)) %&gt;% mutate(column = gsub(&#39;20&#39;,&#39;&#39;,column)) %&gt;% group_by(title, species, year) %&gt;% summarize(g0 = value[column == &#39;g0&#39;][1] %&gt;% as.numeric, g0_cv = value[column == &#39;gcv&#39;][1] %&gt;% as.numeric, ESW = value[column == &#39;esw&#39;] %&gt;% as.numeric, ss = value[column == &#39;ss&#39;] %&gt;% as.numeric, D = value[column == &#39;d&#39;] %&gt;% as.numeric, N = gsub(&#39;,&#39;,&#39;&#39;, value[column == &#39;n&#39;]) %&gt;% as.numeric, CV = value[column == &#39;cv&#39;] %&gt;% as.numeric, L95 = gsub(&#39;,&#39;,&#39;&#39;, value[column == &#39;lci&#39;]) %&gt;% as.numeric, U95 = gsub(&#39;,&#39;,&#39;&#39;, value[column == &#39;uci&#39;]) %&gt;% as.numeric) %&gt;% filter(!is.na(D)) %&gt;% mutate(N = round(N)) # Modify names names(alb)[4:ncol(alb)] &lt;- paste0(&#39;alb_&#39;,names(alb)[4:ncol(alb)] ) Review these results: alb LTabundR results We will compare the ALB et al. (2022) results to all three versions of the WHICEAS analysis we have produced: (1) manually specified g(0) parameters, (2) Rg(0) estimates from Barlow et al. (2015), and (3) new Rg(0) estimates produced by LTabundR automatically. We will bring those results back in and join each to the results from ALB et al. (2022), using a custom helper function, whiceas_join(): whiceas_join &lt;- function(alb, ltas){ # Combine core info from each LTA list into a dataframe ltabundr &lt;- data.frame() for(i in 1:length(ltas)){ lti &lt;- ltas[[i]] ltabundi &lt;- left_join(lti$estimate, lti$bootstrap$summary %&gt;% select(title, Region, year, species, iterations, g0_cv, CV, L95, U95), by=c(&#39;title&#39;, &#39;Region&#39;, &#39;year&#39;, &#39;species&#39;)) ltabundr &lt;- rbind(ltabundr, ltabundi) } # Format ltabundr data ltabundr &lt;- ltabundr %&gt;% mutate(year = as.numeric(year), D = D*1000, N = round(N)) %&gt;% select(title, species, year, g0=g0_est, g0_cv, ESW = ESW_mean, ss = size_mean, D, N, CV, L95, U95) # Join ALB to LTabundR mr &lt;- left_join(ltabundr, alb, by=c(&#39;title&#39;, &#39;year&#39;)) return(mr) } Read in, format, and join the results: # Manually-specified g0 ltas &lt;- lta_enlist(&#39;whiceas/lta_manual/&#39;) mr1 &lt;- whiceas_join(alb, ltas) %&gt;% mutate(scenario = &#39;ALB et al. (2022)&#39;) # Barlow (2015) Rg0 ltas &lt;- lta_enlist(&#39;whiceas/lta_barlow/&#39;) mr2 &lt;- whiceas_join(alb, ltas) %&gt;% mutate(scenario = &#39;Barlow (2015)&#39;) # New auto-generated Rg(0) ltas &lt;- lta_enlist(&#39;whiceas/lta/&#39;) mr3 &lt;- whiceas_join(alb, ltas) %&gt;% mutate(scenario = &#39;LTabundR&#39;) # Join them together mr &lt;- rbind(mr1, mr2, mr3) %&gt;% mutate(scenario = factor(scenario, levels = c(&#39;ALB et al. (2022)&#39;, &#39;Barlow (2015)&#39;, &#39;LTabundR&#39;))) Now we can compare LTabundR estimates to those from ALB et al. (2021): library(ggplot2) library(plotly) # Density ggplot(mr, aes(x=alb_D, y=D, color=title, shape=factor(year))) + geom_point() + scale_x_continuous(trans=&#39;log&#39;, limits=c(0.1, 50), breaks = c(0.01, 0.1, 0.25, 0.5, 1.0, 2.5, 5, 10, 25, 50), labels = function(x)round(x, 2)) + scale_y_continuous(trans=&#39;log&#39;, limits=c(0.1, 50), breaks = c(0.01, 0.1, 0.25, 0.5, 1.0, 2.5, 5, 10, 25, 50), labels = function(x)round(x, 2)) + geom_abline(slope = 1, intercept = 0, lty=3, alpha=.6) + ylab(&#39;LTabundR&#39;) + xlab(&#39;ALB et al. (2022)&#39;) + facet_wrap(~scenario, nrow=3) + labs(title=&#39;Density (cetaceans per 1,000 km2)&#39;, shape=&#39;Year&#39;, color = &#39;Species&#39;) + theme_light()   # Abundance ggplot(mr, aes(x=alb_N, y=N, color=title, shape=factor(year))) + geom_point() + scale_x_continuous(trans=&#39;log&#39;, limits=c(10, 30000), breaks = c(10, 25, 100, 250, 1000, 2500, 10000, 30000), labels = function(x)round(x)) + scale_y_continuous(trans=&#39;log&#39;, limits=c(10, 30000), breaks = c(10, 25, 100, 250, 1000, 2500, 10000, 30000), labels = function(x)round(x)) + geom_abline(slope = 1, intercept = 0, lty=3, alpha=.6) + ylab(&#39;LTabundR&#39;) + xlab(&#39;ALB et al. (2022)&#39;) + facet_wrap(~scenario, nrow=3) + labs(title=&#39;WHICEAS abundance&#39;, shape=&#39;Year&#39;, color = &#39;Species&#39;) + theme_light()   # g(0) estimate ggplot(mr, aes(x=alb_g0, y=g0, color=title, shape=factor(year))) + geom_point() + geom_abline(slope = 1, intercept = 0, lty=3, alpha=.6) + ylab(&#39;LTabundR&#39;) + xlab(&#39;ALB et al. (2022)&#39;) + facet_wrap(~scenario, nrow=3) + labs(title=&#39;Estimates of g(0)&#39;, shape=&#39;Year&#39;, color = &#39;Species&#39;) + theme_light()   # g(0) CV estimate ggplot(mr, aes(x=alb_g0_cv, y=g0_cv, color=title, shape=factor(year))) + geom_point() + geom_abline(slope = 1, intercept = 0, lty=3, alpha=.6) + ylab(&#39;LTabundR&#39;) + xlab(&#39;ALB et al. (2022)&#39;) + facet_wrap(~scenario, nrow=3) + labs(title=&#39;Estimates of CV of g(0)&#39;, shape=&#39;Year&#39;, color = &#39;Species&#39;) + theme_light() "],["stratagallery.html", " 16 Strata gallery Central North Pacific California Current ETP", " 16 Strata gallery This packages comes with several built-in datasets of geographic strata that are commonly used in NOAA/NMFS surveys. The functions strata_explore() and strata_select() were developed to help you explore those built-in options. Central North Pacific strata_explore(&#39;cnp&#39;) To acquire the filepath to one of these strata, pass the index (or indices) printed in the map titles above to the function strata_select(): strata &lt;- strata_select(selections = c(2,3), region = &#39;cnp&#39;) This function returns a named list that can be passed directly to the strata argument in load_settings(). strata %&gt;% names [1] &quot;OtherCNP&quot; &quot;MHI&quot; strata $OtherCNP Lon Lat 1 -131 40.00 2 -126 32.05 3 -120 25.00 4 -120 -5.00 5 -185 -5.00 6 -185 40.00 7 -131 40.00 $MHI Lon Lat 1 -156.00 22.00 2 -154.40 20.60 3 -153.50 19.20 4 -154.35 18.55 5 -155.20 17.75 6 -157.00 18.25 7 -157.50 19.20 8 -161.00 21.84 9 -160.00 23.00 10 -157.00 22.50 11 -156.00 22.00 California Current strata_explore(&#39;ccs&#39;) ETP You can do the same for the Eastern Tropical Pacific (ETP); there are about 70 polygons built-in to LTabundR for this region. We will just show a few of them here, using the start_at argument. strata_explore(&#39;etp&#39;, start_at = 64) "],["segmentizing.html", " 17 Segmentizing Approach Base example Day vs Equal Length Contiguous vs. non-contiguous effort Segment remainder handling Typical settings", " 17 Segmentizing The package’s segmentize() function and its associated settings were designed to give researchers full control over how data are segmented, be it for design-based density analysis (which tend to use long segments of 100 km or more and allow for non-contiguous effort to be included in the same segment) or for habitat modeling (which tend to use short segments of 5 - 10 km and disallow non-contiguous effort to be pooled into the same segment). Approach Segments are built and stored separately for each cohort of species, since each cohort has specific settings for segmentizing. Within each cohort, the survey is first grouped into blocs of data that all share the same “effort scenario”, i.e., all rows share the same Cruise number, study area status (in or out), geographic stratum, and year. Since a survey may leave a stratum then return to it many days hence, it is normal for these blocs to contain non-contiguous data with large spatial gaps. These gaps will be addressed a few steps down. The blocs are split a final time according to whether the effort scenario meets inclusion criteria for the analysis. These inclusion criteria are controlled by the cohort-specific settings such as distance_types, distance_modes, and distance_on_off. Rows of data that meet the inclusion criteria are relegated to their own data bloc, and given a new column, use, with the value TRUE. Data that do not meet the criteria are relegated to their own bloc as well (column use is FALSE). This means that, at the end of this process, we will have segments that will be used in the density/detection function analysis, and segments that will not. (The excluded segments are not deleted or transformed in any other way; they can still be used in summarizing detections, etc.) Next, the segmentize() function loops through each of these blocs of effort and parses its data into segments according to the segment_method. If segmentizing by \"day\", this is straightforward: all data occurring on a unique date are assigned to its own segment. Segmentizing by \"equallength\" is a bit more complicated in terms of coding: segments are built up one line of data at a time; if the segment_target_km is reached or the segment_max_interval is exceeded, a new segment begins. At the end of this process, you have lists of data sorted into their segments, each with a unique seg_id, as well as a summary dataframe that provides the distance (km); time and coordinates for the beginning, middle, and end of the segment; and the weighted averages of sighting conditions and weather data contained in the segment. Setting up this demo The demonstration of segmentize() on in Processing chapter relies on the settings list that is attached as a slot in the cruz object. But you can override those settings with direct function inputs in segmentize(), which gives us a chance to explore segmentization options. First we load the demo data and carry out initial processing: # Load built-in settings example data(example_settings) settings &lt;- example_settings # Set path to DAS file das_file &lt;- &#39;data/surveys/HICEASwinter2020.das&#39; # First steps of formatting das &lt;- das_load(das_file, perform_checks = FALSE, print_glimpse = FALSE) cruz &lt;- process_strata(das, settings, verbose=FALSE) cruz &lt;- das_format(cruz) Base example Here is the segmentize() function parameterized with some basic settings: a target segment length of 30km, with the maximum allowable gap in effort within segments to be 24 hours. cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 30, segment_max_interval = 24, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), distance_types = c(&#39;S&#39;,&#39;F&#39;,&#39;N&#39;), distance_modes = c(&#39;P&#39;,&#39;C&#39;), distance_on_off = c(TRUE), verbose=FALSE, to_plot = TRUE) # A tibble: 76 × 39 seg_id Cruise ship stratum use Mode EffType OnEffort ESWsides dist &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 2001 OES HI_EEZ FALSE NA-P NA-S FALSE-TRUE NA 0 2 14 2001 OES WHICEAS TRUE C F TRUE 2 30.1 3 18 2001 OES WHICEAS TRUE C N TRUE 2 30.4 4 19 2001 OES WHICEAS TRUE C N TRUE 2 30.6 5 26 2001 OES WHICEAS TRUE C S TRUE 2 29.7 6 29 2001 OES WHICEAS TRUE C S TRUE 2 30.0 7 32 2001 OES WHICEAS TRUE C S TRUE 2 29.8 8 38 2001 OES WHICEAS TRUE C S TRUE 2 30.1 9 44 2001 OES WHICEAS TRUE C S TRUE 2 30.2 10 45 2001 OES WHICEAS TRUE C S TRUE 2 29.9 # ℹ 66 more rows # ℹ 29 more variables: minutes &lt;dbl&gt;, n_rows &lt;int&gt;, min_line &lt;int&gt;, # max_line &lt;int&gt;, year &lt;dbl&gt;, month &lt;dbl&gt;, day &lt;int&gt;, lat1 &lt;dbl&gt;, lon1 &lt;dbl&gt;, # DateTime1 &lt;dttm&gt;, timestamp1 &lt;dbl&gt;, yday1 &lt;dbl&gt;, lat2 &lt;dbl&gt;, lon2 &lt;dbl&gt;, # DateTime2 &lt;dttm&gt;, timestamp2 &lt;dbl&gt;, yday2 &lt;dbl&gt;, mlat &lt;dbl&gt;, mlon &lt;dbl&gt;, # mDateTime &lt;dttm&gt;, mtimestamp &lt;dbl&gt;, avgBft &lt;dbl&gt;, avgSwellHght &lt;dbl&gt;, # avgHorizSun &lt;dbl&gt;, avgVertSun &lt;dbl&gt;, avgGlare &lt;dbl&gt;, avgVis &lt;dbl&gt;, … seg_id Cruise ship stratum use Mode EffType OnEffort ESWsides 1 1 2001 OES HI_EEZ FALSE NA-P NA-S FALSE-TRUE NA 2 14 2001 OES WHICEAS TRUE C F TRUE 2 3 18 2001 OES WHICEAS TRUE C N TRUE 2 4 19 2001 OES WHICEAS TRUE C N TRUE 2 5 26 2001 OES WHICEAS TRUE C S TRUE 2 6 29 2001 OES WHICEAS TRUE C S TRUE 2 7 32 2001 OES WHICEAS TRUE C S TRUE 2 8 38 2001 OES WHICEAS TRUE C S TRUE 2 9 44 2001 OES WHICEAS TRUE C S TRUE 2 10 45 2001 OES WHICEAS TRUE C S TRUE 2 11 50 2001 OES WHICEAS TRUE C S TRUE 2 12 53 2001 OES WHICEAS TRUE C S TRUE 2 13 59 2001 OES WHICEAS TRUE C S TRUE 2 14 60 2001 OES WHICEAS TRUE C S TRUE 2 15 66 2001 OES WHICEAS TRUE C S TRUE 2 16 71 2001 OES WHICEAS TRUE C S TRUE 2 17 77 2001 OES WHICEAS TRUE C S TRUE 2 18 78 2001 OES WHICEAS TRUE C-P S TRUE 2 19 79 2001 OES WHICEAS TRUE C S TRUE 2 20 85 2001 OES WHICEAS TRUE C S TRUE 2 21 95 2001 OES WHICEAS TRUE C S TRUE 2 22 103 2001 OES WHICEAS TRUE C S TRUE 2 23 113 2001 OES WHICEAS TRUE C S TRUE 2 24 117 2001 OES WHICEAS TRUE C S TRUE 2 25 123 2001 OES WHICEAS TRUE C S TRUE 2 26 124 2001 OES WHICEAS TRUE C S TRUE 2 27 126 2001 OES WHICEAS TRUE C S TRUE 2 28 132 2001 OES WHICEAS TRUE C S TRUE 2 29 135 2001 OES WHICEAS TRUE C S TRUE 2 30 138 2001 OES WHICEAS TRUE C S TRUE 2 31 147 2001 OES WHICEAS TRUE C S TRUE 2 32 151 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 33 152 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 34 153 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 35 154 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 36 155 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 37 157 2001 OES WHICEAS FALSE C-NA S-NA-N-F FALSE-TRUE 2 38 158 2001 OES WHICEAS FALSE C-NA F-N-NA-S FALSE-TRUE 2 39 159 2001 OES WHICEAS FALSE C-NA N-S-NA FALSE-TRUE 2 40 160 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 41 161 2001 OES WHICEAS FALSE C-NA F-NA FALSE-TRUE 2 42 165 2001 OES WHICEAS FALSE C-NA S-F-NA FALSE-TRUE 2 43 166 2001 OES WHICEAS FALSE C-NA S-N-NA FALSE-TRUE 2 44 171 2001 OES WHICEAS FALSE P-NA-C N-NA-S FALSE-TRUE 2 45 173 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 46 175 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 47 176 2001 OES WHICEAS FALSE C-NA S-NA-F-N FALSE-TRUE 2 48 178 2001 OES WHICEAS FALSE C-NA N-NA-S FALSE-TRUE 2 49 180 2001 OES WHICEAS FALSE NA-C NA-F-S FALSE-TRUE NA 50 181 2001 OES WHICEAS FALSE C-P F-S-NA FALSE-TRUE 2 51 182 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 52 184 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 53 188 2001 OES WHICEAS FALSE NA-C NA-S FALSE-TRUE NA 54 192 2001 OES WHICEAS FALSE C-NA S-NA FALSE 2 55 196 2001 OES WHICEAS FALSE C-NA N-NA FALSE-TRUE 2 56 198 2001 OES WHICEAS FALSE C-NA N-NA FALSE-TRUE 2 57 199 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 58 202 2001 OES WHICEAS FALSE C-NA N-NA-S FALSE-TRUE 2 59 203 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 60 204 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 61 205 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 62 209 2001 OES WHICEAS FALSE NA-C NA-N-S FALSE-TRUE NA 63 214 2001 OES WHICEAS FALSE C-NA S-NA FALSE 2 64 219 2001 OES WHICEAS FALSE C-NA S-NA-N FALSE-TRUE 2 65 220 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 66 221 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 67 224 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 68 227 2001 OES WHICEAS FALSE C-NA S-NA-F-N FALSE-TRUE 2 69 228 2001 OES WHICEAS FALSE C-NA N-F-NA FALSE-TRUE 2 70 235 2001 OES WHICEAS TRUE C-P N TRUE 2 71 236 2001 OES WHICEAS TRUE C N TRUE 2 72 240 2001 OES WHICEAS TRUE C N TRUE 2 73 241 2001 OES WHICEAS TRUE C N TRUE 2 74 247 2001 OES WHICEAS TRUE C S TRUE 2 75 253 2001 OES WHICEAS TRUE C S TRUE 2 76 263 2001 OES WHICEAS TRUE C S TRUE 2 dist minutes n_rows min_line max_line year month day lat1 1 0.00000 NA 15 1080 1549 2020 1 21 22.33300 2 30.11134 1172.033 104 580 872 2020 1 19 21.83833 3 30.41913 17538.917 93 5804 6479 2020 1 30 20.87867 4 30.57750 6934.483 108 10874 11483 2020 2 10 19.40450 5 29.68669 931.767 66 2151 2254 2020 1 22 22.30717 6 29.97408 970.833 81 2485 2613 2020 1 23 22.84950 7 29.82113 1019.683 85 3006 3195 2020 1 24 23.13983 8 30.06078 955.750 79 3507 3614 2020 1 25 23.11800 9 30.18372 898.033 63 4038 4121 2020 1 26 22.08867 10 29.85571 912.467 66 1044 1120 2020 1 20 21.91817 11 29.63976 969.300 92 4545 4715 2020 1 27 22.24983 12 30.23046 1044.600 80 4909 5138 2020 1 28 20.88550 13 29.67595 977.850 89 5581 5697 2020 1 29 19.93500 14 30.21943 1372.267 87 5698 6143 2020 1 30 20.94583 15 30.19170 929.733 70 6571 6680 2020 1 31 20.75250 16 30.07983 912.200 84 6990 7081 2020 2 1 19.58400 17 30.23991 900.750 71 7541 7617 2020 2 2 19.37333 18 30.12915 1205.217 95 1269 1566 2020 1 21 22.15417 19 37.77530 1433.267 104 7618 8078 2020 2 3 20.44300 20 30.07272 872.900 69 8459 8539 2020 2 4 19.98200 21 30.14647 1008.117 77 10295 10472 2020 2 8 21.21033 22 29.95008 899.150 72 9786 9881 2020 2 7 19.51833 23 30.20142 881.200 85 13295 13390 2020 2 22 22.58550 24 29.80910 958.050 87 13789 13940 2020 2 23 21.77583 25 29.93410 1410.900 101 14361 14877 2020 2 24 22.36100 26 29.97225 2475.950 72 14878 15702 2020 2 25 21.45900 27 29.51911 1425.750 98 15794 16218 2020 2 27 18.86183 28 30.00331 963.283 84 12868 13017 2020 2 21 23.01600 29 29.69969 2651.933 66 20657 20917 2020 3 8 21.46933 30 30.11431 881.383 84 21848 21971 2020 3 10 21.34117 31 29.90610 962.700 88 21324 21490 2020 3 9 20.83850 32 30.28818 1381.700 143 2896 3345 2020 1 24 23.24167 33 29.85207 1314.783 79 3346 3691 2020 1 25 23.27300 34 29.67374 1882.033 103 3692 4510 2020 1 26 22.37667 35 30.25388 1311.300 153 4511 4951 2020 1 27 22.23950 36 30.09869 972.600 124 4952 5129 2020 1 28 20.82167 37 30.29592 1533.017 132 5407 5930 2020 1 29 20.13883 38 29.87289 1484.367 149 5931 6441 2020 1 30 20.66533 39 29.88844 1459.733 125 6442 6943 2020 1 31 20.74050 40 30.26964 2570.950 172 6944 7705 2020 2 1 19.62133 41 30.36532 1169.933 153 347 667 2020 1 19 21.98533 42 30.15990 2560.433 114 7957 8664 2020 2 3 20.21333 43 30.27013 1226.300 69 8665 8989 2020 2 5 19.24783 44 30.18635 1151.917 103 9274 9502 2020 2 6 20.33467 45 29.51573 1251.150 102 9503 9864 2020 2 7 19.77117 46 30.09191 1196.117 125 10122 10436 2020 2 8 21.05450 47 29.92316 1570.300 119 10437 11060 2020 2 9 20.75683 48 29.94592 1292.083 94 11211 11571 2020 2 10 20.03083 49 30.07269 1696.517 195 772 1476 2020 1 20 21.53550 50 30.21295 1319.100 123 1477 1950 2020 1 21 21.97083 51 29.76123 1260.917 103 1951 2303 2020 1 22 22.44267 52 29.94687 1176.150 85 2390 2682 2020 1 23 22.70117 53 29.52854 3890.100 122 12408 12990 2020 2 21 22.59067 54 29.88480 937.750 73 16006 16078 2020 2 27 18.98750 55 39.17835 915.133 85 16491 16648 2020 2 28 19.08167 56 29.83348 990.833 94 16998 17208 2020 2 29 19.82750 57 30.44439 1507.200 115 12991 13527 2020 2 22 22.80083 58 30.19830 1426.333 112 17444 17840 2020 3 1 18.78983 59 29.95142 1129.467 112 17841 18122 2020 3 2 18.87733 60 30.16677 2924.150 104 18123 19174 2020 3 3 18.42000 61 29.80048 1330.533 89 19175 19556 2020 3 5 17.75567 62 30.14852 1400.583 85 19725 20212 2020 3 6 18.66700 63 29.88025 873.050 63 20398 20460 2020 3 7 20.38633 64 30.32890 2410.333 101 20732 21416 2020 3 8 21.54700 65 29.96941 1592.167 132 21417 22049 2020 3 10 21.66900 66 29.85556 1102.333 96 13816 14073 2020 2 23 21.76900 67 29.85875 1181.633 113 14074 14388 2020 2 24 22.20917 68 29.86928 1311.183 174 14740 15277 2020 2 25 21.45600 69 29.71765 1122.650 142 15278 15579 2020 2 26 19.60250 70 23.22903 8630.017 70 8927 9263 2020 2 5 19.52183 71 29.55520 3853.250 135 16263 16611 2020 2 28 19.13517 72 29.79137 980.950 104 17021 17228 2020 2 29 19.80067 73 30.53747 1557.567 105 17229 17722 2020 3 1 19.02300 74 29.79833 901.267 84 18534 18644 2020 3 3 18.13633 75 30.21042 852.883 69 18969 19041 2020 3 4 18.19567 76 29.90193 1016.333 85 17923 18093 2020 3 2 18.89667 lon1 DateTime1 timestamp1 yday1 lat2 lon2 1 -161.2520 2020-01-21 07:28:48 1579591728 21 22.68750 -161.1208 2 -160.1427 2020-01-19 17:48:16 1579456096 19 21.75217 -160.5167 3 -157.0888 2020-01-30 11:28:54 1580383734 30 20.74350 -159.3752 4 -154.8637 2020-02-10 07:45:59 1581320759 41 21.12850 -156.6312 5 -159.6305 2020-01-22 18:09:55 1579716595 22 22.67867 -159.5432 6 -160.2023 2020-01-23 15:25:41 1579793141 23 23.32000 -160.3017 7 -159.5945 2020-01-24 15:25:54 1579879554 24 23.42867 -159.2748 8 -158.0090 2020-01-25 16:19:35 1579969175 25 22.43183 -158.4972 9 -157.0058 2020-01-26 17:54:22 1580061262 26 21.96517 -158.0397 10 -161.0890 2020-01-20 17:12:20 1579540340 20 22.29700 -161.1112 11 -159.3815 2020-01-27 17:44:24 1580147064 27 20.98850 -159.1777 12 -158.5883 2020-01-28 14:09:42 1580220582 28 20.25967 -157.8320 13 -156.5590 2020-01-29 16:52:18 1580316738 29 20.94767 -157.3412 14 -157.3360 2020-01-30 09:10:09 1580375409 30 20.36683 -158.2468 15 -159.7268 2020-01-31 17:58:42 1580493522 31 19.81217 -159.4830 16 -158.5412 2020-02-01 16:30:26 1580574626 32 19.72950 -157.5458 17 -156.1343 2020-02-02 17:46:38 1580665598 33 20.44417 -157.0048 18 -160.5357 2020-01-21 11:46:48 1579607208 21 22.68150 -161.0983 19 -156.9997 2020-02-03 08:47:23 1580719643 34 20.37950 -154.8502 20 -153.5155 2020-02-04 17:45:03 1580838303 35 19.11783 -153.4092 21 -155.2308 2020-02-08 17:45:34 1581183934 39 20.78217 -155.2000 22 -153.2655 2020-02-07 17:30:25 1581096625 38 20.86700 -153.8352 23 -157.3837 2020-02-22 16:51:15 1582390275 53 22.04067 -156.7950 24 -155.7580 2020-02-23 16:36:30 1582475790 54 22.04450 -155.2472 25 -156.5795 2020-02-24 17:56:36 1582566996 55 21.45900 -156.2140 26 -156.2140 2020-02-25 17:25:32 1582651532 56 18.78150 -156.9642 27 -157.2763 2020-02-27 12:37:27 1582807047 58 19.15467 -156.5753 28 -157.6487 2020-02-21 18:15:47 1582308947 52 22.75767 -158.2260 29 -161.0363 2020-03-08 13:00:52 1583672452 68 21.19283 -161.5130 30 -158.9122 2020-03-10 16:53:50 1583859230 70 21.80100 -159.1330 31 -160.0968 2020-03-09 16:49:08 1583772548 69 21.64350 -160.0990 32 -159.8905 2020-01-24 13:05:53 1579871153 24 23.27450 -158.6780 33 -158.6727 2020-01-25 12:07:35 1579954055 25 22.38017 -158.2480 34 -158.2438 2020-01-26 10:02:22 1580032942 26 22.23883 -159.3152 35 -159.3212 2020-01-27 17:24:24 1580145864 27 20.82300 -158.5038 36 -158.5065 2020-01-28 15:15:42 1580224542 28 20.26350 -157.8470 37 -157.1785 2020-01-29 12:50:18 1580302218 29 20.66650 -157.0180 38 -157.0158 2020-01-30 14:24:20 1580394260 30 20.73850 -159.2903 39 -159.2873 2020-01-31 15:08:42 1580483322 31 19.62233 -158.6952 40 -158.6907 2020-02-01 15:28:26 1580570906 32 20.42483 -156.9735 41 -159.9723 2020-01-19 13:37:58 1579441078 19 21.61683 -160.7107 42 -156.1027 2020-02-03 16:54:10 1580748850 34 19.25017 -153.8952 43 -153.8957 2020-02-05 11:33:57 1580902437 36 19.30733 -154.8340 44 -154.1268 2020-02-06 16:00:54 1581004854 37 19.77333 -154.1133 45 -154.1118 2020-02-07 11:13:23 1581074003 38 20.83767 -153.7818 46 -154.6462 2020-02-08 13:43:34 1581169414 39 20.75183 -155.3288 47 -155.3277 2020-02-09 09:39:41 1581241181 40 19.76050 -154.8913 48 -155.2315 2020-02-10 15:13:59 1581347639 41 21.23117 -156.8088 49 -160.4632 2020-01-20 11:36:18 1579520178 20 21.97400 -160.1832 50 -160.1843 2020-01-21 15:52:49 1579621969 21 22.44150 -160.1892 51 -160.1907 2020-01-22 13:51:55 1579701115 22 22.72067 -159.7432 52 -159.8097 2020-01-23 13:03:41 1579784621 23 23.27833 -160.1368 53 -155.9668 2020-02-21 07:13:47 1582269227 52 22.80517 -158.2527 54 -157.7680 2020-02-27 17:15:27 1582823727 58 19.22517 -156.9488 55 -156.0213 2020-02-28 18:07:44 1582913264 59 19.18850 -156.3523 56 -156.1603 2020-02-29 16:55:00 1582995300 60 19.01950 -156.0935 57 -158.2507 2020-02-22 09:41:15 1582364475 53 21.95100 -156.4322 58 -155.4313 2020-03-01 14:44:08 1583073848 61 18.87567 -155.6212 59 -155.6228 2020-03-02 14:30:28 1583159428 62 18.42100 -155.5743 60 -155.5698 2020-03-03 09:19:56 1583227196 63 17.75783 -156.1800 61 -156.1852 2020-03-05 10:04:05 1583402645 65 18.46200 -157.2170 62 -158.0068 2020-03-06 12:52:37 1583499157 66 20.19067 -159.3087 63 -160.1068 2020-03-07 17:11:12 1583601072 67 21.23150 -160.1178 64 -161.3487 2020-03-08 14:50:15 1583679015 68 21.67017 -160.2080 65 -160.2052 2020-03-10 06:59:50 1583823590 70 21.69333 -158.8137 66 -155.6813 2020-02-23 17:08:30 1582477710 54 22.20517 -155.6708 67 -155.6685 2020-02-24 11:30:36 1582543836 55 21.66800 -157.0692 68 -156.3440 2020-02-25 15:03:37 1582643017 56 19.60517 -156.0277 69 -156.0267 2020-02-26 12:56:48 1582721808 57 18.65183 -156.4478 70 -154.7868 2020-02-05 17:26:59 1580923619 36 20.32900 -154.1045 71 -156.4318 2020-02-28 13:20:30 1582896030 59 19.20317 -156.2690 72 -156.2268 2020-02-29 17:21:00 1582996860 60 19.02500 -156.0512 73 -156.0487 2020-03-01 09:42:08 1583055728 61 18.72400 -155.2490 74 -154.4443 2020-03-03 17:49:56 1583257796 63 17.85150 -154.8355 75 -156.1580 2020-03-04 16:51:12 1583340672 64 17.68383 -155.7863 76 -155.8423 2020-03-02 15:52:28 1583164348 62 18.44033 -155.6443 DateTime2 timestamp2 yday2 mlat mlon mDateTime 1 2020-01-22 07:42:01 1579678921 22 22.69050 -161.1327 2020-01-22 07:33:52 2 2020-01-20 13:20:18 1579526418 20 21.56783 -160.4438 2020-01-20 12:08:01 3 2020-01-31 16:00:53 1580486453 31 20.57667 -156.8462 2020-01-30 17:46:23 4 2020-02-11 11:25:23 1581420323 42 19.88600 -155.0652 2020-02-10 13:27:59 5 2020-01-23 09:39:41 1579772381 23 22.63000 -159.3107 2020-01-23 08:17:41 6 2020-01-24 07:34:31 1579851271 24 22.92150 -160.4980 2020-01-23 17:18:12 7 2020-01-25 08:23:35 1579940615 25 23.10933 -159.4403 2020-01-24 16:26:57 8 2020-01-26 08:14:22 1580026462 26 23.09167 -157.8485 2020-01-25 17:21:02 9 2020-01-27 08:50:24 1580115024 27 21.90683 -157.9103 2020-01-27 08:02:24 10 2020-01-21 08:22:48 1579594968 21 21.93883 -161.2147 2020-01-20 17:56:20 11 2020-01-28 09:51:42 1580205102 28 21.04950 -159.3770 2020-01-28 07:37:42 12 2020-01-29 07:32:18 1580283138 29 20.84467 -158.4327 2020-01-28 16:03:42 13 2020-01-30 09:08:09 1580375289 30 19.89133 -156.3947 2020-01-29 17:56:18 14 2020-01-31 08:00:25 1580457625 31 20.87500 -157.1435 2020-01-30 11:08:12 15 2020-02-01 09:26:26 1580549186 32 19.85550 -159.6207 2020-02-01 08:18:26 16 2020-02-02 07:40:38 1580629238 33 19.55233 -158.4150 2020-02-01 17:20:30 17 2020-02-03 08:45:23 1580719523 34 20.47317 -157.1222 2020-02-03 08:00:46 18 2020-01-22 07:50:01 1579679401 22 22.07117 -160.3347 2020-01-21 13:18:48 19 2020-02-04 08:38:39 1580805519 35 20.37800 -155.0227 2020-02-04 07:33:03 20 2020-02-05 08:15:57 1580890557 36 19.08667 -153.2877 2020-02-05 07:33:57 21 2020-02-09 10:31:41 1581244301 40 20.79650 -155.3543 2020-02-09 07:59:41 22 2020-02-08 08:27:34 1581150454 39 20.81100 -153.6818 2020-02-08 07:20:14 23 2020-02-23 07:30:27 1582443027 54 22.55267 -157.2653 2020-02-22 17:41:15 24 2020-02-24 08:32:33 1582533153 55 21.74833 -155.5783 2020-02-23 18:00:36 25 2020-02-25 17:25:30 1582651530 56 21.51467 -156.4527 2020-02-25 13:41:12 26 2020-02-27 10:41:27 1582800087 58 19.81033 -156.0953 2020-02-26 11:04:48 27 2020-02-28 12:21:12 1582892472 59 18.89283 -157.4175 2020-02-27 13:33:27 28 2020-02-22 10:17:04 1582366624 53 22.84833 -158.4280 2020-02-22 08:09:15 29 2020-03-09 07:29:08 1583738948 69 21.50533 -161.1775 2020-03-08 13:50:15 30 2020-03-11 07:33:13 1583911993 71 21.30933 -158.7917 2020-03-10 17:37:50 31 2020-03-10 08:49:50 1583830190 70 20.80117 -159.9445 2020-03-09 17:51:08 32 2020-01-25 12:05:35 1579953935 25 23.13483 -159.4045 2020-01-24 17:05:54 33 2020-01-26 10:00:22 1580032822 26 22.44500 -158.5460 2020-01-26 07:56:04 34 2020-01-27 17:22:24 1580145744 27 22.14200 -158.9722 2020-01-27 14:21:57 35 2020-01-28 15:13:42 1580224422 28 20.99633 -159.2700 2020-01-28 09:11:42 36 2020-01-29 07:26:18 1580282778 29 20.78467 -158.3702 2020-01-28 17:02:16 37 2020-01-30 14:22:20 1580394140 30 20.85867 -157.2002 2020-01-30 10:38:12 38 2020-01-31 15:06:42 1580483202 31 20.56167 -156.8263 2020-01-30 17:12:20 39 2020-02-01 15:26:26 1580570786 32 19.78867 -159.3478 2020-02-01 10:18:26 40 2020-02-03 10:17:23 1580725043 34 19.39117 -156.2590 2020-02-02 16:50:38 41 2020-01-20 09:05:54 1579511154 20 21.85967 -159.9373 2020-01-19 16:09:58 42 2020-02-05 11:32:36 1580902356 36 20.12617 -154.0845 2020-02-04 13:39:03 43 2020-02-06 07:58:54 1580975934 37 19.40733 -154.8660 2020-02-06 06:51:18 44 2020-02-07 11:12:25 1581073945 38 19.77000 -154.2380 2020-02-07 09:34:25 45 2020-02-08 08:03:34 1581149014 39 19.67917 -153.9075 2020-02-07 13:02:25 46 2020-02-09 09:37:41 1581241061 40 20.87667 -155.4635 2020-02-09 07:09:41 47 2020-02-10 11:47:59 1581335279 41 19.29517 -154.8390 2020-02-10 07:05:53 48 2020-02-11 12:44:04 1581425044 42 21.19117 -156.4023 2020-02-11 09:12:21 49 2020-01-21 15:50:49 1579621849 21 22.11200 -160.4542 2020-01-21 12:21:12 50 2020-01-22 13:49:55 1579700995 22 22.66667 -161.0353 2020-01-22 08:13:17 51 2020-01-23 10:50:50 1579776650 23 22.32667 -159.6942 2020-01-22 17:43:35 52 2020-01-24 08:37:50 1579855070 24 22.87300 -160.3497 2020-01-23 16:23:41 53 2020-02-22 09:39:15 1582364355 53 22.88383 -157.2532 2020-02-21 15:42:18 54 2020-02-28 08:51:12 1582879872 59 19.20800 -157.0698 2020-02-28 07:49:12 55 2020-02-29 09:22:44 1582968164 60 19.18033 -156.1485 2020-02-29 07:51:24 56 2020-03-01 09:24:08 1583054648 61 19.06000 -155.9935 2020-03-01 08:02:13 57 2020-02-23 10:46:27 1582454787 54 22.72650 -157.9458 2020-02-22 12:53:15 58 2020-03-02 14:28:28 1583159308 62 18.65467 -154.9318 2020-03-02 09:12:26 59 2020-03-03 09:17:56 1583227076 63 19.04017 -156.0997 2020-03-02 17:54:28 60 2020-03-05 10:02:05 1583402525 65 17.80233 -154.6550 2020-03-04 07:37:12 61 2020-03-06 08:12:37 1583482357 66 18.42233 -157.0558 2020-03-06 07:03:45 62 2020-03-07 12:11:12 1583583072 67 20.14767 -159.1818 2020-03-07 11:20:24 63 2020-03-08 07:42:15 1583653335 68 21.19583 -159.9885 2020-03-08 06:56:15 64 2020-03-10 06:58:35 1583823515 70 21.14183 -161.3468 2020-03-09 08:37:08 65 2020-03-11 09:30:45 1583919045 71 21.43433 -159.2933 2020-03-10 14:13:10 66 2020-02-24 11:28:50 1582543730 55 21.98117 -155.0772 2020-02-24 07:26:33 67 2020-02-25 07:10:28 1582614628 56 22.25250 -155.9182 2020-02-24 13:34:36 68 2020-02-26 12:54:48 1582721688 57 21.46117 -156.3065 2020-02-25 16:25:32 69 2020-02-27 07:37:27 1582789047 58 19.58917 -156.0185 2020-02-26 16:42:48 70 2020-02-06 15:50:54 1581004254 37 20.19217 -154.1955 2020-02-06 14:46:44 71 2020-02-29 08:40:03 1582965603 60 19.06033 -156.0258 2020-02-28 17:59:12 72 2020-03-01 09:40:54 1583055654 61 19.22900 -155.9470 2020-03-01 06:57:34 73 2020-03-02 11:38:28 1583149108 62 18.93683 -155.9200 2020-03-01 10:41:04 74 2020-03-04 08:49:12 1583311752 64 17.81283 -154.6905 2020-03-04 07:51:12 75 2020-03-05 07:02:05 1583391725 65 18.22867 -156.2812 2020-03-04 17:35:56 76 2020-03-03 08:46:48 1583225208 63 18.94217 -156.0327 2020-03-02 16:59:56 mtimestamp avgBft avgSwellHght avgHorizSun avgVertSun avgGlare avgVis 1 1579678432 NaN NaN NaN NaN NaN NaN 2 1579522081 6.000000 7.000000 5.006984 1.265965 0.000000000 5.906890 3 1580406383 3.449423 3.424377 9.873088 1.476085 0.025320055 5.758327 4 1581341279 4.984964 6.609969 6.354441 1.629767 0.000000000 4.945342 5 1579767461 3.908640 10.589045 5.969112 2.853276 0.000000000 5.840040 6 1579799892 5.000000 8.018897 10.000000 2.841371 0.000000000 5.134127 7 1579883217 4.604578 6.928138 6.608468 2.323192 0.323191764 5.063091 8 1579972862 4.848507 9.697013 7.019397 2.288485 0.288485308 4.388787 9 1580112144 4.244529 6.076286 5.977618 2.868228 0.000000000 5.538539 10 1579542980 4.500147 7.000000 11.457187 2.457187 1.000000000 6.123167 11 1580197062 2.856164 4.287279 11.699757 2.666664 1.000000000 5.974494 12 1580227422 3.209909 4.859939 5.125472 1.887951 0.000000000 6.000000 13 1580320578 3.374642 3.687321 6.900354 2.398922 0.312679018 5.687321 14 1580382492 3.933876 3.731258 6.666280 2.051634 0.320376349 5.182815 15 1580545106 3.293369 7.000000 11.964046 2.800021 1.000000000 5.000000 16 1580577630 4.514820 11.604233 4.771231 2.169337 0.057092434 5.057092 17 1580716846 5.356001 4.761397 10.190179 3.000000 0.761397228 5.119301 18 1579612728 3.635635 5.545779 2.719912 1.000000 0.000000000 6.388294 19 1580801583 5.395991 5.780025 6.824756 2.148208 0.716175060 4.716175 20 1580888037 4.000000 6.000000 5.808575 3.000000 0.000000000 5.139739 21 1581235181 4.558632 6.144830 5.793097 2.260357 0.861473879 5.185497 22 1581146414 3.728904 6.728904 5.967942 2.814019 0.000000000 6.210854 23 1582393275 4.424060 6.707446 6.794641 2.307835 0.292553879 5.292554 24 1582480836 4.489379 6.503823 4.927080 2.037364 0.000000000 5.068030 25 1582638072 5.743381 6.827423 3.453381 1.189094 0.045773787 4.956306 26 1582715088 4.840472 6.848849 6.449868 1.597097 0.000000000 5.494620 27 1582810407 5.394489 8.185377 5.822288 1.000000 0.000000000 5.124709 28 1582358955 5.000000 4.973905 12.000000 2.347732 1.000000000 5.657877 29 1583675415 5.714547 7.000000 9.428236 1.543040 0.271519945 5.142726 30 1583861870 2.305764 3.356311 7.572113 1.734890 0.367444759 6.030576 31 1583776268 4.668650 5.337300 7.299543 2.019907 0.331349978 6.000867 32 1579885554 4.892875 8.590881 2.505890 1.357796 0.000000000 5.116564 33 1580025364 4.705114 9.335287 1.922227 1.512782 0.428943475 4.736200 34 1580134917 4.170120 6.425127 9.170609 1.845090 0.000000000 5.320284 35 1580202702 3.164030 4.216177 6.855799 1.963057 0.463619780 6.121177 36 1580230936 2.746796 4.757814 5.806156 1.873854 0.126602105 6.000000 37 1580380692 3.240878 3.139038 7.889379 1.580339 0.580338554 5.337239 38 1580404340 3.205111 3.151402 4.661279 1.868094 0.000000000 5.751274 39 1580552306 4.617969 7.406833 4.195037 1.652238 0.477562502 4.515688 40 1580662238 4.654442 5.800119 5.893508 2.062579 0.314448079 5.231499 41 1579450198 4.877867 6.766707 4.671661 2.118434 0.156681937 6.255405 42 1580823543 5.645166 6.843838 4.835606 1.891730 0.008451011 4.411348 43 1580971878 3.259680 4.277834 7.000000 1.000000 0.000000000 4.397393 44 1581068065 3.982997 6.000000 1.000000 2.000000 1.000000000 5.536595 45 1581080545 3.594203 6.763412 6.000000 3.000000 0.000000000 5.868732 46 1581232181 4.943615 8.539837 9.974673 1.658224 0.658224324 4.600021 47 1581318353 5.954118 8.427358 6.007619 1.194107 0.147237008 5.200512 48 1581412341 5.794124 8.808814 8.692008 1.464069 0.000000000 4.612411 49 1579609272 4.224909 5.776142 5.431631 1.154758 0.154757717 6.178133 50 1579680797 3.062491 4.000000 1.361171 1.000000 0.000000000 6.306494 51 1579715015 2.855288 6.940613 4.916079 1.899784 0.000000000 5.070247 52 1579796621 4.259707 9.503537 10.741760 2.124116 0.370879984 5.623382 53 1582299738 5.135638 5.864874 10.121714 1.370616 0.177347502 5.214919 54 1582876152 7.000000 10.376316 7.717734 2.156957 0.589716702 4.452502 55 1582962684 2.126459 3.117313 6.298214 2.701786 0.000000000 6.525142 56 1583049733 2.803562 1.956678 8.430326 2.088816 0.095953080 5.961721 57 1582375995 5.000000 5.476359 6.659753 1.506336 0.506336432 5.186444 58 1583140346 5.559758 8.361793 6.480688 1.614324 0.000000000 5.117277 59 1583171668 4.713039 6.989733 10.100864 1.884139 0.328714703 4.926707 60 1583307432 5.478923 7.621954 5.878409 1.550156 0.203464108 4.606435 61 1583478225 6.256296 6.256296 6.000000 2.256296 0.000000000 5.185926 62 1583580024 6.759634 7.562509 7.566860 1.000000 0.000000000 5.000000 63 1583650575 7.000000 7.515620 10.750647 1.750647 0.750646863 4.387116 64 1583743028 6.440106 6.973877 10.154108 1.269022 0.268246133 5.235192 65 1583849590 2.748577 4.356881 7.012370 1.826218 0.413109098 5.212602 66 1582529193 5.000000 5.435467 6.299025 1.350488 0.000000000 5.341576 67 1582551276 4.984325 5.015675 9.261732 1.000000 0.000000000 5.984325 68 1582647932 5.709386 6.519762 8.405883 1.549700 0.094283570 4.993677 69 1582735368 4.737827 4.068456 6.435252 1.898638 0.184123829 5.039025 70 1581000404 4.661152 5.392446 NaN NaN NaN 4.661152 71 1582912752 3.124031 4.696677 4.746863 1.568667 0.000000000 5.896244 72 1583045854 3.556184 3.587292 8.310047 2.641688 0.332120086 6.245119 73 1583059264 4.581598 6.182030 7.242571 1.436082 0.351593765 5.762387 74 1583308272 6.000000 7.355514 5.867296 2.132704 0.000000000 4.232596 75 1583343356 5.000000 6.551219 10.199061 2.578598 0.839812233 4.806988 76 1583168396 5.511027 7.210660 11.087888 2.000000 1.000000000 6.152876 avgCourse avgSpdKt 1 NaN NaN 2 277.06736 9.378851 3 196.90959 10.100242 4 149.24496 9.374871 5 278.11305 9.761088 6 282.49730 9.636017 7 107.52274 8.904260 8 105.21406 8.911500 9 283.41214 10.158480 10 193.76189 9.751525 11 152.74168 9.371143 12 113.29408 9.758419 13 106.87208 9.305486 14 165.77821 9.602570 15 109.26712 7.829979 16 104.73646 8.433167 17 103.72003 9.528129 18 95.67923 10.108030 19 100.55758 8.897106 20 268.49225 10.437658 21 113.38549 9.139067 22 236.64596 9.041504 23 105.53631 8.377777 24 203.79156 8.968617 25 128.57580 8.158653 26 229.71624 9.037101 27 224.24830 9.068803 28 109.56795 8.581114 29 234.39393 9.647128 30 105.98013 9.380064 31 104.17249 8.712338 32 86.96993 7.626220 33 104.47611 9.382258 34 235.42051 9.639034 35 112.17294 9.518207 36 137.42443 9.052868 37 113.27997 8.960292 38 136.70792 9.412532 39 99.17958 8.314833 40 154.58488 7.781760 41 220.49238 7.535807 42 134.82876 8.957854 43 218.43837 6.996102 44 215.34375 9.262080 45 124.22752 8.617591 46 211.30645 7.604920 47 213.10523 8.891897 48 305.41260 7.949114 49 191.42105 9.114373 50 184.02287 6.541224 51 217.62333 9.006339 52 228.88836 8.317266 53 198.76962 9.651166 54 179.82995 8.747033 55 192.06600 9.751325 56 213.97951 9.544307 57 96.29962 8.368427 58 130.96991 8.413619 59 200.45450 8.739029 60 207.09261 9.142551 61 284.05139 9.793405 62 282.14061 9.465631 63 287.19773 9.844472 64 224.62145 9.440073 65 106.12943 9.114607 66 272.51401 9.387362 67 277.92410 9.422015 68 240.28175 7.508963 69 206.66743 10.133638 70 63.34059 9.370393 71 139.44272 9.419229 72 205.93911 9.496354 73 68.28127 8.935122 74 267.62211 9.177008 75 285.29438 9.806098 76 255.80068 9.543037 Day vs Equal Length By day cruz_demo &lt;- segmentize(cruz, segment_method = &#39;day&#39;, verbose=FALSE, to_plot = TRUE) By target length of 100 km cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 100, segment_max_interval = 48, verbose=FALSE, to_plot = TRUE) # A tibble: 62 × 39 seg_id Cruise ship stratum use Mode EffType OnEffort ESWsides dist &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 2001 OES HI_EEZ FALSE NA-P NA-S FALSE-TRUE NA 0 2 6 2001 OES WHICEAS FALSE C-NA NA-F-N-S TRUE-FALSE NA 99.9 3 7 2001 OES WHICEAS FALSE C-NA S-NA-F-N FALSE-TRUE 2 99.6 4 8 2001 OES WHICEAS FALSE C-NA F-N-NA-S FALSE-TRUE 2 100. 5 9 2001 OES WHICEAS FALSE C-NA-P S-NA-F FALSE-TRUE 2 99.5 6 10 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 101. 7 11 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 99.6 8 12 2001 OES WHICEAS FALSE C-NA S-NA-N-F FALSE-TRUE 2 99.9 9 13 2001 OES WHICEAS FALSE C-NA S-N-NA FALSE-TRUE 2 100. 10 14 2001 OES WHICEAS FALSE C-NA S-F-NA-N FALSE-TRUE 2 98.7 # ℹ 52 more rows # ℹ 29 more variables: minutes &lt;dbl&gt;, n_rows &lt;int&gt;, min_line &lt;int&gt;, # max_line &lt;int&gt;, year &lt;dbl&gt;, month &lt;dbl&gt;, day &lt;int&gt;, lat1 &lt;dbl&gt;, lon1 &lt;dbl&gt;, # DateTime1 &lt;dttm&gt;, timestamp1 &lt;dbl&gt;, yday1 &lt;dbl&gt;, lat2 &lt;dbl&gt;, lon2 &lt;dbl&gt;, # DateTime2 &lt;dttm&gt;, timestamp2 &lt;dbl&gt;, yday2 &lt;dbl&gt;, mlat &lt;dbl&gt;, mlon &lt;dbl&gt;, # mDateTime &lt;dttm&gt;, mtimestamp &lt;dbl&gt;, avgBft &lt;dbl&gt;, avgSwellHght &lt;dbl&gt;, # avgHorizSun &lt;dbl&gt;, avgVertSun &lt;dbl&gt;, avgGlare &lt;dbl&gt;, avgVis &lt;dbl&gt;, … seg_id Cruise ship stratum use Mode EffType OnEffort ESWsides 1 1 2001 OES HI_EEZ FALSE NA-P NA-S FALSE-TRUE NA 2 6 2001 OES WHICEAS FALSE C-NA NA-F-N-S TRUE-FALSE NA 3 7 2001 OES WHICEAS FALSE C-NA S-NA-F-N FALSE-TRUE 2 4 8 2001 OES WHICEAS FALSE C-NA F-N-NA-S FALSE-TRUE 2 5 9 2001 OES WHICEAS FALSE C-NA-P S-NA-F FALSE-TRUE 2 6 10 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 7 11 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 8 12 2001 OES WHICEAS FALSE C-NA S-NA-N-F FALSE-TRUE 2 9 13 2001 OES WHICEAS FALSE C-NA S-N-NA FALSE-TRUE 2 10 14 2001 OES WHICEAS FALSE C-NA S-F-NA-N FALSE-TRUE 2 11 15 2001 OES WHICEAS FALSE C-NA N-NA FALSE 2 12 16 2001 OES WHICEAS FALSE NA-P-C NA-N-S FALSE-TRUE NA 13 17 2001 OES WHICEAS FALSE NA-P NA-N FALSE-TRUE NA 14 18 2001 OES WHICEAS FALSE C-NA S-NA FALSE-TRUE 2 15 19 2001 OES WHICEAS FALSE NA-C NA-N-S FALSE-TRUE NA 16 20 2001 OES WHICEAS FALSE C-NA S-NA FALSE 2 17 22 2001 OES WHICEAS FALSE NA-C NA-S FALSE-TRUE NA 18 23 2001 OES WHICEAS FALSE NA-C NA-S-N FALSE-TRUE NA 19 25 2001 OES WHICEAS FALSE NA-C NA-S FALSE-TRUE NA 20 26 2001 OES WHICEAS FALSE NA-C NA-S FALSE-TRUE NA 21 27 2001 OES WHICEAS FALSE NA-C NA-S-F-N FALSE-TRUE NA 22 28 2001 OES WHICEAS FALSE NA-C NA-S FALSE-TRUE NA 23 29 2001 OES WHICEAS FALSE NA-C NA-S-N FALSE-TRUE NA 24 30 2001 OES WHICEAS FALSE C-P-NA N-NA FALSE-TRUE 2 25 31 2001 OES WHICEAS FALSE C-NA N-NA-S FALSE-TRUE 2 26 33 2001 OES WHICEAS TRUE C F TRUE 2 27 39 2001 OES WHICEAS TRUE C N TRUE 2 28 40 2001 OES WHICEAS TRUE C-P N TRUE 2 29 41 2001 OES WHICEAS TRUE C N TRUE 2 30 43 2001 OES WHICEAS TRUE C N TRUE 2 31 45 2001 OES WHICEAS TRUE C-P N TRUE 2 32 47 2001 OES WHICEAS TRUE C N TRUE 2 33 48 2001 OES WHICEAS TRUE C S TRUE 2 34 49 2001 OES WHICEAS TRUE C S TRUE 2 35 51 2001 OES WHICEAS TRUE C S TRUE 2 36 52 2001 OES WHICEAS TRUE C S TRUE 2 37 54 2001 OES WHICEAS TRUE C S TRUE 2 38 55 2001 OES WHICEAS TRUE C S TRUE 2 39 57 2001 OES WHICEAS TRUE C S TRUE 2 40 59 2001 OES WHICEAS TRUE C-P S TRUE 2 41 60 2001 OES WHICEAS TRUE C S TRUE 2 42 62 2001 OES WHICEAS TRUE C S TRUE 2 43 63 2001 OES WHICEAS TRUE C S TRUE 2 44 65 2001 OES WHICEAS TRUE C S TRUE 2 45 66 2001 OES WHICEAS TRUE C S TRUE 2 46 69 2001 OES WHICEAS TRUE C S TRUE 2 47 70 2001 OES WHICEAS TRUE C S TRUE 2 48 71 2001 OES WHICEAS TRUE C S TRUE 2 49 72 2001 OES WHICEAS TRUE C S TRUE 2 50 74 2001 OES WHICEAS TRUE C S TRUE 2 51 76 2001 OES WHICEAS TRUE C S TRUE 2 52 78 2001 OES WHICEAS TRUE C S TRUE 2 53 80 2001 OES WHICEAS TRUE C S TRUE 2 54 81 2001 OES WHICEAS TRUE C S TRUE 2 55 82 2001 OES WHICEAS TRUE C S TRUE 2 56 84 2001 OES WHICEAS TRUE C S TRUE 2 57 85 2001 OES WHICEAS TRUE C S TRUE 2 58 87 2001 OES WHICEAS TRUE C S TRUE 2 59 88 2001 OES WHICEAS TRUE C S TRUE 2 60 90 2001 OES WHICEAS TRUE C S TRUE 2 61 92 2001 OES WHICEAS TRUE C S TRUE 2 62 93 2001 OES WHICEAS TRUE C S TRUE 2 dist minutes n_rows min_line max_line year month day lat1 1 0.00000 NA 15 1080 1549 2020 1 21 22.33300 2 99.91877 NA 405 11 975 2020 1 19 21.85200 3 99.62116 4289.567 363 9523 11034 2020 2 7 19.73233 4 100.45678 1850.367 325 11035 11819 2020 2 10 19.72267 5 99.47289 4139.367 438 976 2373 2020 1 20 21.87317 6 100.51920 2855.900 395 2374 3345 2020 1 23 22.66483 7 99.63959 4548.117 362 3346 4989 2020 1 25 23.27300 8 99.86654 3979.150 476 4990 6229 2020 1 28 20.83917 9 100.19596 4491.317 420 6230 7816 2020 1 31 20.46583 10 98.72089 3166.283 284 7817 8950 2020 2 3 20.34800 11 77.46174 1044.450 138 8951 9088 2020 2 5 19.47583 12 100.28697 1469.517 261 9089 9522 2020 2 6 19.69033 13 99.54315 11100.800 291 11820 12188 2020 2 17 21.47567 14 99.79480 4260.683 264 18225 19645 2020 3 3 18.33950 15 100.50612 1628.583 222 19646 20270 2020 3 6 18.56800 16 99.48314 1097.050 207 20271 20477 2020 3 7 20.24800 17 99.94458 1280.983 194 20532 20890 2020 3 8 21.32333 18 100.29250 3577.983 385 20891 22486 2020 3 9 21.21333 19 100.33608 6621.400 395 12394 13875 2020 2 19 21.91733 20 99.99051 1740.650 350 13876 14561 2020 2 24 21.96067 21 99.48936 2706.250 463 14562 15632 2020 2 25 21.56367 22 100.47495 1459.167 271 15633 16096 2020 2 27 18.72617 23 100.09871 1362.933 248 16097 16581 2020 2 28 19.17133 24 100.00432 1690.583 326 16582 17349 2020 2 29 19.20117 25 99.64886 2843.800 336 17350 18224 2020 3 1 18.86233 26 100.47710 3146.833 358 154 1411 2020 1 19 22.15500 27 30.41913 17538.917 93 5804 6479 2020 1 30 20.87867 28 23.22903 8630.017 70 8927 9263 2020 2 5 19.52183 29 30.57750 6934.483 108 10874 11483 2020 2 10 19.40450 30 99.78729 16955.750 358 15198 16743 2020 2 26 19.81117 31 100.04330 2825.467 330 16807 17722 2020 2 29 19.40967 32 32.50858 7087.367 86 21382 22378 2020 3 9 20.79533 33 99.59164 NA 211 876 1139 2020 1 20 21.75400 34 100.18874 1195.300 289 4231 4593 2020 1 27 21.98250 35 100.12115 1356.083 237 5013 5496 2020 1 28 20.81400 36 100.21993 2486.517 272 5497 6161 2020 1 29 20.00500 37 100.41031 1303.733 245 6395 6787 2020 1 31 20.63717 38 99.77602 1224.200 264 6788 7104 2020 2 1 19.74983 39 100.41197 2496.067 268 7386 8055 2020 2 2 19.48233 40 100.33778 1453.067 333 1140 1700 2020 1 21 22.27383 41 99.95286 1118.900 226 8313 8561 2020 2 4 20.11067 42 100.15064 2814.467 275 8803 9623 2020 2 5 19.33800 43 65.89289 1036.067 167 9624 9817 2020 2 7 19.66050 44 100.01454 1273.717 258 10161 10549 2020 2 8 21.09550 45 99.92863 2598.650 252 10550 11258 2020 2 9 20.76300 46 99.82449 1357.833 223 2028 2438 2020 1 22 22.38500 47 100.08784 1468.150 285 2439 2977 2020 1 23 22.79333 48 100.16879 1327.683 252 2978 3375 2020 1 24 23.16417 49 99.93565 1219.067 237 3376 3658 2020 1 25 23.24400 50 99.96439 1188.033 269 3937 4230 2020 1 26 22.17567 51 100.39218 1198.033 232 12619 12944 2020 2 21 22.81750 52 24.74341 861.200 69 13295 13374 2020 2 22 22.58550 53 99.65584 1362.100 263 13801 14261 2020 2 23 21.76567 54 100.20570 4010.850 257 14262 15687 2020 2 24 22.26667 55 100.26581 1811.750 293 15688 16427 2020 2 27 18.77083 56 100.07364 1214.700 265 21267 21615 2020 3 9 20.88667 57 99.77226 1155.383 252 21616 21950 2020 3 10 21.56100 58 94.98663 1323.717 286 17865 18280 2020 3 2 18.87900 59 100.05430 1197.017 331 18281 18643 2020 3 3 18.29917 60 100.07779 1108.883 213 18874 19120 2020 3 4 18.07967 61 100.27928 2510.483 208 19373 20084 2020 3 5 17.97517 62 100.03120 2922.567 239 20085 21037 2020 3 7 20.05817 lon1 DateTime1 timestamp1 yday1 lat2 lon2 1 -161.2520 2020-01-21 07:28:48 1579591728 21 22.68750 -161.1208 2 -159.7727 2020-01-19 07:31:11 1579419071 19 21.87200 -160.8287 3 -154.0735 2020-02-07 11:46:25 1581075985 38 19.72050 -154.8105 4 -154.8148 2020-02-10 11:15:59 1581333359 41 21.41150 -157.1700 5 -160.8292 2020-01-20 15:32:19 1579534339 20 22.66250 -159.7163 6 -159.7220 2020-01-23 12:31:41 1579782701 23 23.27450 -158.6780 7 -158.6727 2020-01-25 12:07:35 1579954055 25 20.83583 -158.4482 8 -158.4432 2020-01-28 15:55:42 1580226942 28 20.46433 -158.6277 9 -158.6332 2020-01-31 10:14:42 1580465682 31 20.34917 -156.6358 10 -156.6317 2020-02-03 13:06:10 1580735170 34 19.47600 -154.7962 11 -154.7963 2020-02-05 17:50:32 1580925032 36 19.68550 -154.5397 12 -154.5363 2020-02-06 11:16:54 1580987814 37 19.73167 -154.0748 13 -158.3557 2020-02-17 13:35:46 1581946546 48 21.70483 -157.2177 14 -155.2558 2020-03-03 11:39:56 1583235596 63 18.56650 -157.6283 15 -157.6342 2020-03-06 10:40:37 1583491237 66 20.24683 -159.5543 16 -159.5595 2020-03-07 13:49:12 1583588952 67 21.25000 -160.1823 17 -160.4623 2020-03-08 09:38:15 1583660295 68 21.21333 -161.5942 18 -161.5942 2020-03-09 06:57:14 1583737034 69 21.16850 -158.2605 19 -158.0488 2020-02-19 16:35:09 1582130109 50 21.95917 -154.9905 20 -154.9962 2020-02-24 06:56:33 1582527393 55 21.56417 -156.6513 21 -156.6485 2020-02-25 11:57:12 1582631832 56 18.72333 -156.6857 22 -156.6907 2020-02-27 09:03:27 1582794207 58 19.17233 -157.0005 23 -157.0015 2020-02-28 09:21:12 1582881672 59 19.19833 -156.1737 24 -156.1763 2020-02-29 08:05:00 1582963500 60 18.86350 -155.7190 25 -155.7145 2020-03-01 12:16:08 1583064968 61 18.34050 -155.2600 26 -159.8438 2020-01-19 09:49:58 1579427398 19 22.03983 -160.1947 27 -157.0888 2020-01-30 11:28:54 1580383734 30 20.74350 -159.3752 28 -154.7868 2020-02-05 17:26:59 1580923619 36 20.32900 -154.1045 29 -154.8637 2020-02-10 07:45:59 1581320759 41 21.12850 -156.6312 30 -156.1415 2020-02-26 11:25:48 1582716348 57 19.41917 -156.2707 31 -156.1407 2020-02-29 12:35:00 1582979700 60 18.72400 -155.2490 32 -159.9180 2020-03-09 18:01:52 1583776912 69 21.25333 -158.5208 33 -160.5175 2020-01-20 13:21:01 1579526461 20 22.27533 -161.0252 34 -158.5092 2020-01-27 11:30:24 1580124624 27 21.05833 -159.4150 35 -158.3635 2020-01-28 16:29:42 1580228982 28 20.00517 -156.8398 36 -156.8395 2020-01-29 15:04:18 1580310258 29 20.38817 -158.3330 37 -159.2742 2020-01-31 14:08:42 1580479722 31 19.75117 -159.2102 38 -159.2048 2020-02-01 11:52:26 1580557946 32 19.70800 -157.4600 39 -156.5632 2020-02-02 14:36:38 1580654198 33 20.38800 -154.9230 40 -161.0195 2020-01-21 08:54:48 1579596888 21 22.63017 -160.8893 41 -154.0280 2020-02-04 14:15:03 1580825703 35 19.14500 -153.5140 42 -154.4548 2020-02-05 14:47:57 1580914077 36 19.66150 -153.8213 43 -153.8163 2020-02-07 13:42:25 1581082945 38 20.79433 -153.6182 44 -154.8023 2020-02-08 14:41:34 1581172894 39 20.76417 -154.9823 45 -154.9768 2020-02-09 11:55:42 1581249342 40 21.06500 -156.1200 46 -159.9168 2020-01-22 15:43:55 1579707835 22 22.79133 -160.0227 47 -160.0282 2020-01-23 14:21:41 1579789301 23 23.16550 -159.6905 48 -159.6855 2020-01-24 14:49:54 1579877394 24 23.24517 -158.5435 49 -158.5380 2020-01-25 12:57:35 1579957055 25 22.44167 -158.3548 50 -157.3620 2020-01-26 15:42:22 1580053342 26 21.98100 -158.5037 51 -156.8680 2020-02-21 12:31:47 1582288307 52 22.83583 -158.3788 52 -157.3837 2020-02-22 16:51:15 1582390275 53 22.05400 -156.8443 53 -155.7195 2020-02-23 16:52:30 1582476750 54 22.26567 -156.1680 54 -156.1738 2020-02-24 15:34:36 1582558476 55 18.76967 -156.9142 55 -156.9198 2020-02-27 10:25:27 1582799127 58 19.01667 -156.1702 56 -160.2858 2020-03-09 15:31:08 1583767868 69 21.56300 -159.6428 57 -159.6378 2020-03-10 11:45:50 1583840750 70 21.82317 -159.2223 58 -155.6678 2020-03-02 14:48:28 1583160508 62 18.30033 -155.0988 59 -155.0950 2020-03-03 12:51:56 1583239916 63 17.85017 -154.8307 60 -155.7152 2020-03-04 14:17:12 1583331432 64 17.74417 -155.9767 61 -156.9033 2020-03-05 15:16:05 1583421365 65 20.05767 -158.8173 62 -158.8190 2020-03-07 09:05:12 1583571912 67 21.11550 -161.1690 DateTime2 timestamp2 yday2 mlat mlon mDateTime 1 2020-01-22 07:42:01 1579678921 22 22.69050 -161.1327 2020-01-22 07:33:52 2 2020-01-20 15:30:19 1579534219 20 21.57383 -160.5790 2020-01-20 08:04:18 3 2020-02-10 11:13:59 1581333239 41 21.22617 -155.2492 2020-02-08 17:55:34 4 2020-02-11 18:04:21 1581444261 42 21.18783 -156.4072 2020-02-11 09:14:21 5 2020-01-23 12:29:41 1579782581 23 22.59867 -160.7162 2020-01-22 10:07:52 6 2020-01-25 12:05:35 1579953935 25 23.29883 -160.1023 2020-01-24 10:55:58 7 2020-01-28 15:53:42 1580226822 28 22.23800 -159.3092 2020-01-27 17:20:24 8 2020-01-31 10:12:51 1580465571 31 19.99317 -156.7923 2020-01-29 15:22:19 9 2020-02-03 13:04:10 1580735050 34 19.62217 -157.1203 2020-02-02 11:06:13 10 2020-02-05 17:50:27 1580925027 36 20.21200 -156.0975 2020-02-03 16:56:10 11 2020-02-06 11:14:54 1580987694 37 19.35817 -154.7625 2020-02-06 08:58:54 12 2020-02-07 11:44:25 1581075865 38 20.33267 -154.1053 2020-02-06 15:52:54 13 2020-02-19 11:05:09 1582110309 50 22.04250 -158.2913 2020-02-17 18:13:43 14 2020-03-06 10:38:37 1583491117 66 18.42300 -157.0583 2020-03-06 07:04:37 15 2020-03-07 13:47:12 1583588832 67 19.96317 -158.4395 2020-03-07 06:47:12 16 2020-03-08 08:04:15 1583654655 68 20.35783 -159.9943 2020-03-07 16:31:12 17 2020-03-09 06:57:14 1583737034 69 21.43617 -160.9142 2020-03-08 12:18:15 18 2020-03-11 18:35:13 1583951713 71 21.82383 -159.2247 2020-03-11 06:58:19 19 2020-02-24 06:54:33 1582527273 55 22.53633 -157.1957 2020-02-22 18:10:38 20 2020-02-25 11:55:12 1582631712 56 21.66850 -157.0722 2020-02-25 07:08:33 21 2020-02-27 09:01:27 1582794087 58 19.91500 -156.0813 2020-02-26 09:50:48 22 2020-02-28 09:20:37 1582881637 59 18.93817 -157.5803 2020-02-27 16:01:27 23 2020-02-29 08:03:33 1582963413 60 19.13750 -156.3623 2020-02-28 14:39:12 24 2020-03-01 12:14:08 1583064848 61 19.70700 -156.3120 2020-02-29 18:23:00 25 2020-03-03 11:37:56 1583235476 63 18.65917 -155.2378 2020-03-02 10:58:28 26 2020-01-21 14:14:48 1579616088 21 21.80333 -160.0330 2020-01-19 17:10:48 27 2020-01-31 16:00:53 1580486453 31 20.57667 -156.8462 2020-01-30 17:46:23 28 2020-02-06 15:50:54 1581004254 37 20.19217 -154.1955 2020-02-06 14:46:44 29 2020-02-11 11:25:23 1581420323 42 19.88600 -155.0652 2020-02-10 13:27:59 30 2020-02-29 11:19:28 1582975168 60 19.04500 -156.0290 2020-02-28 17:53:12 31 2020-03-02 11:38:28 1583149108 62 19.72267 -156.3125 2020-02-29 18:17:00 32 2020-03-11 16:33:59 1583944439 71 21.42967 -158.4652 2020-03-11 14:51:13 33 2020-01-21 08:52:48 1579596768 21 21.90333 -160.9970 2020-01-20 16:40:23 34 2020-01-28 07:23:42 1580196222 28 22.16217 -159.0873 2020-01-27 15:06:24 35 2020-01-29 15:04:11 1580310251 29 20.16533 -157.4640 2020-01-29 09:48:18 36 2020-01-31 08:30:42 1580459442 31 20.97050 -157.4058 2020-01-30 08:43:41 37 2020-02-01 11:50:26 1580557826 32 19.86550 -159.6578 2020-02-01 08:04:26 38 2020-02-02 08:14:38 1580631278 33 19.61050 -158.6485 2020-02-01 15:46:26 39 2020-02-04 08:10:42 1580803842 35 20.49433 -157.2042 2020-02-03 07:30:05 40 2020-01-22 09:05:52 1579683952 22 22.07067 -160.3225 2020-01-21 13:22:48 41 2020-02-05 08:51:57 1580892717 36 20.00533 -153.5913 2020-02-04 17:15:03 42 2020-02-07 13:40:25 1581082825 38 19.83367 -154.4798 2020-02-07 08:02:25 43 2020-02-08 06:56:29 1581144989 39 19.58017 -153.4778 2020-02-07 16:00:39 44 2020-02-09 11:53:42 1581249222 40 20.86767 -155.4500 2020-02-09 07:15:41 45 2020-02-11 07:12:21 1581405141 42 20.63567 -154.4630 2020-02-09 15:29:42 46 2020-01-23 14:19:45 1579789185 23 22.63133 -159.3223 2020-01-23 08:21:41 47 2020-01-24 14:47:54 1579877274 24 23.30067 -160.2283 2020-01-24 08:00:40 48 2020-01-25 12:55:35 1579956935 25 23.41117 -159.2327 2020-01-25 08:40:17 49 2020-01-26 09:14:39 1580030079 26 23.13117 -158.0577 2020-01-25 16:00:26 50 2020-01-27 11:28:24 1580124504 27 21.92233 -157.9422 2020-01-27 08:14:24 51 2020-02-22 08:29:15 1582360155 53 22.94933 -157.3898 2020-02-21 16:47:47 52 2020-02-23 07:10:27 1582441827 54 22.55700 -157.2845 2020-02-22 17:33:15 53 2020-02-24 15:32:36 1582558356 55 22.08633 -155.4900 2020-02-24 09:58:33 54 2020-02-27 10:23:27 1582799007 58 21.52683 -156.5017 2020-02-25 13:19:12 55 2020-02-28 16:35:12 1582907712 59 18.89100 -157.3922 2020-02-27 13:23:27 56 2020-03-10 11:43:50 1583840630 70 21.64983 -160.1223 2020-03-10 08:40:25 57 2020-03-11 06:59:13 1583909953 71 21.40467 -159.1643 2020-03-10 15:03:50 58 2020-03-03 12:50:11 1583239811 63 18.42883 -155.6037 2020-03-03 09:04:59 59 2020-03-04 08:47:12 1583311632 64 18.18233 -154.6457 2020-03-03 16:19:56 60 2020-03-05 08:44:05 1583397845 65 18.20600 -156.1960 2020-03-04 17:05:12 61 2020-03-07 09:04:34 1583571874 67 18.08650 -157.3400 2020-03-05 17:46:05 62 2020-03-09 09:47:08 1583747228 69 21.50950 -161.1948 2020-03-08 13:56:15 mtimestamp avgBft avgSwellHght avgHorizSun avgVertSun avgGlare avgVis 1 1579678432 NaN NaN NaN NaN NaN NaN 2 1579507458 5.128083 7.063235 4.467992 1.682369 0.344517876 5.967136 3 1581184534 4.710589 8.112782 7.546051 1.568233 0.262374221 4.994149 4 1581412461 5.651883 8.704314 8.611582 1.263381 0.000000000 4.880111 5 1579687672 2.913489 6.293346 5.276111 1.511402 0.261874672 5.898653 6 1579863358 4.435180 8.240447 7.710615 1.913823 0.402052802 5.534311 7 1580145624 3.909132 6.485278 6.099406 1.713658 0.263751103 5.455732 8 1580311339 3.478643 3.806445 4.825466 1.824426 0.419240667 5.680916 9 1580641573 5.313369 6.805166 4.291268 1.631312 0.252835438 4.746676 10 1580748970 6.321631 7.387893 3.711664 1.698973 0.002814260 4.134020 11 1580979534 2.267559 3.267559 NaN NaN NaN 4.866220 12 1581004374 3.257916 4.567725 1.000000 2.000000 1.000000000 4.817161 13 1581963223 6.670442 6.207687 5.666958 2.199876 0.199876411 4.474237 14 1583478277 6.438988 6.769782 5.805191 2.071638 0.000000000 4.996988 15 1583563632 6.927898 7.446039 7.775695 1.000000 0.000000000 5.000000 16 1583598672 7.000000 7.781233 10.155382 1.155382 0.155382336 4.705996 17 1583669895 6.997715 7.000000 8.170470 1.066167 0.000000000 5.000000 18 1583909899 2.941494 4.157675 4.529329 1.541471 0.237820380 6.024604 19 1582395038 5.513195 6.482226 6.248737 1.377216 0.214242704 5.156535 20 1582614513 5.364939 5.724447 7.531359 1.574425 0.382497255 5.374254 21 1582710648 5.810685 6.255111 6.096551 2.061110 0.081455686 4.714421 22 1582819287 6.808077 9.432417 8.335450 1.916293 0.415347692 4.593235 23 1582900752 4.994516 7.785228 4.338661 1.661617 0.000000000 5.181479 24 1583000580 3.798583 3.835136 6.514971 1.590703 0.209360315 6.141686 25 1583146708 5.755097 7.557364 6.049363 1.560368 0.133177633 5.138967 26 1579453848 5.477926 7.203073 7.091664 1.428277 0.204144664 5.887604 27 1580406383 3.449423 3.424377 9.873088 1.476085 0.025320055 5.758327 28 1581000404 4.661152 5.392446 NaN NaN NaN 4.661152 29 1581341279 4.984964 6.609969 6.354441 1.629767 0.000000000 4.945342 30 1582912392 3.062833 3.646181 6.336261 1.283594 0.246636211 6.150588 31 1583000220 3.848871 4.940123 7.633274 1.679792 0.282113660 5.959173 32 1583938273 2.634420 3.275438 2.217477 1.338525 0.000000000 6.437218 33 1579538423 4.886062 7.000000 10.084827 1.853057 0.489668211 5.947775 34 1580137584 4.642617 5.850631 9.097312 1.477874 0.141743994 5.510219 35 1580291298 3.182694 4.606170 6.844428 1.985086 0.422467226 5.734008 36 1580373821 3.611551 4.077656 5.983574 1.984656 0.189190296 5.609887 37 1580544266 3.989753 6.367106 9.368393 2.084873 0.830262067 5.055214 38 1580571986 4.694184 9.434106 4.461701 1.693785 0.126339686 5.114734 39 1580715005 4.212352 4.988016 6.755251 2.187387 0.421468157 5.129102 40 1579612968 3.773264 5.545910 4.619530 1.477611 0.477611450 6.103830 41 1580836503 4.000000 6.399287 4.702839 2.246963 0.000000000 5.064723 42 1581062545 3.882397 5.762492 8.732053 1.709282 0.363972171 5.889121 43 1581091239 3.152021 6.834814 5.000000 3.000000 0.000000000 5.076102 44 1581232541 4.819183 8.170048 6.567596 1.855079 0.318533492 5.055913 45 1581262182 4.525836 7.086960 3.188279 1.556084 0.000000000 5.181024 46 1579767701 3.043304 8.727071 5.149226 2.578913 0.000000000 5.952430 47 1579852840 5.000000 7.407662 6.809698 1.701839 0.167492006 5.649273 48 1579941617 4.882279 8.236228 4.796593 1.838151 0.451259503 4.864896 49 1579968026 4.895222 9.708907 4.833667 1.642630 0.170370192 5.026551 50 1580112864 4.661570 6.422097 5.616595 2.275634 0.000000000 5.711704 51 1582303667 5.309142 6.225222 10.503905 1.749561 0.420841258 4.827602 52 1582392795 4.419995 6.863498 5.646419 2.155154 0.136502170 5.136502 53 1582538313 4.492900 5.308411 8.190381 1.286301 0.078544037 5.587378 54 1582636752 4.690722 5.930044 7.789109 1.834519 0.490520372 5.565201 55 1582809807 5.150536 7.649064 5.555537 1.045965 0.000000000 5.163195 56 1583829625 4.924334 4.857467 4.584329 1.414874 0.184908431 6.000259 57 1583852630 2.734279 4.922333 3.393594 1.017305 0.008652364 6.000000 58 1583226299 5.537977 8.963847 8.924995 1.415656 0.597126543 5.204848 59 1583252396 6.000000 9.937443 4.576505 1.767917 0.000000000 4.057354 60 1583341512 5.338520 6.422540 9.573000 1.876897 0.410951830 4.960688 61 1583430365 5.097312 5.983217 9.738967 1.974344 0.565257371 5.443199 62 1583675775 5.561021 7.410153 8.291528 1.540274 0.384855106 5.229441 avgCourse avgSpdKt 1 NaN NaN 2 194.55192 7.709562 3 203.84112 8.446307 4 299.02269 8.208691 5 180.53558 7.634308 6 155.06208 7.184731 7 145.97905 9.521554 8 101.79133 7.513526 9 125.94909 8.155691 10 132.46935 8.525247 11 153.36457 4.681607 12 139.81125 7.196281 13 194.85132 9.557688 14 274.55391 9.662940 15 284.45524 9.624089 16 286.28803 9.624374 17 285.18629 9.888097 18 113.73998 9.596219 19 148.11257 8.599527 20 200.29541 7.500685 21 228.06092 8.710984 22 236.00468 9.066602 23 152.87788 8.505417 24 172.99904 9.115407 25 141.67293 8.040222 26 244.18631 9.538651 27 196.90959 10.100242 28 63.34059 9.370393 29 149.24496 9.374871 30 153.24485 9.616660 31 147.26786 9.306176 32 176.41172 9.793078 33 242.72350 9.900679 34 285.58399 9.661279 35 109.68594 9.421157 36 142.34723 9.430886 37 172.35968 9.097397 38 104.26843 8.519835 39 102.62332 9.327991 40 103.17709 9.987417 41 177.88442 9.713084 42 178.88062 9.451000 43 107.06494 8.682113 44 192.61294 9.249831 45 114.09609 9.673976 46 229.43209 9.878247 47 195.55715 9.527608 48 103.92613 9.123261 49 104.86979 9.089860 50 209.23402 9.663045 51 249.95890 9.876131 52 105.67999 8.326220 53 264.45975 9.475117 54 220.45518 9.058134 55 216.06000 9.188690 56 102.65765 8.779436 57 105.81961 9.561930 58 180.45547 8.739032 59 152.38277 8.119670 60 283.92925 10.093604 61 285.28456 10.085211 62 217.03890 9.396838 Contiguous vs. non-contiguous effort The default example at the top allows for non-contiguous effort; a segment is allowed to contain effort separated by gaps as large as 24 hours (settings$segment_max_interval). To coerce segments to represent only contiguous effort, make that setting very small: cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 30, segment_max_interval = .1, verbose=FALSE, to_plot = TRUE) You can see that many contiguous periods of effort were much shorter than the target length of 30 km. This is why allowing for non-contiguous effort can be advantageous for target segment lengths larger than 5 - 10 km. Segment remainder handling In the top example, the setting for segment_remainder_handling, c('append','segment'), means that remainders less than half the target length will be randomly appended to another segment, while remainders more than half will be treated as their own segment (and will be placed randomly along the trackline). If you don’t want that level of complexity, you can simply assign a single setting: 'append' will append the remainder in all cases, regardless of remainder length relative to the target length. The same idea goes for 'segment'. The other possible setting is 'disperse', which disperses the remainder evenly across all segments. To demonstrate, let’s use a target length of 10 km. cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 10, segment_max_interval = 6, segment_remainder_handling = &#39;disperse&#39;, verbose=FALSE, to_plot = TRUE) Note that most segments are longer than the target length, due to the impact of dispersing the remainder. If you wanted, you could combat this by making the target length slightly smaller: cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 9.5, segment_max_interval = 6, segment_remainder_handling = &#39;disperse&#39;, verbose=FALSE, to_plot = TRUE) But in general, the disperse option may be more appropriate for shorter segment lengths. Typical settings Design-based line transect analysis To replicate methods used in density estimation analyses, use large segment lengths (100 km or more) or simply segmentize by day. (See the examples above.) Remember that long segment lengths won’t work well unless you allow for non-contiguous effort. Habitat modeling To replicate the methods used in typical habitat modeling studies, use smaller segment lengths of contiguous effort. cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 5, segment_max_interval = .1, verbose=FALSE, to_plot = TRUE) "],["abund9_compare.html", " 18 LTabundR vs. ABUND9 Differences in total effort Differences in on-effort distance Differences in total sightings Differences in on-effort sightings Differences in school size estimation", " 18 LTabundR vs. ABUND9 We have tried to develop LTabundR with the flexibility either to replicate ABUND results or to produce customizable results that could potentially vary from ABUND quite significantly (e.g., formatted for habitat modeling). However, even when we use LTabundR settings intended to replicate ABUND results, there are likely to be some small differences. These are detailed below: Differences in total effort After loading the data, LTabundR removes rows with invalid Cruise numbers, invalid times, and invalid coordinates. As far as we can tell, ABUND does not remove such missing data. This is a relatively minor point; in processing the 1986-2020 data (623,640 rows), 287 rows are missing Cruise info; 1,430 are missing valid times; and 556 are missing valid coordinates, for a total of 2,273 rows removed out of more than 625,000 (0.3% of rows). Many of these rows with missing data have the same coordinates as complete rows nearby (since WinCruz can sometimes produce multiple lines at the same time when setting up metadata for the research day). In ABUND, custom functions are used to calculate whether DAS coordinates occur within geostrata are difficult to validate, and it is possible that they differ from the functions used in R for the same purpose. LTabundR uses functions within the well-established sf package to do these same calculations. Both ABUND and LTabundR calculate the distance surveyed based on the sum of distances between adjacent rows in the DAS file. They do this differently (see below), based on the way they loop through the data, which may yield minor differences in segment track lengths. ABUND loops through the data one row at a time, calculating distance traveled at the same time as allocating effort to segments and processing sightings. It calculates the distance between each new row and the beginning of a segment of effort. That beginning location (object BEGTIME in the Fortran code) is reset with various triggers (including a new date), and the distance traveled is calculated using a subroutine (DISTRAV). For surveys occurring after 1991, the distance between a new coordinate and the BEGTIME coordinate is calculated using a subroutine named GRCIRC (great-circle distance). Prior to 1991, the ship speed and the time since BEGTIME is used to estimate distance traveled. After 1991, the function calculates distance based on coordinates. For all years, the distance calculation only happens if the time gap in time is at least 1.2 minutes (line 405 in ABUND9.FOR), otherwise the distance is returned as 0 km. This function also seems to allow for large gaps between subsequent rows within a single day of effort. The subroutine prints a warning message when the gap is greater than 30 km, but does not modify its estimate of distance traveled. This allows for the possibility that, in rare cases, estimates of distance surveyed will be spuriously large. LTabundR processes data using a modular approach rather than a single large loop. Prior to the segmentizing stage, it calculates the distance between rows of data. Its approach is to calculate the distance between each row and its subsequent row (it does so using the swfscDAS function distance_greatcircle(), which is a nearly-exact recode of the ABUND subroutine GRCIRC for R. There are two important differences that LTabundR applies: (1) In anticipation of WinCruz surveys that operate on much smaller scales with more frequent position updates, we calculate distances for time gaps as small as 30 seconds, not 1.2 minutes. This may generate minor differences in the total length of tracks; (2) If the distance between rows is greater than 30 km, then it is assumed that effort has stopped and the distance is changed to 0 km (that distance can be modified by the user; see the LTabundR function load_survey_settings(). This approach should avoid the misinterpretation of large gaps in effort as large periods of effort. Differences in on-effort distance LTabundR works with DAS data that are loaded and formatted using swfscDAS:das_read() and das_process(). It is possible that these functions categorize events as On- or Off-Effort slightly differently than ABUND, or apply other differences that would be difficult for us to know or track. While ABUND uses a minimum length threshold to create segments, such that full-length segments are never less than that threshold and small remainder segments always occur at the end of a continuous period of effort, LTabundR uses an approach more similar to the effort-chopping functions in swfscDAS: it looks at continuous blocs of effort, determines how many full-length segments can be defined in each bloc, then randomly places the remainder within that bloc according to a set of user-defined settings (see load_survey_settings(). This process produces full-length segments whose distribution of exact lengths is centered about the target length, rather than always being greater than the target length. To control the particularities of segmentizing, LTabundR uses settings such as segment_max_interval, which controls how discontinuous effort is allowed to be pooled into the same segment. These rules may produce slight differences in segment lengths. Note that, since ABUND is a loop-based routine while LTabundR is modular, segments identified by the two program will never be exactly identical, and a 1:1 comparison of segments produced by the two programs is not possible. Differences in total sightings In ABUND9, only sightings that occur while OnEffort == TRUE are returned; in contrast, LTabundR does not remove any sightings (it just flags them differently, using the included column variable). But we can easily filter LTabundR sightings to emulate ABUND9 output. Differences in on-effort sightings LTabundR includes an additional criterion for inclusion in analysis: the sighting must occur at or forward of the beam (this can be deactivated in load_survey_settings(). Since geostratum handling is different in the two programs, it is possible that sightings occurring near stratum margins may be included/excluded differently. Differences in school size estimation If an observer is not included in the Group Size Calibration Coefficients .DAT file, ABUND applies a default coefficient (0.8625) to scale group size estimates; however, it applies this calibration to group sizes of all sizes, including solo animals or small groups of 2-3. In LTabundR, users can choose to restrict calibrations for unknown observers to group size estimates of any size (see load_cohort_settings()) Note that ABUND9 calibrates school sizes slightly differently than ABUND7. The ABUND9 release notes mention a bug in previous versions that incorrectly calibrated school size. LTabundR corresponds perfectly with ABUND9 school size calibrations, but not with ABUND8 or earlier. "],["spp_codes.html", " 19 NOAA/NMFS species codes Table species_translator()", " 19 NOAA/NMFS species codes Table LTabundR provides the standard table of NOAA/NMFS species codes as a built-in dataset: This version of the species codes table was provided by Amanda Bradford (Pacific Islands Fisheries Science Center) in 2021. For an easier way to find a species of interest, look into the species_translator() function on the Miscellaneous functions page. species_translator() To streamline the management of species codes, scientific names, common names, etc., in the functions throughout this package, we have developed a “translator” function that returns the various identifiers for a species according to a variety of search terms. You can search by species code: # source(&#39;R/species_translator.R`) species_translator(id = &#39;035&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common &quot;Long-finned pilot whale&quot; description &quot;Atlantic pilot whale; blackfish; pothead&quot; By the short code name: species_translator(id = &#39;LONG_PILOT&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common &quot;Long-finned pilot whale&quot; description &quot;Atlantic pilot whale; blackfish; pothead&quot; By the scientific name: species_translator(id = &#39;Globicephala melas&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common &quot;Long-finned pilot whale&quot; description &quot;Atlantic pilot whale; blackfish; pothead&quot; Or by one of the species’ common names: species_translator(id = &#39;Long-finned pilot whale&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common &quot;Long-finned pilot whale&quot; description &quot;Atlantic pilot whale; blackfish; pothead&quot; The function will return any species for which there is a partial match: species_translator(id = &#39;megap&#39;) %&gt;% t() 76 code &quot;076&quot; short_name &quot;HUMPBACK_W&quot; scientific_name &quot;Megaptera novaeangliae&quot; common &quot;Humpback whale&quot; description &quot;&quot; 70 code &quot;070&quot; short_name &quot;UNID_RORQL&quot; scientific_name &quot;Balaenopterid sp&quot; common &quot;Unidentified rorqual (Balaenoptera; Megaptera)&quot; description &quot;&quot; species_translator(id = &#39;killer&#39;) code short_name scientific_name common 32 032 PYGMY_KLLR Feresa attenuata Pygmy killer whale 33 033 FALSE_KLLR Pseudorca crassidens False killer whale 37 037 KILLER_WHA Orcinus orca Killer whale 110 110 Orcinus orca Transient killer whale 111 111 Orcinus orca Resident killer whale 112 112 Orcinus orca Offshore killer whale 113 113 Orcinus orca Type A Antarctic killer whale 114 114 Orcinus orca Type B Antarctic killer whale 115 115 Orcinus orca Type C Antarctic killer whale description 32 slender blackfish 33 37 110 111 112 113 114 115 Note that if species_codes is NULL, as in the examples above, the list of codes used in ABUND9 will be used as a default. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
