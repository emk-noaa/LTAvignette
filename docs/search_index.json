[["index.html", "User’s guide for LTabundR Overview", " User’s guide for LTabundR Overview The R package LTabundR offers tools that facilitate and standardize design-based line-transect abundance estimation of cetaceans, based on typical workflow used following NOAA Fisheries ship surveys in the central and eastern Pacific (e.g., Barlow 2006, Barlow and Forney 2007, Bradford et al. 2017, Bradford et al. 2021). That workflow typically involves four stages: (1) Data processing This step involves reading in &amp; processing raw DAS files (the files produced by the software Wincruz commonly used during NOAA Fisheries line-transect surveys in the Pacific), breaking effort into discrete segments for variance estimation, correcting group size estimates according to calibration models, and then averaging together group size estimates for each sighting. Most importantly, this step standardizes the data structure in a way that all downstream analyses depend upon. The name we will use for this standardized data object is a cruz object. The key LTabundR functions you will use in this stage are load_settings() and process_surveys(). (2) Data exploration This step involves summarizing effort and sightings totals, determining the appropriate truncation distances for each species – or pool of species – based on sample sizes, and producing maps. The key LTabundR functions in this stage are cruz_explorer() (a Shiny dashboard for data exploration) and the summarize... functions, such as summarize_species() and summarize_effort(). (3) Data analysis This step involves estimating Beaufort-specific “relative” trackline detection probabilities – i.e., g(0) estimates; estimating density/abundance with detection functions and determining the CV of that estimate; handling stratified analyses; and evaluating if random variation in the encounter rate of a species may be driving differences in abundance estimates over time. The key LTabundR functions in this stage are g0_table() and lta(). Most analyses are group-based analyses, but false killer whales (Pseudorca crassidens) are analyzed differently using a subgroup-based approach. For this exception, the function lta_subgroup() will be used. (4) Reports &amp; plots This step produces summary tables of the processed data and line-transect estimates; plots the best-fitting detection function model(s); and plots species-specific abundance estimates (and their CV). They key LTabundR functions in this stage are lta_report(), df_plot(), and lta_plot(). This user’s guide is structured around these four workflow stages. Those pages are followed by a set of case studies with full-fledged example code. The guide concludes with appendices that offer further details and minutiae on certain aspects of the package. Throughout this user’s guide, we will primarily be using example data from the winter Hawaiian Islands Cetacean Ecosystem and Assessment Survey (WHICEAS) of 2020, along with the summer-fall HICEAS 2017 data collected within the WHICEAS study area. Installation Note: BETA testing only. This package is currently in beta testing and is not yet ready for widespread use. The LTabundR package is available on GitHub here. To install directly within R, use the following code: # Install &#39;devtools&#39; package, if needed if (!require(&#39;devtools&#39;)) install.packages(&#39;devtools&#39;) # Increase timeout for download, since there are datasets options(timeout=9999999) # Install LTabundR remotely from GitHub devtools::install_github(&#39;PIFSC-Protected-Species-Division/LTabundR&#39;) # Load into session library(LTabundR) Note that this package contains large built-in datasets and may take several minutes to install. Credits This R package is a consolidation of code developed over many years by many NOAA Fisheries scientists, primarily Jay Barlow, Tim Gerrodette, Jeff Laake, Karin Forney, Amanda Bradford, and Jeff Moore. This package was developed by Eric Keen and Amanda Bradford with support from the NOAA Fisheries National Protected Species Toolbox Initiative. "],["settings.html", " 1 Settings Survey strata Survey-wide settings Cohort-specific settings Example code", " 1 Settings To customize the way data are processed and included in your analysis, use the load_settings() function. This function emulates and expands upon the settings file, ABUND.INP, that was used to run ABUND7/9 in FORTRAN. ABUND7/9 was a program written by Jay Barlow (NOAA Fisheries) to process DAS files. This function allows you to use ‘factory defaults’ if you don’t wish to specify anything special such as strata or study area polygons: settings &lt;- load_settings() If you do not want to use all the defaults, you can provide load_settings() with custom inputs. The function accepts three arguments: strata: dataframe(s) of coordinates survey: settings that will apply universally to the analysis cohorts: settings that are specific to groups of species. By providing cohort-specific settings, the code for a single analysis becomes simpler and more easily reproduced, since the code only needs to be run once without modification. The output of load_settings() is a named list with a slot for each of these arguments: settings %&gt;% names [1] &quot;strata&quot; &quot;survey&quot; &quot;cohorts&quot; Survey strata Stratum polygons can be provided as a named list of data.frame objects. Each data.frame must have Lon and Lat as the first two columns, providing coordinates in decimal degrees in which West and South coordinates are negative. Other columns are allowed, but the first two need to be Lon and Lat. The name of the slot holding the data.frame will be used as a reference name for the stratum. Note that if coordinates in your data span the International Date Line (IDL) such that some longitudes are positive and some are negative, during data processing all longitudes will be coerced to negative degrees West. The same transformation will be applied to any survey strata you provide that span the IDL. If strata is NULL, abundance will not be estimated; only density within the searched area (i.e., the total segment length x effective strip width). While users are welcome to upload polygons of their own, the package comes with built-in polygons for strata that are commonly used in three NOAA Fisheries study regions: the central and eastern Pacific: the Central North Pacific (CNP, including Hawaii) … data(strata_cnp) names(strata_cnp) [1] &quot;HI_EEZ&quot; &quot;OtherCNP&quot; &quot;MHI&quot; &quot;WHICEAS&quot; [5] &quot;Spotted_OU&quot; &quot;Spotted_FI&quot; &quot;Spotted_BI&quot; &quot;Bottlenose_KaNi&quot; [9] &quot;Bottlenose_OUFI&quot; &quot;Bottlenose_BI&quot; &quot;NWHI&quot; # HI-EEZ: U.S. Exclusive Economic Zone around the Hawaiian Islands. # OtherCNP: Extended Central North Pacific study area. # MHI: Main Hawaiian Islands survey stratum during HICEAS 2002. # WHICEAS: Study area for winter HICEAS of 2020. # Spotted_OU: Stock boundary for the Oahu population of spotted dolphins. # Spotted_FI: Stock boundary for the 4-Islands population of spotted dolphins. # Spotted_BI: Stock boundary for the Hawaii Island population of spotted dolphins. # Bottlenose_KaNi: Stock boundary for the Kauai/Niihau population of bottlenose dolphins. # Bottlenose_OUFI: Collective stock boundaries for the adjacent Oahu and 4-Islands populations of bottlenose dolphins. # Bottlenose_BI: Stock boundary for the Hawaii Island population of bottlenose dolphins. # NWHI: Stock boundary for the Northwestern Hawaiian Islands population of false killer whales. …the California Current System (CCS) … data(strata_ccs) names(strata_ccs) [1] &quot;CCS&quot; &quot;Southern_CA&quot; &quot;Central_CA&quot; &quot;Nothern_CA&quot; &quot;OR_WA&quot; #For more information on these strata, see Barlow (2010). … and the Eastern Tropical Pacific (ETP): data(strata_etp) names(strata_etp) [1] &quot;MOPS_AreaCoreM&quot; &quot;MOPS_AreaIn&quot; &quot;MOPS_AreaIn1&quot; [4] &quot;MOPS_AreaIn2&quot; &quot;MOPS_AREAINS&quot; &quot;MOPS_AREAMID&quot; [7] &quot;MOPS_AreaMid1&quot; &quot;MOPS_AreaMid2&quot; &quot;MOPS_AreaMOPS&quot; [10] &quot;MOPS_AREANORS&quot; &quot;MOPS_AreaOuterM&quot; &quot;MOPS_AreaSou&quot; [13] &quot;MOPS_AREASOUS&quot; &quot;MOPS_AreaSpin&quot; &quot;MOPS_AreaSpinS&quot; [16] &quot;MOPS_AREAWES&quot; &quot;PODS_93STRAT1&quot; &quot;PODS_93STRAT2&quot; [19] &quot;PODS_Area92&quot; &quot;PODS_AREA92RS&quot; &quot;PODS_Area92s&quot; [22] &quot;PODS_AREA93&quot; &quot;PODS_AREA93A&quot; &quot;PODS_AREA93AR&quot; [25] &quot;PODS_AREA93AS&quot; &quot;PODS_AREA93BR&quot; &quot;PODS_AREA93M&quot; [28] &quot;PODS_AREA93MS&quot; &quot;PODS_AREA93R&quot; &quot;PODS_AREA93R1&quot; [31] &quot;PODS_AREA93R2&quot; &quot;PODS_AREA93RS&quot; &quot;PODS_AREA93S&quot; [34] &quot;PODS_AREANCOR&quot; &quot;PODS_GOCpoly&quot; &quot;Pre1986_Area79ES1&quot; [37] &quot;Pre1986_Area79ES1s&quot; &quot;Pre1986_Area79ES2&quot; &quot;Pre1986_Area79ES2s&quot; [40] &quot;Pre1986_Area79NE1&quot; &quot;Pre1986_Area79NE1s&quot; &quot;Pre1986_Area79NE2&quot; [43] &quot;Pre1986_Area79NE2s&quot; &quot;Pre1986_Area79NE3&quot; &quot;Pre1986_Area79NE3s&quot; [46] &quot;Pre1986_AreaCal&quot; &quot;Pre1986_AreaCals&quot; &quot;Pre1986_AreaMid&quot; [49] &quot;Pre1986_AreaMidS&quot; &quot;Pre1986_AreaNorth&quot; &quot;Pre1986_AreaNorthS&quot; [52] &quot;Pre1986_AreaSouth&quot; &quot;Pre1986_AreaSouthS&quot; &quot;STAR_Area98a&quot; [55] &quot;STAR_Area98b&quot; &quot;STAR_AreaCore&quot; &quot;STAR_AreaCore2&quot; [58] &quot;STAR_AreaCoreS&quot; &quot;STAR_AreaNCoast&quot; &quot;STAR_AreaNCstS&quot; [61] &quot;STAR_AreaOuter&quot; &quot;STAR_AreaOuter00&quot; &quot;STAR_AreaSCoast&quot; [64] &quot;STAR_AreaSCstS&quot; &quot;STAR_AreaSPn&quot; &quot;STAR_AreaSPs&quot; [67] &quot;STAR_AreaSTAR&quot; &quot;STAR_AreaSTAR2&quot; &quot;STAR_AreaSTARlite&quot; [70] &quot;STAR_Dcaparea&quot; #For more information on these strata, contact &lt;swfsc.info@noaa.gov&gt; The package includes functions for visualizing and selecting from these strata. See the Strata Gallery appendix. To create your own geostratum, you would use code like this: # Create a dataframe of coordinates # (this example is a closed rectangle) mine1 &lt;- data.frame(Lon = c(-120, -125, -125, -120, 120), Lat = c(20, 20, 40, 40, 20)) # Make into a list my_strata &lt;- list(mine1 = mine1) # Check it out my_strata $mine1 Lon Lat 1 -120 20 2 -125 20 3 -125 40 4 -120 40 5 120 20 # Supply to load_settings() my_settings &lt;- load_settings(strata = my_strata) To keep some of the strata from strata_cnp then add one of your own design: # Subset `strata_cnp`: my_cnp &lt;- strata_cnp[c(2,3)] # Check it out my_cnp %&gt;% names [1] &quot;OtherCNP&quot; &quot;MHI&quot; # Create your new geostratum mine1 &lt;- data.frame(Lon = c(-120, -125, -125, -120, 120), Lat = c(20, 20, 40, 40, 20)) # Assemble your list: my_strata &lt;- c(my_cnp, list(mine1 = mine1)) # Check it out my_strata $OtherCNP Lon Lat 1 -131 40.00 2 -126 32.05 3 -120 25.00 4 -120 -5.00 5 -185 -5.00 6 -185 40.00 7 -131 40.00 $MHI Lon Lat 1 -156.00 22.00 2 -154.40 20.60 3 -153.50 19.20 4 -154.35 18.55 5 -155.20 17.75 6 -157.00 18.25 7 -157.50 19.20 8 -161.00 21.84 9 -160.00 23.00 10 -157.00 22.50 11 -156.00 22.00 $mine1 Lon Lat 1 -120 20 2 -125 20 3 -125 40 4 -120 40 5 120 20 # Supply to load_settings() my_settings &lt;- load_settings(strata = my_strata) Survey-wide settings Survey-wide settings apply universally to all species in the analysis. Defaults settings$survey $out_handling [1] &quot;remove&quot; $max_row_interval [1] 900 $segment_method [1] &quot;day&quot; $segment_target_km [1] 150 $segment_max_interval [1] 48 $segment_remainder_handling [1] &quot;segment&quot; $ship_list NULL $species_codes NULL $group_size_coefficients NULL $smear_angles [1] FALSE Defaults for the survey argument list are built up efficiently using the function load_survey_settings() (see example code at bottom). Details The survey_settings input accepts a list with any of the following named slots: out_handling: the first slot allows you to specify how data occurring outside of geo-strata should be handled. If this is set to \"remove\", those rows will be filtered out of the data early in the process. This reduces memory usage, speeds up processing, and gives you geographic control of how effort and sightings will be summarize. If this is set to \"stratum\", those data will be assigned to a fake geo-stratum, named \"out\". Effort in the \"out\" stratum will not be segmentized, but \"out\" sightings will be processed and retained in the final datasets. This setting might be useful if you want to use \"out\" data for survey summaries and/or detection function estimation. The default is \"remove\", since that saves the most time and memory. max_row_interval: The maximum alloweable time interval, in seconds, between rows before LTabundR assumes that there has been a break in survey data logging. The default is 900 seconds, or 15 minutes. segment_method: This and the next few slots are devoted to controlling how effort will be “segmentized”, or chopped into discrete sections for the purposes of estimating the variance of the density/abundance estimates. The two method options are \"day\" – all effort within the same Cruise-StudyArea-Stratum-Year-Effort scenario will be binned into segments by calendar date – and \"equallength\" – effort within each unique effort scenario (Cruise-StudyArea-etc.) will be divided into segments of approximately equal length. See the Appendix on segmentizing for details. segment_target_km: if segmentizing by \"equallength\", this field allows you to specify what that target length is, in km. segment_max_interval: if segmentizing by \"equallength\", this setting allows you to specify the time gaps in effort that are allowed to be contained within a single segment. For example, if your goal is a few large segments of equal length (e.g., 150-km segments, for bootstrap estimation of density variance), you are probably willing for discrete periods of effort to be concatenated into a single segment, even if the gaps between effort are as large as 1 or 2 days, in which case you would set segment_max_interval to 24 or 48 (hours), respectively. However, if your goal is many smaller segments (e.g., 5-km segments, for habitat modeling), you want to ensure that effort is contiguous so that segment locations can be accurately related to environmental variables, in which case you would set segment_max_interval to be very small (e.g., .2 hours, or 12 minutes). Setting this interval to a small number, such as 0.2, also allows the segmentizing function to overlook momentary breaks in effort. segment_remainder_handling: if segmentizing by \"equallength\", periods of effectively-contiguous effort (as specified by segment_max_interval) are unlikely to be perfectly divisible by your segment_target_km; there is going to be a remainder. You can handle this remainder in three ways: (1) \"disperse\" allows the function to adjust segment_target_km so that there is in fact no remainder, effectively dispersing the remainder evenly across all segments within that period of contiguous effort; (2) \"append\" asks the function to append the remainder to a randomly selected segment, such that most segments are the target length with the exception of one longer one; or (3) \"segment\" asks the function to simply place the remainder in its own segment, placed randomly within the period of contiguous effort. This setting also has a second layer of versatility because it can accept a one- or two-element character vector. If a two-element vector is provided (e.g., c(\"append\",\"segment\")), the first element will be used in the event that the remainder is less than or equal to half your segment_target_km; if the remainder is more than half that target length, the second element will be used. This feature allows for replication of the segmentizing methods in Becker et al. (2010). The remaining slots in survey_settings pertain to various datasets and settings used in data processing: ship_list: A data.frame containing a list of survey numbers and ship names. If not provided the default version, which was current as of the release of ABUND9 in 2020, will be used (data(ships)), although note that surveys not included there will not be associated with a ship. Supplied data.frames must match the column naming structure of data(ships). species_codes: A data.frame containing species codes. This is an optional input, chiefly used to format species names in the reporting stage of the workflow (lta_report() especially). If missing neither data processing nor line transect analysis will be obstructed. If the user supplies a data.frames it must match the column naming structure of data(species_codes). group_size_coefficients: A data.frame of calibration factors. Find details in the subsection on processing sightings and estimating school size. smear_angles: If TRUE (the default is FALSE), bearing angles to a group of animals will be “smeared” by adding a uniformly distributed random number between -5 and +5 degrees. This has not been used in any recent analyses because observers have not been rounding angles as much as they used to. It was suggested by Buckland et al. (2001) as a method for dealing with rounding which is especially influential when rounding to zero places many sightings at zero perpendicular distance. Cohort-specific settings Cohort-specific settings apply only to a group of species. Since you can add as many cohorts to a settings object as you need, this allows you to stage your entire analysis and run your code once without modifying code or creating multiple versions of your code for each analysis of each cohort. Defaults The default is to use a single cohort for all species: settings$cohorts %&gt;% names [1] &quot;default&quot; Default values for the default cohort: settings$cohorts$default $id [1] &quot;default&quot; $species NULL $strata NULL $probable_species [1] FALSE $sighting_method [1] 0 $cue_range [1] 0 1 2 3 4 5 6 7 $school_size_range [1] 0 10000 $school_size_calibrate [1] TRUE $calibration_floor [1] 0 $use_low_if_na [1] FALSE $io_sightings [1] 0 $geometric_mean_group [1] TRUE $truncation_km [1] 5.5 $beaufort_range [1] 0 1 2 3 4 5 6 $abeam_sightings [1] FALSE $strata_overlap_handling [1] &quot;smallest&quot; &quot;largest&quot; &quot;each&quot; $distance_types [1] &quot;S&quot; &quot;F&quot; &quot;N&quot; $distance_modes [1] &quot;P&quot; &quot;C&quot; $distance_on_off [1] TRUE Defaults for the cohorts argument list are built up efficiently using the function load_cohort_settings() (see example code at bottom). Details The cohort_settings input accepts a list of any length. Each slot in that list can contain settings for a different cohort. Each cohort list can have any of the following named slots: id: An informal identifier for this cohort, to help you keep track of which cohort is which. For example, settings for a cohort of large whales species could be named \"big whales\"; settings for small delphinids and phocoenids could be named \"small_odontocetes\"; settings for beaked whales could be named \"beakers\". species: A character vector of species codes to include in this cohort. If NULL (the default), all species within the survey data will be included. Note that if you specify a vector, all species to be used in modeling a detection function for this cohort must be included here. For example, in Hawaii the bottlenose dolphin is analyzed as part of a multi-species pool along with the rough-toothed dolphin, Risso’s dolphin, and pygmy killer whale. However, the bottlenose dolphin has insular populations that need to be differentiated from their pelagic counterpart, which requires some special geostratum handling that behooves the preparation of a dedicated cohort for bottlenose dolphin. Even so, in the cohort_settings object for the bottlenose dolphin cohort, the species codes for the rough-toothed, Risso’s, and pygmy-killer-whale dolphins need to be provided in this species argument. Conversely, in the cohort_settings object that holds most other species, including the rough-toothed, Risso’s, and pygmy-killer-whale dolphins, the bottlenose dolphin’s code still needs to be included in this species argument. strata: A character vector of geostratum names. These must match the names listed in the strata slot of your survey settings (see documentation for load_survey_settings()). If NULL (the default), all geostrata in your survey settings will be used. This argument is an opportunity to subset the geostrata used for a cohort. For example, as discussed above, certain dolphin species in Hawaiian waters have unique geostrata that apply only to their insular/pelagic populations, and should only have a role in breaking effort segments in the bootstrap variance analysis for these specific species. Those dolphins should be given their own cohort, and those insular/pelagic geostrata should be included in this strata argument. Conversely, all other species should be placed in a separate cohort and only the generic geostrata should be included in this strata argument. See the WHICEAS example below for a demonstration. probable_species: If TRUE (default is FALSE), the “probable” species identifications will be used in place of the “unidentified” categories. sighting_method: A coded integer which determines which sightings will be included based on how they were first seen. Allowable codes are 0=any method, 1=with 25X only, 2=neither with 25x binoculars nor from the helicopter (i.e., naked eyes and 7x binoculars only). These codes match those used in ABUND7/9. cue_range: Numeric vector of acceptable “observation cues” for sightings used in estimates of abundance. (0=this detail is missing in the data, 1=associated birds, 2=splashes, 3=body of the marine mammal, 4=associated vessel, 5=?, 6=blow / spout, 7=associated helicopter). These codes match those used in ABUND7/9. school_size_range: Minimum and maximum group sizes to be included in estimates of abundance. This is the overall group size, not the number of the given species that are present in a mixed-species group. school_size_calibrate: A logical (TRUE or FALSE) specifying whether or not to carry out group size adjustments according to the calibration table provided in survey$group_size_coefficients (if that table is provided). This setting allows you to toggle the survey-wide setting for certain cohorts. For example, perhaps you want to carry out calibration for a cohort of dolphin species, but not for a cohort of large whales whose group sizes tend to be smaller and easier to estimate accurately. calibration_floor: A numeric indicating the minimum school size estimate for which group size calibration will be attempted. This pertains only to observers who do no have an entry in the group_size_coefficients table provided in load_survey_settings() (that table has a calibration floor for each observer). The default is 0, meaning that calibration will be attempted for all group size estimates, regardless of the raw estimate. use_low_if_na: If this setting is TRUE, and an observer(s) does not make a best estimate of group size, mean group size will be calculated from “low” estimates. This will be done only if no observer has a “best” estimate. io_sightings: A coded integer which specifies how sightings by the independent observer will be handled. Allowable codes, which are inherited from those used in ABUND7/9, are \"_1\"=include independent observer sightings wih all other sightings, \"0\"=ignore sightings by independent observer, \"1\"=use only sightings made by regular observer team WHEN an independent observer was present, \"2\"=include only sightings made by the independent observer. IO sightings are typically used only for making g(0) estimates, otherwise IO sightings are usually ignored (code = \"0\"). geometric_mean_group: This logical variable specifies whether to use a weighted geometric mean when calculating mean group size. Barlow et al. (1998) found that this gave slightly better performance than an arithmetic mean group size for calibrated estimates. Default is TRUE, but a geometric mean will only be calculated if group_size_coefficients is not NULL. If group_size_coefficients is NULL, then an arithmetic mean will be calculated. (This setting does not apply to subgroup analyses.) truncation_km: Specifies the maximum perpendicular distance for groups that could potentially be included for abundance estimation. This is not the stage at which you set the truncation distance for detection function modeling; it is simply a preliminary cutoff for sightings made at unrealiable detection distances. The default is set at 5.5 km, which is the maximum distance sightings are typically approached (“closing mode”) during NOAA Fisheries surveys. beaufort_range: Vector of Beaufort sea states (integers) that are acceptable in estimating the detection function and density. Beaufort data with a decimal place will be rounded to the nearest integer to evaluate for inclusion. abeam_sightings: = If TRUE, sightings that occur aft of beam (i.e., 90 degrees) are included in estimating the detection function and densities. Default is FALSE: all abeam sightings will be ignored. strata_overlap_handling: This setting informs how effort is split into segments when surveys cross stratum boundaries, and also which stratum name is assigned to each row of data. Note that the main impact of this setting is on how effort is broken into segments; the assigned stratum name is for display only and will not constrain options for including/excluding strata in analyses farther along in the LTabundR workflow. The default option is \"smallest\", which means that effort will always be assigned to the smallest stratum when multiple strata overlap spatially. This is a safe option for surveys with “nested” strata (such as the Central North Pacific strata used by NOAA Fisheries; see below). Another option is \"each\"in which each time a stratum boundary is crossed the current segment will end and a new segment will begin. Also, stratum assignments for each row of effort will be shown as a concatenation of all the stratum layers overlapping at its position (e.g., “OtherCNP&amp;HI_EEZ”). Note that the \"each\" option segmentizes effort in the exact same was as \"smallest\" when strata are fully nested; its main advantage is in dealing with partially overlapping strata (such as strata used in the Marianas archipelago; see below). The third option is \"largest\", in which the largest of overlapping strata is used to assign a stratum name to each row. (We are not sure what use case this would serve, but we offer it as an option for niche analyses.) distance_types: A character vector of the effort types that will be included in detection function estimation and density estimation, and therefore considered in effort segmentizing. Accepted values are \"S\" (systematic/standard effort), \"F\" (fine-scale effort), and \"N\" (non-systematic/non-standard effort, in which systematic protocols are being used but effort is not occurring along design-based transect routes). The default values are c(\"S\",\"F\",\"N\"). distance_modes: The effort modes that will be included in detection function estimation and density estimation, and therefore considered in effort segmentizing. Accepted values are \"P\" (passing) and \"C\" (closing), and the default values are c(\"P\",\"C\"). distance_on_off: The value(s) of OnEffort (On Effort is TRUE, Off Effort is FALSE) that will be included in detection function estimation and density estimation, and therefore considered in effort segmentizing. Default is TRUE only. (We don’t expect FALSE or c(TRUE,FALSE) to be used much, if at all, but we make this option available). Example code Use settings defaults No strata or group size calibration. settings &lt;- load_settings() Use settings defaults, but with strata # Load strata dataframes data(strata_cnp) settings &lt;- load_settings(strata = strata_cnp) Customize survey, but not cohorts This code will process survey data such that effort segments are 5 km in length, with any effort falling outside of the provided geostrata relegated to a virtual geostratum named \"out\". Since a cohort is not specified, the default values will be used. # Load built-in datasets data(strata_cnp) data(group_size_coefficients) data(ships) data(species_codes) # Survey settings survey &lt;- load_survey_settings(out_handling = &#39;stratum&#39;, max_row_interval = 700, segment_method = &#39;equallength&#39;, segment_target_km = 5, segment_max_interval = .3, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), ship_list = ships, species_codes = species_codes, group_size_coefficients = group_size_coefficients, smear_angles = FALSE) # Load settings settings &lt;- load_settings(strata = strata_cnp, survey) Fully custom: WHICEAS case study These are the settings we will use in the remainder of the tutorial. Survey-wide settings To emulate the analysis done in Bradford et al. (2021), we want to process effort with 150-km segments, relegating any remainder to its own segments. We also want to make sure to remove any survey data that falls outside of the geostrata, to ensure that detection functions are regionally specific. We will use the built-in tables for species codes, ship codes, and group size calibration coefficients. data(species_codes) data(ships) data(group_size_coefficients) survey &lt;- load_survey_settings( out_handling = &#39;remove&#39;, max_row_interval = Inf, segment_method = &quot;equallength&quot;, segment_target_km = 150, segment_max_interval = 24, segment_remainder_handling = c(&quot;segment&quot;), ship_list = ships, species_codes = species_codes, group_size_coefficients = group_size_coefficients, smear_angles = FALSE ) Geostrata We will use the built-in dataset of Central North Pacific geostrata: data(strata_cnp) strata_cnp %&gt;% names [1] &quot;HI_EEZ&quot; &quot;OtherCNP&quot; &quot;MHI&quot; &quot;WHICEAS&quot; [5] &quot;Spotted_OU&quot; &quot;Spotted_FI&quot; &quot;Spotted_BI&quot; &quot;Bottlenose_KaNi&quot; [9] &quot;Bottlenose_OUFI&quot; &quot;Bottlenose_BI&quot; &quot;NWHI&quot; Species cohorts Cohort 1: All species At least one cohort needs to be specified in order to run process survey data, so this first cohort will serve as a ‘catch-all’ for species who do not need special handling. It does not hurt to include all species in this catch-all cohort – except perhaps by increasing the file-size of your processed data by a few kilobytes – even if you will be creating a separate, dedicated cohort for one of these species downstream. Having a catch-all cohort like this serves two purposes: (1) it avoids unforeseen complications if you will be modeling detection functions with multi-species pools, as mentioned above; and (2) it will simplify the code you will use to produce summary statistics of effort and sightings totals, since you will not need to pool together statistics from multiple cohorts. To build this catch-all cohort, we will not specify any species so that all species in the data are included, and we will specify that only the generic geostrata should be used, so that species specific insular stock boundaries are ignored. Here we show all possible inputs. Most of these match the built-in defaults. Those that do not (there are just 3) are noted with a commented asterisk. all_species &lt;- load_cohort_settings( id = &quot;all&quot;, # * species = NULL, strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;), # * probable_species = FALSE, sighting_method = 0, cue_range = 0:7, school_size_range = c(0, 10000), school_size_calibrate = TRUE, calibration_floor = 0, use_low_if_na = TRUE, io_sightings = 0, geometric_mean_group = TRUE, truncation_km = 7.5, # * beaufort_range = 0:6, abeam_sightings = FALSE, # * strata_overlap_handling = c(&quot;smallest&quot;), distance_types = c(&#39;S&#39;,&#39;F&#39;,&#39;N&#39;), distance_modes = c(&#39;P&#39;,&#39;C&#39;), distance_on_off = TRUE ) Cohort 2: Bottlenose dolphin As mentioned above, bottlenose dolphins are going to be analyzed as part of a multi-species pool that includes the rough-toothed dolphin, Risso’s dolphin, and pygmy killer whale. Because those species’ codes will be needed to model the detection function used in bottlenose dolphin density/abundance estimation, those species codes will be included in this cohort’s settings. Also mentioned above: Hawaii has a pelagic population of bottlenose dolphins as well as several distinct insular stocks. In this case study, we are interested in estimating only the abundance of the pelagic population, which means we will need to include geostrata of the insular stock boundaries in order to make sure the effort and sightings within those insular areas are ignored. That means we will specify the generic geostrata (\"WHICEAS\", \"HI_EEZ\", and \"Other_CNP\"), as well as the geostrata for the insular stocks. We can spell out as many any inputs that we wish, but here will only show the inputs that are non-default and/or different from Cohort 1 above. bottlenose &lt;- load_cohort_settings( id = &quot;bottlenose&quot;, species = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;, &#39;Bottlenose_BI&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_KaNi&#39;), truncation_km = 7.5) Cohort 3: Pantropical spotted dolphin Similar to the bottlenose dolphin above, spotted dolphins in Hawaiian waters belong to pelagic stocks as well as insular stocks. We will be estimating density/abundance for the only pelagic stocks here, but we need to include the geostrata for the insular stocks in order to ignore their effort and sightings. spotted &lt;- load_cohort_settings( id = &quot;spotted&quot;, species = &#39;002&#39;, strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;, &#39;Spotted_OU&#39;,&#39;Spotted_FI&#39;,&#39;Spotted_BI&#39;), truncation_km = 7.5) Compile settings Finally, we create our settings object, providing cohorts as a list of lists: settings &lt;- load_settings(strata = strata_cnp, survey = survey, cohorts = list(all_species, bottlenose, spotted)) Save this settings object locally, to use in downstream scripts: save(settings, file=&#39;whiceas_settings.RData&#39;) "],["das.html", " 2 DAS editing Reviewing a DAS file Staging edits", " 2 DAS editing The LTabundR package includes several functions that facilitate the exploration of DAS files of WinCruz survey data, as well as a function that allows you to apply “edits” to the survey data in a reproducible way (i.e., using code, without modifying the actual data file). Reviewing a DAS file das_readtext() The swfscDAS package has functions for reading in a DAS file and parsing it into columns of fixed-width text. To complement those functions, LTabundR includes the function das_readtext(), which reads in a DAS file without applying any column parsing, so that the data can be read in its true raw format. # Local path to DAS file das_file &lt;- &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39; das &lt;- das_readtext(das_file) das$das %&gt;% head(20) [1] &quot; 1B 1805 073086 N31:57. W116:57. 989&quot; [2] &quot; 1S 1805 073086 N31:57. W116:57. 01 004 6 4 300 1.77 1.5&quot; [3] &quot; 2A 1805 073086 N31:57. W116:57. 01 15.0 N 099&quot; [4] &quot; 1 004 0001 0001 0001 100 000 000 000&quot; [5] &quot; 1S 0610 073186 N30:05. W116:04. 01 004 3 4 340 5.00 0.1&quot; [6] &quot; 2A 0610 073186 N30:05. W116:04. 01 18.3 N 005&quot; [7] &quot; 1 004 0150 0175 0125 100 000 000 000&quot; [8] &quot; 3B.1311 073186 N29:46. W115:52. 989 N&quot; [9] &quot; 4R.1311 073186 N29:46. W115:52. N&quot; [10] &quot; 5P.1311 073186 N29:46. W115:52. 022 031 056&quot; [11] &quot; 6V.1311 073186 N29:46. W115:52. 4 16.1&quot; [12] &quot; 7N.1311 073186 N29:46. W115:52. 167 10.5&quot; [13] &quot; 8W.1311 073186 N29:46. W115:52. 2&quot; [14] &quot; 9V.1329 073186 N29:43. W115:51. 5 16.1&quot; [15] &quot; 10P.1404 073186 N29:37. W115:50. 004 056 062&quot; [16] &quot; 11V.1404 073186 N29:37. W115:50. 5 18.3&quot; [17] &quot; 12S.1406 073186 N29:37. W115:50. 02 004 3 4 355 1.4 1.8&quot; [18] &quot; 13A.1406 073186 N29:37. W115:50. 02 18.3 Y 005&quot; [19] &quot; 1 022 0030 0100 0020 100 000 000 000&quot; [20] &quot; 2 004 0250 0300 0200 100 000 000 000&quot; To follow along, this data file can be downloaded here. das_inspector() We also provide a function, das_inspector(), that allows you to explore a DAS file within an interactive Shiny app. This app can also be used to find, prepare, and preview coded edits to the DAS data. das_inspector(das_file) In this app, you specify the rows and ‘columns’ (i.e., character indices) that will be affected by your edit, the edit you want to apply, and the type of edit it will be (see next subsection). You then “save” that edit with the click of a button, and when you close the app your log of staged edits will be returned as a list. You can then pass this list of edits to das_editor() (next subsection on this page), or as an argument within process_surveys() (next page). Staging edits The das_editor() function allows you to apply edits to a DAS file without modifiyng the original data. You supply edits to this function as a set of instructions saved within a list object. You can prepare these instructions manually or use the das_inspector() function above to get help. The das_editor() function then loops through each edit, applies them to a local version of the data, and returns that modified data to the user. This allows survey data to be modified reproducibly before being processed with LTabundR::process_surveys() without touching the original DAS data files or requiring analysts to duplicate files and make one-off modifications manually. We expect it would be rare for a user to call das_editor() directly; instead, they would supply their edits as an argument in process_surveys(), and that function will call das_editor() internally to amend the survey data before it is processed (this is discussed on the next page). Note that there is no limit to the number of edits that can be provided in a single list, and they can be provided in any order. The das_editor() function will sort the edits by DAS file and by edit type, and then apply edits in increasing order of “disruption”, i.e., text replacements first, then moving rows of data (no net change in number of rows), then copying-pasting, inserting, and deleting. The das_editor() function can currently handle 6 types of edits, which we shall demonstrate below. Types of editing To demonstrate the types of editing actions that can be achieved through das_editor(), we will use the DAS file from the 2020 WHICEAS survey (you can download here). # Local path to das_file das_file &lt;- &quot;data/surveys/HICEASwinter2020.das&quot; das &lt;- das_readtext(das_file) Verbatim text replacement The edit will be interpreted verbatim as text that will replace the specified data. edits &lt;- list(list(das_file = das_file, type = &#39;text&#39;, rows = 10:15, chars = 20:39, edit = &#39;lat, lon&#39;)) Here is what this edit log object will look like: edits [[1]] [[1]]$das_file [1] &quot;data/surveys/HICEASwinter2020.das&quot; [[1]]$type [1] &quot;text&quot; [[1]]$rows [1] 10 11 12 13 14 15 [[1]]$chars [1] 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 [[1]]$edit [1] &quot;lat, lon&quot; And here is the effect it will have: # Before das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; # After dase &lt;- das_editor(edits) Applying edits to DAS file: data/surveys/HICEASwinter2020.das --- verbatim/function text replacements ... --- --- working on edit 1 ... dase$das[[1]]$das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 lat, lon&quot; [3] &quot;011B.073111 011920 lat, lon 2001 C -10 N&quot; [4] &quot;012R.073111 011920 lat, lon F&quot; [5] &quot;013P.073111 011920 lat, lon 126 307 238 &quot; [6] &quot;014V.073111 011920 lat, lon 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 lat, lon 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; Function-based text replacement The edit will be evaluated as a function that is applied to the specified characters in each of rows. edits &lt;- list(list(das_file = das_file, type = &#39;function&#39;, rows = 10:15, chars = 20:39, edit = &#39;tolower&#39;)) # Before das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; # After dase &lt;- das_editor(edits) Applying edits to DAS file: data/surveys/HICEASwinter2020.das --- verbatim/function text replacements ... --- --- working on edit 1 ... dase$das[[1]]$das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 n21:50.91 w159:46.33&quot; [3] &quot;011B.073111 011920 n21:51.12 w159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 n21:51.12 w159:46.36 F&quot; [5] &quot;013P.073111 011920 n21:51.12 w159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 n21:51.12 w159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 n21:51.12 w159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; A special application of this form of editing is adjusting timestamps using the LTabundR function das_time(). In this next example, we subtract an hour from the first 5 rows of timestamps: edits &lt;- list(list(das_file = das_file, type = &#39;function&#39;, rows = 1:5, chars = 6:40, edit = &#39;function(x){das_time(x, tz_adjust = -1)$dt}&#39;)) # Before das$das[1:6] [1] &quot;001* 071152 011920 N21:47.99 W159:45.91&quot; [2] &quot;002* 071352 011920 N21:48.31 W159:45.94&quot; [3] &quot;003* 071552 011920 N21:48.63 W159:45.97&quot; [4] &quot;004* 071752 011920 N21:48.95 W159:46.01&quot; [5] &quot;005* 071952 011920 N21:49.28 W159:46.04&quot; [6] &quot;006* 072152 011920 N21:49.60 W159:46.08&quot; # After dase &lt;- das_editor(edits) Applying edits to DAS file: data/surveys/HICEASwinter2020.das --- verbatim/function text replacements ... --- --- working on edit 1 ... dase$das[[1]]$das$das[1:6] [1] &quot;001* 061152 011920 N21:47.99 W159:45.91&quot; [2] &quot;002* 061352 011920 N21:48.31 W159:45.94&quot; [3] &quot;003* 061552 011920 N21:48.63 W159:45.97&quot; [4] &quot;004* 061752 011920 N21:48.95 W159:46.01&quot; [5] &quot;005* 061952 011920 N21:49.28 W159:46.04&quot; [6] &quot;006* 072152 011920 N21:49.60 W159:46.08&quot; In the event that a survey was conducted using UTC timestamps instead of local time, you can adjust each timestamp according to its actual timezone as determined from its corresponding lat/long coordinates. Let’s say the first 5 timestamps were collected in UTC by accident. The following code could correct that mistake: edits &lt;- list(list(das_file = das_file, type = &#39;function&#39;, rows = 1:5, chars = 6:40, edit = &#39;function(x){das_time(x, tz_adjust = &quot;from utc&quot;)$dt}&#39;)) # Before das$das[1:6] [1] &quot;001* 071152 011920 N21:47.99 W159:45.91&quot; [2] &quot;002* 071352 011920 N21:48.31 W159:45.94&quot; [3] &quot;003* 071552 011920 N21:48.63 W159:45.97&quot; [4] &quot;004* 071752 011920 N21:48.95 W159:46.01&quot; [5] &quot;005* 071952 011920 N21:49.28 W159:46.04&quot; [6] &quot;006* 072152 011920 N21:49.60 W159:46.08&quot; # After dase &lt;- das_editor(edits) Applying edits to DAS file: data/surveys/HICEASwinter2020.das --- verbatim/function text replacements ... --- --- working on edit 1 ... dase$das[[1]]$das$das[1:6] [1] &quot;001* 201152 011820 N21:47.99 W159:45.91&quot; [2] &quot;002* 201352 011820 N21:48.31 W159:45.94&quot; [3] &quot;003* 201552 011820 N21:48.63 W159:45.97&quot; [4] &quot;004* 201752 011820 N21:48.95 W159:46.01&quot; [5] &quot;005* 201952 011820 N21:49.28 W159:46.04&quot; [6] &quot;006* 072152 011920 N21:49.60 W159:46.08&quot; Moving data The rows will be deleted from their current location and pasted immediately below the row number specified by edit. The moved rows will be given the same date, time, latitude, and longitude, as the edit row. edits &lt;- list(list(das_file = das_file, type = &#39;move&#39;, rows = 10, chars = NULL, edit = 15)) # Before das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; # After dase &lt;- das_editor(edits) Applying edits to DAS file: data/surveys/HICEASwinter2020.das --- move, copy/paste, insertion, and deletion events ... --- --- working on edit 1 ... dase$das[[1]]$das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [3] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [4] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [5] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [6] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [7] &quot;010* 073111 011920 N21:51.12 W159:46.36&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; Copying &amp; pasting data The rows will be copied from their current location and pasted immediately below the row number specified by edit. The pasted rows will be given the same date, time, latitude, and longitude, as the edit row. edits &lt;- list(list(das_file = das_file, type = &#39;copy&#39;, rows = 10, edit = 15)) # Before das$das[9:17] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; [9] &quot;017*.073152 011920 N21:51.24 W159:46.39&quot; # After dase &lt;- das_editor(edits) Applying edits to DAS file: data/surveys/HICEASwinter2020.das --- move, copy/paste, insertion, and deletion events ... --- --- working on edit 1 ... dase$das[[1]]$das$das[9:17] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;010* 073111 011920 N21:51.12 W159:46.36&quot; [9] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; Inserting data The text provided in edit will be inserted verbatim immediately below the first of the rows provided. edits &lt;- list(list(das_file = das_file, type = &#39;insert&#39;, rows = 10, edit = &quot;SEQ* HHMMSS MMDDYY NDG:MI.NT WDEG:MI.NT&quot;)) # Before das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; # After dase &lt;- das_editor(edits) Applying edits to DAS file: data/surveys/HICEASwinter2020.das --- move, copy/paste, insertion, and deletion events ... --- --- working on edit 1 ... dase$das[[1]]$das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;SEQ* HHMMSS MMDDYY NDG:MI.NT WDEG:MI.NT&quot; [4] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [5] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [6] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [7] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [8] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; Deleting data The specified rows will be deleted. edits &lt;- list(list(das_file = das_file, type = &#39;delete&#39;, rows = 10)) # Before das$das[9:16] # After dase &lt;- das_editor(edits) dase$das[[1]]$das$das[9:16] Actual edits The above examples were silly demonstrations of the types of edits that can be handled by das_editor(). Here we show the preparation of four edits that we will actually use when we process surveys on the next page. These edits will be applied to the following DAS file of survey data from 1986-2020: # Local path to das_file das_file &lt;- &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39; das &lt;- das_readtext(das_file) Cruise 1607 sighting 55 This sighting, at sequence ID 032 below, currently triggers errors in swfscDAS due to a manually entered R event a few lines above that does not have the P (observer positions) event that typically follows it. Without tht P entry, the observer positions of the sighting are unknown. das[128111:128125,] [1] &quot;022P 120643 041597 N37:00.08 W151:55.47 143 091 005&quot; [2] &quot;023C 120643 041597 N37:00.08 W151:55.47 both right and left on 7x and naked eye&quot; [3] &quot;024N 120643 041597 N37:00.08 W151:55.47 075 08.0&quot; [4] &quot;025W 120643 041597 N37:00.08 W151:55.47 1 310 6.0&quot; [5] &quot;026C 120916 041597 N37:00.33 W151:55.18 both right and left on 7x and naked eye&quot; [6] &quot;027* 121120 041597 N37:00.45 W151:54.85&quot; [7] &quot;028C 121933 041597 N37:00.75 W151:53.50 both right and left back on 25x&quot; [8] &quot; R.121933 041597 N37:00.75 W151:53.50 S&quot; [9] &quot;029*.122120 041597 N37:00.81 W151:53.22&quot; [10] &quot;030V.122120 041597 N37:00.81 W151:53.22 4 07 320 21.0&quot; [11] &quot;031C.122946 041597 N37:01.12 W151:51.77 wind speed appears lower than bridge speed, 10-15&quot; [12] &quot;032S.123023 041597 N37:01.14 W151:51.70 055 005 3 4 058 4.0 0.8&quot; [13] &quot;033A.123023 041597 N37:01.14 W151:51.70 055 N N 022&quot; [14] &quot; 1 005 0002 0002 0002 100&quot; [15] &quot;034C.123120 041597 N37:01.17 W151:51.51 remained on effort after sighting&quot; (Note that we used das_inspector() to get the row numbers for this region of the data.) To fix this, we can stage an edit that copies the P line that occurs minutes earlier and pastes that line just below the rogue R line. edit_1607_55 &lt;- list(das_file = das_file, type = &#39;copy&#39;, rows = 128111, chars = NULL, edit = 128118) Here is what this change will look like: dase &lt;- das_editor(list(edit_1607_55)) Applying edits to DAS file: data/surveys/CenPac1986-2020_Final_alb.das --- move, copy/paste, insertion, and deletion events ... --- --- working on edit 1 ... dase$das[[1]]$das$das[128111:128123] [1] &quot;022P 120643 041597 N37:00.08 W151:55.47 143 091 005&quot; [2] &quot;023C 120643 041597 N37:00.08 W151:55.47 both right and left on 7x and naked eye&quot; [3] &quot;024N 120643 041597 N37:00.08 W151:55.47 075 08.0&quot; [4] &quot;025W 120643 041597 N37:00.08 W151:55.47 1 310 6.0&quot; [5] &quot;026C 120916 041597 N37:00.33 W151:55.18 both right and left on 7x and naked eye&quot; [6] &quot;027* 121120 041597 N37:00.45 W151:54.85&quot; [7] &quot;028C 121933 041597 N37:00.75 W151:53.50 both right and left back on 25x&quot; [8] &quot; R.121933 041597 N37:00.75 W151:53.50 S&quot; [9] &quot;022P 121933 041597 N37:00.75 W151:53.50 143 091 005&quot; [10] &quot;029*.122120 041597 N37:00.81 W151:53.22&quot; [11] &quot;030V.122120 041597 N37:00.81 W151:53.22 4 07 320 21.0&quot; [12] &quot;031C.122946 041597 N37:01.12 W151:51.77 wind speed appears lower than bridge speed, 10-15&quot; [13] &quot;032S.123023 041597 N37:01.14 W151:51.70 055 005 3 4 058 4.0 0.8&quot; Cruise 1607 sighting 68 This sighting faces a similar issue: a rogue R event without the follow-up P event. This case is also missing the follow-up V event (viewing conditions). das[129980:129993,] [1] &quot;012* 065738 042797 N31:03.98 W136:03.60&quot; [2] &quot;013C 065944 042797 N31:04.25 W136:03.55 Both observers on 7X since going ON EFFORT this morning. JP.&quot; [3] &quot;014P 070118 042797 N31:04.43 W136:03.51 091 005 148&quot; [4] &quot;015V 070118 042797 N31:04.43 W136:03.51 5 06 320 20.0&quot; [5] &quot;016N 070118 042797 N31:04.43 W136:03.51 010 08.0&quot; [6] &quot;017W 070118 042797 N31:04.43 W136:03.51 1 02 03 035 6.0&quot; [7] &quot;018C 070257 042797 N31:04.65 W136:03.47 This rotation is now on the 25X&quot; [8] &quot; R.070257 042797 N31:04.65 W136:03.47 S&quot; [9] &quot;019*.070738 042797 N31:05.25 W136:03.36&quot; [10] &quot;020S.071601 042797 N31:06.34 W136:03.16 068 148 3 4 028 1.8 1.5&quot; [11] &quot;021A.071601 042797 N31:06.34 W136:03.16 068 N N 037&quot; [12] &quot; 1 091 0009 0011 0008 100&quot; [13] &quot; 2 005 0014 0015 0012 100&quot; [14] &quot; 3 148 0018 0023 0015 100&quot; To fix this we will stage a similar edit, this time copying and pasting two rows (P and V events) below the rogue R event: edit_1607_68 &lt;- list(das_file = das_file, type = &#39;copy&#39;, rows = c(129982, 129983 , 129985), chars = NULL, edit = 129987) Preview of change: dase &lt;- das_editor(list(edit_1607_68)) Applying edits to DAS file: data/surveys/CenPac1986-2020_Final_alb.das --- move, copy/paste, insertion, and deletion events ... --- --- working on edit 1 ... dase$das[[1]]$das$das[129982:129995] [1] &quot;014P 070118 042797 N31:04.43 W136:03.51 091 005 148&quot; [2] &quot;015V 070118 042797 N31:04.43 W136:03.51 5 06 320 20.0&quot; [3] &quot;016N 070118 042797 N31:04.43 W136:03.51 010 08.0&quot; [4] &quot;017W 070118 042797 N31:04.43 W136:03.51 1 02 03 035 6.0&quot; [5] &quot;018C 070257 042797 N31:04.65 W136:03.47 This rotation is now on the 25X&quot; [6] &quot; R.070257 042797 N31:04.65 W136:03.47 S&quot; [7] &quot;014P 070257 042797 N31:04.65 W136:03.47 091 005 148&quot; [8] &quot;015V 070257 042797 N31:04.65 W136:03.47 5 06 320 20.0&quot; [9] &quot;017W 070257 042797 N31:04.65 W136:03.47 1 02 03 035 6.0&quot; [10] &quot;019*.070738 042797 N31:05.25 W136:03.36&quot; [11] &quot;020S.071601 042797 N31:06.34 W136:03.16 068 148 3 4 028 1.8 1.5&quot; [12] &quot;021A.071601 042797 N31:06.34 W136:03.16 068 N N 037&quot; [13] &quot; 1 091 0009 0011 0008 100&quot; [14] &quot; 2 005 0014 0015 0012 100&quot; Cruise 1621 sighting 245 This is another case of a rogue R event, again missing both the requisite P and the V post-R events. das[271930:271939,] [1] &quot;036E 085345 103002 N20:22.56 W160:02.21 U&quot; [2] &quot;037R.085346 103002 N20:22.56 W160:02.21 S&quot; [3] &quot;038P.085346 103002 N20:22.56 W160:02.21 126 224 200&quot; [4] &quot;039V.085346 103002 N20:22.56 W160:02.21 5 07 000 21.0&quot; [5] &quot;040N.085346 103002 N20:22.56 W160:02.21 285 09.7&quot; [6] &quot;041W.085346 103002 N20:22.56 W160:02.21 1 06 02 045 6.0&quot; [7] &quot;042E 085352 103002 N20:22.56 W160:02.22 U&quot; [8] &quot;043R.085354 103002 N20:22.57 W160:02.23 S&quot; [9] &quot;048S.085359 103002 N20:22.57 W160:02.24 245 200 3 4 057 11.0 0.35 270 2.0&quot; [10] &quot;049A.085359 103002 N20:22.57 W160:02.24 245 N N 015&quot; Staged edit: edit_1621_245 &lt;- list(das_file = das_file, type = &#39;copy&#39;, rows = 271932:271933, chars = NULL, edit = 271937) Preview of change: dase &lt;- das_editor(list(edit_1621_245)) Applying edits to DAS file: data/surveys/CenPac1986-2020_Final_alb.das --- move, copy/paste, insertion, and deletion events ... --- --- working on edit 1 ... dase$das[[1]]$das$das[271930:271941] [1] &quot;036E 085345 103002 N20:22.56 W160:02.21 U&quot; [2] &quot;037R.085346 103002 N20:22.56 W160:02.21 S&quot; [3] &quot;038P.085346 103002 N20:22.56 W160:02.21 126 224 200&quot; [4] &quot;039V.085346 103002 N20:22.56 W160:02.21 5 07 000 21.0&quot; [5] &quot;040N.085346 103002 N20:22.56 W160:02.21 285 09.7&quot; [6] &quot;041W.085346 103002 N20:22.56 W160:02.21 1 06 02 045 6.0&quot; [7] &quot;042E 085352 103002 N20:22.56 W160:02.22 U&quot; [8] &quot;043R.085354 103002 N20:22.57 W160:02.23 S&quot; [9] &quot;038P.085354 103002 N20:22.57 W160:02.23 126 224 200&quot; [10] &quot;039V.085354 103002 N20:22.57 W160:02.23 5 07 000 21.0&quot; [11] &quot;048S.085359 103002 N20:22.57 W160:02.24 245 200 3 4 057 11.0 0.35 270 2.0&quot; [12] &quot;049A.085359 103002 N20:22.57 W160:02.24 245 N N 015&quot; Timestamp issues with Cruise 1004 This edit will correct for the fact that all of Cruise 1004 was conducted using UTC timestamps instead of local timestamps. edit_1004_utc &lt;- list(das_file = das_file, type = &#39;function&#39;, rows = 433327:437665, chars = 6:39, edit = &#39;function(x){das_time(x, tz_adjust = &quot;from utc&quot;)$dt}&#39;) # Before das$das[433326:433330] # beginning of cruise [1] &quot;229* 181728 020510 N13:14.49 E145:00.34&quot; [2] &quot;001* 201109 041910 N13:35.01 E145:59.35&quot; [3] &quot;002* 201309 041910 N13:35.03 E145:59.42&quot; [4] &quot;003* 201509 041910 N13:35.08 E145:59.54&quot; [5] &quot;004* 201709 041910 N13:35.13 E145:59.70&quot; das$das[437664:437667] # end of cruise [1] &quot;431* 044625 050410 N21:15.78 W158:53.32&quot; [2] &quot;432* 044825 050410 N21:16.05 W158:53.31&quot; [3] &quot;001* 144327 080410 N32:39.39 W117:13.60&quot; [4] &quot;002* 144827 080410 N32:38.58 W117:13.49&quot; dase &lt;- das_editor(list(edit_1004_utc)) # After dase$das[[1]]$das$das[433326:433330] [1] &quot;229* 181728 020510 N13:14.49 E145:00.34&quot; [2] &quot;001* 061109 042010 N13:35.01 E145:59.35&quot; [3] &quot;002* 061309 042010 N13:35.03 E145:59.42&quot; [4] &quot;003* 061509 042010 N13:35.08 E145:59.54&quot; [5] &quot;004* 061709 042010 N13:35.13 E145:59.70&quot; dase$das[[1]]$das$das[437664:437667] [1] &quot;431* 184625 050310 N21:15.78 W158:53.32&quot; [2] &quot;432* 184825 050310 N21:16.05 W158:53.31&quot; [3] &quot;001* 074327 080410 N32:39.39 W117:13.60&quot; [4] &quot;002* 144827 080410 N32:38.58 W117:13.49&quot; Note that this type of edit can be dangerous, however, since ships can cross time zone boundaries mid-day, potentially repeating timestamps and giving the appearance that the DAS data is out of chronological order, which may bring about consequences for data processing that are difficult to predict. This type of edit is also time-consuming; since the time zone needs to be calculated in each DAS row individually, this edit could take &gt;20 minutes to process. An expedited (and safer) approximation of this edit would be to simply adjust the timezone by the GMT offset for Guam (UTC + 10 hours). edit_1004_gmt10 &lt;- list(das_file = das_file, type = &#39;function&#39;, rows = 433327:437665, chars = 6:39, edit = &#39;function(x){das_time(x, tz_adjust = 10)$dt}&#39;) dase &lt;- das_editor(list(edit_1004_gmt10)) Applying edits to DAS file: data/surveys/CenPac1986-2020_Final_alb.das --- verbatim/function text replacements ... --- --- working on edit 1 ... dase$das[[1]]$das$das[433326:433330] [1] &quot;229* 181728 020510 N13:14.49 E145:00.34&quot; [2] &quot;001* 061109 042010 N13:35.01 E145:59.35&quot; [3] &quot;002* 061309 042010 N13:35.03 E145:59.42&quot; [4] &quot;003* 061509 042010 N13:35.08 E145:59.54&quot; [5] &quot;004* 061709 042010 N13:35.13 E145:59.70&quot; dase$das[[1]]$das$das[437664:437667] [1] &quot;431* 144625 050410 N21:15.78 W158:53.32&quot; [2] &quot;432* 144825 050410 N21:16.05 W158:53.31&quot; [3] &quot;001* 144327 080410 N32:39.39 W117:13.60&quot; [4] &quot;002* 144827 080410 N32:38.58 W117:13.49&quot; Combining and saving edits Finally, we will collect these edits into a single list and save them for use during survey processing (next page). # Combine edits &lt;- list(edit_1607_55, edit_1607_68, edit_1621_245, #edit_1004_utc, edit_1004_gmt10) # Save saveRDS(edits,file=&#39;cnp_1986_2020_edits.RData&#39;) "],["processing.html", " 3 Data processing Behind the scenes Review Validation", " 3 Data processing In our WHICEAS case study example, we are interested in estimating density/abundance for 2017 and 2020 only, but we want to use surveys from previous years to help model species detection functions. We will therefore be using a dataset of NOAA Fisheries surveys in the Central North Pacific from 1986 to 2020. # Local path to DAS file das_file &lt;- &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39; To follow along, this data file can be downloaded here. You can process your survey data using a single function, process_surveys(), which takes two primary arguments: the filepath(s) to your DAS survey data, and your settings object. For example: cruz &lt;- process_surveys(das_file, settings) That single command will convert your raw DAS data to a “cruz” object, a list of polished datasets that are prepared to be passed to subsequent analyses. In our case we will use a third argument to apply edits to the DAS data before processing (see previous page for details on those edits): edits &lt;- readRDS(&#39;cnp_1986_2020_edits.RData&#39;) cruz &lt;- process_surveys(das_file, settings, edits) Behind the scenes The process_surveys() function is a wrapper for several discrete stages of data formatting/processing. Behind the scenes, each of those stages is carried out using a specific LTabundR function. The remainder of this page is a detailed step-by-step explanation of the data processing that occurs when you call process_surveys(). Edit cruise data If the edits input argument is supplied to process_surveys(), temporary copies of the DAS file(s) are made and edited before processing. This step is discussed on the previous page. Bring in cruise data Read in and process your .DAS file using the functions in Sam Woodward’s swfscDAS package. To do so quickly, we built a wrapper function that makes this quick and easy: das &lt;- das_load(das_file, perform_checks = TRUE, print_glimpse = TRUE) Process strata Run the following function to add strata and study-area information to each row of DAS data: das_strata &lt;- process_strata(das, settings) This function loops through each stratum data.frame you have provided it in settings$strata, formats the stratum, and asks whether each DAS row occurs within it. For each stratum, a column named stratum_&lt;StratumName&gt; is added to the das object; each row in this column is TRUE (included) or FALSE. Format DAS data into a cruz object The function das_format() takes care of some final formatting and initiates the cruz object data structure. cruz &lt;- das_format(das_strata, verbose=TRUE) This function (1) removes rows with invalid Cruise numbers, times, or locations; (ii) calculates the distance, in km, between each row of data; (iii) adds a ship column to the dataset, with initials for the ship corresponding to each cruise; (iv) creates a new list, cohorts, which copies the cruise data for each cohort specified in your settings; and (v) adds a stratum column to the data in each cohort. That column specifies a single stratum assignment for each row of DAS data in the event of overlapping strata, based upon the cohort setting stratum_overlap_handling. The cruz object The function das_format() returns a list, which we have saved in an object named cruz, with several slots: cruz %&gt;% names [1] &quot;settings&quot; &quot;strata&quot; &quot;cohorts&quot; The slots strata and study_area provide the area, in square km, of each polygon being used: cruz$strata stratum area 1 HI_EEZ 2474595.769 2 OtherCNP 34215265.219 3 MHI 212033.063 4 WHICEAS 402948.734 5 Spotted_OU 5102.666 6 Spotted_FI 10509.869 7 Spotted_BI 39454.720 8 Bottlenose_KaNi 2755.024 9 Bottlenose_OUFI 14417.035 10 Bottlenose_BI 4668.072 11 NWHI 449375.569 The slot cohorts is itself a list with one slot for each cohort. The slots are named using the id cohort setting. cruz$cohorts %&gt;% names [1] &quot;all&quot; &quot;bottlenose&quot; &quot;spotted&quot; Each cohort slot has a copy of the DAS data with a new stratum column, which contains a stratum assignment tailored to its cohort-specific settings. For instance, the all cohort, whose stratum_overlap_handling is set to \"smallest\", assigns the smallest stratum in the event of overlapping or nested strata: cruz$cohorts$all$stratum %&gt;% table(useNA=&#39;ifany&#39;) . HI_EEZ OtherCNP WHICEAS 117715 126248 85669 Since the bottlenose cohort uses a different subset of geostrata, its distribution of stratum assignments will also differ: cruz$cohorts$bottlenose$stratum %&gt;% table(useNA=&#39;ifany&#39;) . Bottlenose_BI Bottlenose_KaNi Bottlenose_OUFI HI_EEZ OtherCNP 3415 1495 6862 117715 126248 WHICEAS 73897 This list, with these three primary slots, will be referred to from hereon as a cruz object. Segmentize the data To allocate survey data into discrete ‘effort segments’, which are used in variance estimation in subsequent steps, run the function segmentize(). This process is controlled by both survey-wide and cohort-specific settings, which are now carried in a slot within the cruz object. The process is outlined in detail in the Appendix on Segmentizing. cruz &lt;- segmentize(cruz, verbose=FALSE) This function does not change the high-level structure of the cruz object … cruz %&gt;% names [1] &quot;settings&quot; &quot;strata&quot; &quot;cohorts&quot; … or the cohort names in the cohorts slot: cruz$cohorts %&gt;% names [1] &quot;all&quot; &quot;bottlenose&quot; &quot;spotted&quot; For each cohorts slot, the list structure is the same: cruz$cohorts$all %&gt;% names [1] &quot;segments&quot; &quot;das&quot; cruz$cohorts$bottlenose %&gt;% names [1] &quot;segments&quot; &quot;das&quot; cruz$cohorts$spotted %&gt;% names [1] &quot;segments&quot; &quot;das&quot; The segments slot contains summary data for each effort segment, including start/mid/end coordinates, average conditions, and segment distance: cruz$cohorts$all$segments %&gt;% glimpse Rows: 1,890 Columns: 37 $ Cruise &lt;dbl&gt; 901, 901, 901, 901, 901, 901, 901, 901, 901, 901, 901, 90… $ ship &lt;chr&gt; &quot;Mc2&quot;, &quot;Mc2&quot;, &quot;Mc2&quot;, &quot;Mc2&quot;, &quot;Mc2&quot;, &quot;Mc2&quot;, &quot;Mc2&quot;, &quot;Mc2&quot;, &quot;… $ stratum &lt;chr&gt; &quot;WHICEAS&quot;, &quot;WHICEAS&quot;, &quot;WHICEAS&quot;, &quot;WHICEAS&quot;, &quot;WHICEAS&quot;, &quot;W… $ seg_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… $ yday &lt;dbl&gt; 37, 41, 41, 44, 49, 51, 37, 37, 41, 43, 46, 47, 48, 37, 3… $ dist &lt;dbl&gt; 149.851072, 3.848268, 149.477608, 150.656675, 16.254889, … $ lat1 &lt;dbl&gt; 21.99817, 21.14833, 21.14817, 19.19833, 20.40850, 21.4130… $ lon1 &lt;dbl&gt; -159.1487, -158.0103, -158.0943, -156.0760, -156.1288, -1… $ DateTime1 &lt;dttm&gt; 2009-02-06 07:33:52, 2009-02-10 09:41:09, 2009-02-10 10:… $ timestamp1 &lt;dbl&gt; 1233905632, 1234258869, 1234262132, 1234548006, 123498100… $ lat2 &lt;dbl&gt; 21.14850, 21.14967, 19.19783, 20.40850, 21.41083, 21.6810… $ lon2 &lt;dbl&gt; -158.0097, -158.0918, -156.0758, -156.1288, -157.2680, -1… $ DateTime2 &lt;dttm&gt; 2009-02-10 09:40:54, 2009-02-10 10:33:32, 2009-02-13 17:… $ timestamp2 &lt;dbl&gt; 1234258854, 1234262012, 1234547957, 1234981000, 123514077… $ mlat &lt;dbl&gt; 19.57617, 21.16417, 21.33983, 19.80333, 21.40417, 21.9613… $ mlon &lt;dbl&gt; -156.0153, -158.0875, -160.2627, -155.0388, -157.2693, -1… $ mDateTime &lt;dttm&gt; 2009-02-08 07:57:43, 2009-02-10 10:13:32, 2009-02-11 11:… $ mtimestamp &lt;dbl&gt; 1233905632, 1234258869, 1234262132, 1234548006, 123498100… $ use &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRU… $ Mode &lt;chr&gt; &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C… $ EffType &lt;chr&gt; NA, &quot;N&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;… $ OnEffort &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE… $ ESWsides &lt;dbl&gt; NA, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,… $ year &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 200… $ month &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … $ day &lt;int&gt; 6, 10, 10, 13, 18, 20, 6, 6, 10, 12, 15, 16, 17, 6, 7, 7,… $ min_line &lt;int&gt; 424489, 425732, 425764, 426976, 428376, 428554, 424728, 4… $ max_line &lt;int&gt; 425731, 425763, 426975, 428375, 428553, 429845, 424824, 4… $ n_rows &lt;int&gt; 588, 21, 611, 611, 82, 476, 74, 31, 51, 211, 62, 22, 95, … $ avgBft &lt;dbl&gt; 3.937522, 2.000000, 2.693855, 4.236977, 5.650836, 5.42778… $ avgSwellHght &lt;dbl&gt; 3.665611, 2.000000, 2.990173, 4.295680, 6.000000, 5.91794… $ avgHorizSun &lt;dbl&gt; 5.476773, 7.386038, 5.251324, 7.670550, 7.650836, 4.41183… $ avgVertSun &lt;dbl&gt; 1.510175, 2.806981, 1.581961, 1.660281, 1.000000, 1.56607… $ avgGlare &lt;dbl&gt; 0.01104594, 0.00000000, 0.24616396, 0.11466734, 0.0000000… $ avgVis &lt;dbl&gt; 6.707940, 6.500000, 5.839215, 5.630527, 6.500000, 5.76905… $ avgCourse &lt;dbl&gt; 182.5003, 244.8516, 149.4506, 204.8376, 284.8100, 147.147… $ avgSpdKt &lt;dbl&gt; 8.296386, 7.427145, 9.226427, 9.395206, 9.681004, 9.17516… # Number of segments cruz$cohorts$all$segments %&gt;% nrow [1] 1890 # Segment length distribution x &lt;- cruz$cohorts$all$segments$dist hist(x, breaks = seq(0,ceiling(max(x, na.rm=TRUE)),by=1), xlab=&#39;Segment lengths (km)&#39;, main=paste0(&#39;Target km: &#39;,settings$survey$segment_target_km)) And the das slot holds the original data.frame of DAS data, modified slightly: the column OnEffort has been modified according to Beaufort range conditions, and the column seg_id indicates which segment the event occurs within. cruz$cohorts$all$das %&gt;% names [1] &quot;Event&quot; &quot;DateTime&quot; &quot;Lat&quot; &quot;Lon&quot; [5] &quot;OnEffort&quot; &quot;Cruise&quot; &quot;Mode&quot; &quot;OffsetGMT&quot; [9] &quot;EffType&quot; &quot;ESWsides&quot; &quot;Course&quot; &quot;SpdKt&quot; [13] &quot;Bft&quot; &quot;SwellHght&quot; &quot;WindSpdKt&quot; &quot;RainFog&quot; [17] &quot;HorizSun&quot; &quot;VertSun&quot; &quot;Glare&quot; &quot;Vis&quot; [21] &quot;ObsL&quot; &quot;Rec&quot; &quot;ObsR&quot; &quot;ObsInd&quot; [25] &quot;Data1&quot; &quot;Data2&quot; &quot;Data3&quot; &quot;Data4&quot; [29] &quot;Data5&quot; &quot;Data6&quot; &quot;Data7&quot; &quot;Data8&quot; [33] &quot;Data9&quot; &quot;Data10&quot; &quot;Data11&quot; &quot;Data12&quot; [37] &quot;EffortDot&quot; &quot;EventNum&quot; &quot;file_das&quot; &quot;line_num&quot; [41] &quot;stratum_HI_EEZ&quot; &quot;stratum_OtherCNP&quot; &quot;stratum_WHICEAS&quot; &quot;year&quot; [45] &quot;month&quot; &quot;day&quot; &quot;yday&quot; &quot;km_int&quot; [49] &quot;km_cum&quot; &quot;ship&quot; &quot;stratum&quot; &quot;seg_id&quot; [53] &quot;use&quot; The segmentize() function and its associated settings were designed to give researchers full control over how data are segmented, be it for design-based density analysis (which tend to use long segments of 100 km or more and allow for non-contiguous effort to be included in the same segment) or for habitat modeling (which tend to use short segments of 5 - 10 km and disallow non-contiguous effort to be pooled into the same segment). To demonstrate that versatility, checkout the appendix on segmentizing. Process sightings To process sightings for each cohort of species, use the function process_sightings(). This function has three basic steps: for each cohort, the function (1) prepares a sightings table using the function das_sight() from swfscDAS; (2) filters those sightings to species codes specified for the cohort in your settings input; and (3) evaluates each of those sightings, asking if each should be included in the analysis according to your settings. cruz &lt;- process_sightings(cruz) The function produces a formatted dataset and adds it to a new sightings slot. cruz$cohorts$all %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; cruz$cohorts$bottlenose %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; cruz$cohorts$spotted %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; Note that the sightings table has a column named included (TRUE = yes, use it in the analysis). Any sightings that do not meet the inclusion criteria as specified in your settings will be included = FALSE, but they won’t be removed from the data. The sightings table also has a new column, ss_valid, indicating whether or not the group size estimate for this sighting is valid and appropriate for use in abundance estimation and detection function fitting when group size is used as a covariate. Since the sightings in each cohort are processed slightly differently according to the cohort’s specific settings – most importantly the species that will be included – you should expect different numbers of included/excluded sightings in each cohort dataset: cruz$cohorts$all$sightings$included %&gt;% table . FALSE TRUE 810 3124 cruz$cohorts$bottlenose$sightings$included %&gt;% table . FALSE TRUE 114 409 When this function’s verbose argument is TRUE (the default), a message is printed each time a sighting does not meet the inclusion criteria. Sightings data structure The sightings table has many other variables: cruz$cohorts$all$sightings %&gt;% names [1] &quot;Event&quot; &quot;DateTime&quot; &quot;Lat&quot; &quot;Lon&quot; [5] &quot;OnEffort&quot; &quot;Cruise&quot; &quot;Mode&quot; &quot;OffsetGMT&quot; [9] &quot;EffType&quot; &quot;ESWsides&quot; &quot;Course&quot; &quot;SpdKt&quot; [13] &quot;Bft&quot; &quot;SwellHght&quot; &quot;WindSpdKt&quot; &quot;RainFog&quot; [17] &quot;HorizSun&quot; &quot;VertSun&quot; &quot;Glare&quot; &quot;Vis&quot; [21] &quot;ObsL&quot; &quot;Rec&quot; &quot;ObsR&quot; &quot;ObsInd&quot; [25] &quot;EffortDot&quot; &quot;EventNum&quot; &quot;file_das&quot; &quot;line_num&quot; [29] &quot;stratum_HI_EEZ&quot; &quot;stratum_OtherCNP&quot; &quot;stratum_WHICEAS&quot; &quot;year&quot; [33] &quot;month&quot; &quot;day&quot; &quot;yday&quot; &quot;km_int&quot; [37] &quot;km_cum&quot; &quot;ship&quot; &quot;stratum&quot; &quot;seg_id&quot; [41] &quot;use&quot; &quot;SightNo&quot; &quot;Subgroup&quot; &quot;SightNoDaily&quot; [45] &quot;Obs&quot; &quot;ObsStd&quot; &quot;Bearing&quot; &quot;Reticle&quot; [49] &quot;DistNm&quot; &quot;Cue&quot; &quot;Method&quot; &quot;Photos&quot; [53] &quot;Birds&quot; &quot;CalibSchool&quot; &quot;PhotosAerial&quot; &quot;Biopsy&quot; [57] &quot;CourseSchool&quot; &quot;TurtleSp&quot; &quot;TurtleGs&quot; &quot;TurtleJFR&quot; [61] &quot;TurtleAge&quot; &quot;TurtleCapt&quot; &quot;PinnipedSp&quot; &quot;PinnipedGs&quot; [65] &quot;BoatType&quot; &quot;BoatGs&quot; &quot;PerpDistKm&quot; &quot;species&quot; [69] &quot;best&quot; &quot;low&quot; &quot;high&quot; &quot;prob&quot; [73] &quot;mixed&quot; &quot;ss_tot&quot; &quot;lnsstot&quot; &quot;ss_percent&quot; [77] &quot;n_sp&quot; &quot;n_obs&quot; &quot;n_best&quot; &quot;n_low&quot; [81] &quot;n_high&quot; &quot;calibr&quot; &quot;ss_valid&quot; &quot;mixed_max&quot; [85] &quot;spp_max&quot; &quot;included&quot; Columns 42 onwards correspond to sightings information. Columns of note: species contains the species code. There is only one species-code per row (i.e, multi-species sightings have been expanded to multiple rows). best, low, and high contain the refined group size estimates, averaged across observers and calibrated according to the cohort’s settings specifications. For multi-species sightings, these numbers represent the number of individuals for the single species represented in the row (i.e., the original group size estimate has been scaled by the percentage attritbuted to this species). The columns following those group size estimates (prob through spp_max) detail how group sizes were estimated: prob indicates whether probable species codes were accepted; mixed indicates whether this species’ sighting is part of a mixed-species sighting; n_sp provides the number of species occurring in this sighitng; n_obs gives the number of observers who contributed group size estimates; n_best through n_high gives the number of valid group size estimates given; and calibr indicates whether or not calibration was attempted for this sighting based on the settings (see next section); mixed_max indicates whether this species was the most abundant in the sighting (if multi-species); spp_max indicates the species code for the most abundant species in the sighting (if multi-species). As explained above, the final column, included, indicates whether this species should be included in the analysis. Here is a glimpse of the data: cruz$cohorts$all$sightings %&gt;% glimpse Rows: 3,934 Columns: 86 $ Event &lt;chr&gt; &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;… $ DateTime &lt;dttm&gt; 1986-11-26 09:00:00, 1986-11-26 14:40:00, 1986-11-26… $ Lat &lt;dbl&gt; 4.983333, 5.616667, 5.866667, 7.050000, 7.466667, 9.4… $ Lon &lt;dbl&gt; -120.9500, -121.6667, -121.9833, -123.5000, -123.9167… $ OnEffort &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ Cruise &lt;dbl&gt; 989, 989, 989, 989, 989, 989, 989, 989, 989, 989, 989… $ Mode &lt;chr&gt; &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;… $ OffsetGMT &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ EffType &lt;chr&gt; &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;… $ ESWsides &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,… $ Course &lt;dbl&gt; 310, 316, 313, 310, 310, 305, 305, 305, 305, 305, 305… $ SpdKt &lt;dbl&gt; 10.2, 9.9, 9.6, 10.2, 10.3, 9.8, 9.8, 9.6, 10.1, 10.1… $ Bft &lt;dbl&gt; 4, 4, 4, 3, 3, 2, 2, 2, 1, 1, 1, 4, 5, 4, 4, 3, 1, 2,… $ SwellHght &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ WindSpdKt &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ RainFog &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… $ HorizSun &lt;dbl&gt; 6, 9, 10, NA, 7, 5, 5, 7, 8, 8, 8, 3, 5, NA, 4, NA, N… $ VertSun &lt;dbl&gt; 2, 1, 3, NA, 1, 3, 3, 1, 1, 1, 1, 2, 1, NA, 1, NA, NA… $ Glare &lt;lgl&gt; FALSE, FALSE, FALSE, NA, FALSE, FALSE, FALSE, FALSE, … $ Vis &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ ObsL &lt;chr&gt; &quot;004&quot;, &quot;022&quot;, &quot;056&quot;, &quot;056&quot;, &quot;004&quot;, &quot;031&quot;, &quot;031&quot;, &quot;004… $ Rec &lt;chr&gt; &quot;056&quot;, &quot;031&quot;, &quot;062&quot;, &quot;062&quot;, &quot;056&quot;, &quot;022&quot;, &quot;022&quot;, &quot;056… $ ObsR &lt;chr&gt; &quot;062&quot;, &quot;057&quot;, &quot;004&quot;, &quot;004&quot;, &quot;062&quot;, &quot;057&quot;, &quot;057&quot;, &quot;062… $ ObsInd &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ EffortDot &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ EventNum &lt;chr&gt; &quot;18&quot;, &quot;43&quot;, &quot;59&quot;, &quot;9&quot;, &quot;32&quot;, &quot;10&quot;, &quot;10&quot;, &quot;22&quot;, &quot;34&quot;, … $ file_das &lt;chr&gt; &quot;CenPac1986-2020_Final_alb.das&quot;, &quot;CenPac1986-2020_Fin… $ line_num &lt;int&gt; 10295, 10321, 10340, 10358, 10384, 10451, 10451, 1046… $ stratum_HI_EEZ &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… $ stratum_OtherCNP &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ stratum_WHICEAS &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… $ year &lt;dbl&gt; 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1986,… $ month &lt;dbl&gt; 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1… $ day &lt;int&gt; 26, 26, 26, 27, 27, 28, 28, 28, 28, 28, 28, 29, 29, 3… $ yday &lt;dbl&gt; 330, 330, 330, 331, 331, 332, 332, 332, 332, 332, 332… $ km_int &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… $ km_cum &lt;dbl&gt; 64.38787, 172.92124, 221.24918, 248.23899, 317.83407,… $ ship &lt;chr&gt; &quot;DSJ&quot;, &quot;DSJ&quot;, &quot;DSJ&quot;, &quot;DSJ&quot;, &quot;DSJ&quot;, &quot;DSJ&quot;, &quot;DSJ&quot;, &quot;DSJ… $ stratum &lt;chr&gt; &quot;OtherCNP&quot;, &quot;OtherCNP&quot;, &quot;OtherCNP&quot;, &quot;OtherCNP&quot;, &quot;Othe… $ seg_id &lt;int&gt; 34, 35, 35, 35, 36, 37, 37, 37, 37, 37, 29, 38, 38, 3… $ use &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ SightNo &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;01&quot;, &quot;02&quot;, &quot;01&quot;, &quot;01&quot;, &quot;02&quot;, &quot;03&quot;,… $ Subgroup &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ SightNoDaily &lt;chr&gt; &quot;19861126_1&quot;, &quot;19861126_2&quot;, &quot;19861126_3&quot;, &quot;19861127_1… $ Obs &lt;chr&gt; &quot;004&quot;, &quot;057&quot;, &quot;004&quot;, &quot;056&quot;, &quot;004&quot;, &quot;031&quot;, &quot;031&quot;, &quot;004… $ ObsStd &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ Bearing &lt;dbl&gt; 335, 354, 336, 18, 332, 333, 333, 6, 312, 312, 38, 80… $ Reticle &lt;dbl&gt; 8.41, 3.88, 0.50, 11.40, 0.80, 0.30, 0.30, 2.98, 0.40… $ DistNm &lt;dbl&gt; 0.4, 0.8, 3.2, 0.3, 2.5, 3.8, 3.8, 1.0, 3.5, 3.5, 7.0… $ Cue &lt;dbl&gt; 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 6, 6, 3, 2, 3, 2,… $ Method &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,… $ Photos &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ Birds &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;N&quot;… $ CalibSchool &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ PhotosAerial &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ Biopsy &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ CourseSchool &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleSp &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleGs &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleJFR &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleAge &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleCapt &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ PinnipedSp &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ PinnipedGs &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ BoatType &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ BoatGs &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ PerpDistKm &lt;dbl&gt; 0.3130756, 0.1548694, 2.4104840, 0.1716898, 2.1736533… $ species &lt;chr&gt; &quot;049&quot;, &quot;015&quot;, &quot;077&quot;, &quot;002&quot;, &quot;002&quot;, &quot;033&quot;, &quot;018&quot;, &quot;037… $ best &lt;dbl&gt; 2.318841, 8.843199, 4.637681, 27.517262, 21.826776, 3… $ low &lt;dbl&gt; 2.000000, 6.253644, 4.637681, 17.139473, 14.144886, 1… $ high &lt;dbl&gt; 2.000000, 10.711798, NA, 35.140620, 27.788449, 45.560… $ prob &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… $ mixed &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE,… $ ss_tot &lt;dbl&gt; 2.318841, 8.843199, 4.637681, 27.517262, 21.826776, 3… $ lnsstot &lt;dbl&gt; 0.8410673, 2.1796487, 1.5342145, 3.3148135, 3.0831375… $ ss_percent &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000… $ n_sp &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,… $ n_obs &lt;int&gt; 1, 3, 1, 3, 3, 3, 3, 5, 5, 5, 1, 1, 3, 4, 2, 2, 2, 5,… $ n_best &lt;int&gt; 1, 2, 0, 3, 3, 1, 1, 4, 5, 5, 0, 1, 3, 3, 1, 2, 2, 4,… $ n_low &lt;int&gt; 1, 3, 1, 3, 3, 3, 3, 5, 5, 5, 1, 1, 3, 4, 2, 2, 2, 5,… $ n_high &lt;int&gt; 1, 2, 0, 3, 3, 1, 1, 4, 5, 5, 0, 1, 3, 3, 1, 2, 2, 4,… $ calibr &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ ss_valid &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ mixed_max &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, FALS… $ spp_max &lt;chr&gt; &quot;049&quot;, &quot;015&quot;, &quot;077&quot;, &quot;002&quot;, &quot;002&quot;, &quot;033&quot;, &quot;033&quot;, &quot;037… $ included &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… Note that the process_sightings() function draws upon cruz$settings for inclusion criteria, but some of those settings can be overridden with the function’s manual inputs if you want to explore your options (see below). Group size estimates In the settings we are using in this tutorial, group size estimates are adjusted using the calibration models from Barlow et al. (1998) (their analysis is refined slightly and further explained in Gerrodette et al. (2002)). These calibration corrections are observer-specific. Most observers tend to underestimate group size and their estimates are adjusted up; others tend to overestimate and their estimates are adjusted down. Some observers do not have calibration coefficients, and for them a generic adjustment (upwards, by dividing estimates by 0.8625) is used. In LTabundR, each observer’s estimate is calibrated, then all observer estimates are averaged. To do that averaging, our settings specify that we shall use a geometric weighted mean, instead of an arithmetic mean, that weights school size estimates from multiple observers according to the variance of their calibration coefficients. Here are our current best estimates of group size: cruz$cohorts$all$sightings$best %&gt;% head(20) [1] 2.318841 8.843199 4.637681 27.517262 21.826776 31.713333 [7] 3.786667 3.478261 21.284389 221.965766 2.318841 1.159420 [13] 13.758964 6.242983 16.940000 18.247174 1.159420 38.004297 [19] 35.000000 16.596526 Let’s compare those estimates to unadjusted ones, in which calibration (and therefore weighted geometric mean) is turned off: cruz_demo &lt;- process_sightings(cruz, calibrate = FALSE, verbose = FALSE) cruz_demo$cohorts$all$sightings$best %&gt;% head(20) [1] 2.000000 8.485281 4.000000 21.897596 16.570558 23.226667 [7] 2.773333 3.000000 20.885620 217.807182 2.000000 1.000000 [13] 11.744603 5.517848 12.000000 15.000000 1.000000 38.985490 [19] 35.000000 14.642958 You can also carry out calibration corrections without using a geometric weighted mean (the arithmetic mean will be used instead): cruz_demo &lt;- process_sightings(cruz, calibrate = TRUE, geometric_mean = FALSE, verbose = FALSE) cruz_demo$cohorts$all$sightings$best %&gt;% head(20) [1] 2.318841 9.217391 4.637681 27.866184 22.455556 31.713333 [7] 3.786667 3.478261 24.139715 251.742745 2.318841 1.159420 [13] 13.744928 6.198068 16.940000 18.095652 1.159420 46.792494 [19] 35.000000 17.131014 Note that when geometric_mean = TRUE but calibration is not carried out, the simple geometric mean is calculated instead of the weighted geometric mean, since the weights are the variance estimates from the calibration routine. Also note that group size calibration is only carried out if settings$group_size_calibration is not NULL. However, even when calibration coefficients are provided, it is possible to specify that calibration should only be carried out for raw estimates above a minimum threshold (see cohort setting calibration_floor, whose default is 0), since observers may be unlikely to mis-estimate the group size of a lone whale or pair. For observers who have calibration coefficients in the settings$group_size_coefficients table, that minimum is specified for each observer individually. For observers not in that table, calibration will only be applied to raw group size estimates above settings$cohorts[[i]]$calibration_floor or above. Subgroup size estimates After sightings data are processed, the process_surveys() function calls the subroutine process_subgroups() to find and calculate subgroup group size estimates for false killer whales (or other species that may have been recorded using the subgroup functionality in WinCruz), if any occur in the DAS data (Event code “G”). cruz &lt;- process_subgroups(cruz) If subgroups are found, a subgroups slot is added to the analysis list for a cohort. cruz$cohorts$all %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; &quot;subgroups&quot; This subgroups slot holds a list with three dataframes: events (each row is a group size estimate for a single subgroup during a single phase of the false killer whale protocol (if applicable) within a single sighting; this is effectively the raw data); subgroups (each row is a single subgroup for a single protocol phase, with all group size estimates averaged together (both arithmetically and geometrically); and sightings (each row is a “group” size estimate for a single sighting during a single protocol, with all subgroup group sizes summed together). Note for false killer whales this “group” size estimate is not likely to represent actual group size because groups can be spread out over tens of kilometers, and it is not expected that every subgroup is detected during each protocol phase. cruz$cohorts$all$subgroups %&gt;% names [1] &quot;sightings&quot; &quot;subgroups&quot; &quot;events&quot; For a detailed example, see the vignette page on subgroup analysis using data on Central North Pacific false killer whales. Review By the end of this process, you have a single data object, cruz, with all the data you need to move forward into the next stages of mapping and analysis. The LTabundR function cruz_structure() provides a synopsis of the data structure: cruz_structure(cruz) &quot;cruz&quot; list structure ======================== $settings $strata --- with 11 polygon coordinate sets $survey --- with 10 input arguments $cohorts --- with 3 cohorts specified, each with 19 input arguments $strata ... containing a summary dataframe of 11 geostrata and their spatial areas ... geostratum names: HI_EEZ, OtherCNP, MHI, WHICEAS, Spotted_OU, Spotted_FI, Spotted_BI, Bottlenose_KaNi, Bottlenose_OUFI, Bottlenose_BI, NWHI $cohorts $all geostrata: WHICEAS, HI_EEZ, OtherCNP $segments --- with 1890 segments (median = 149.3 km) $das --- with 329632 data rows $sightings --- with 3934 detections $subgroups --- with 255 subgroups, 49 sightings, and 389 events $bottlenose geostrata: WHICEAS, HI_EEZ, OtherCNP, Bottlenose_BI, Bottlenose_OUFI, Bottlenose_KaNi $segments --- with 2049 segments (median = 148.6 km) $das --- with 329632 data rows $sightings --- with 523 detections $spotted geostrata: WHICEAS, HI_EEZ, OtherCNP, Spotted_OU, Spotted_FI, Spotted_BI $segments --- with 2057 segments (median = 148.5 km) $das --- with 329632 data rows $sightings --- with 527 detections Each species-specific cohort has its own list under cruz$cohorts, and each of these cohorts has the same list structure: segments is a summary table of segments. das is the raw DAS data, modified with seg_id to associate each row with a segment. sightings is a dataframe of sightings processed according to this cohort’s settings. subgroups (if any subgroup data exist in your survey) is a list with subgroup details. In each of these data.frame’s, there are three critically important columns to keep in mind: seg_id: this column is used to indicate the segment ID that a row of data belongs to. use: this column indicates whether a row of effort should be used in the line-transect analysis. Every row of data within a single segment will have the same use value. included: this column occurs in the sightings dataframe only. It indicates whether the sightings should be included in line-transect analysis based on the specified settings. Any sighting with use == FALSE will also have included == FALSE, but it is possible for sightings to have use == TRUE with included == FALSE. For example, if the setting abeam_sightings is set to FALSE, a sighting with a bearing angle beyond the ship’s beam can be excluded from the analysis (included == FALSE) even though the effort segment it occurs within will still be used (use == TRUE). Finally, let’s save this cruz object locally, to use in downstream scripts: save(cruz, file=&#39;whiceas_cruz.RData&#39;) Validation To validate these LTabundR functions, we can compare its output to that of ABUND9, written by Jay Barlow (NOAA Fisheries). First, we bring in the ABUND9 output files for the same DAS data: # Local paths to these files SIGHTS &lt;- read.csv(&#39;data/SIGHTS.csv&#39;) EFFORT &lt;- read.csv(&#39;data/EFFORT.csv&#39;) You may download these files here: SIGHTS.csv and EFFORT.csv. Sightings Pivot and format the ABUND SIGHTS data… abund &lt;- SIGHTS %&gt;% tidyr::pivot_longer(cols = 31:101, names_to = &#39;species&#39;, values_to = &#39;best&#39;) %&gt;% filter(best &gt; 0) %&gt;% mutate(Region = gsub(&#39; &#39;,&#39;&#39;,Region)) %&gt;% mutate(DateTime = paste0(Yr,&#39;-&#39;,Mo,&#39;-&#39;,Da,&#39; &#39;,Hr,&#39;:&#39;,Min)) …then summarize counts of species within each cruise: abund_summ &lt;- abund %&gt;% group_by(cruise = CruzNo, species) %&gt;% summarize(ntot_abund = n(), nsys_abund = length(which(! Region %in% c(&#39;NONE&#39;, &#39;Off-Transect&#39;) &amp; EffortSeg &gt; 0))) %&gt;% mutate(species = gsub(&#39;SP&#39;,&#39;&#39;,species)) Then do the same for LTabundR output: ltabundr &lt;- cruz$cohorts$all$sightings %&gt;% # Filter out species that ABUND ignored based on its INP file filter(!species %in% c(&#39;CU&#39;, &#39;PU&#39;)) ltabundr_summ &lt;- ltabundr %&gt;% filter(OnEffort == TRUE) %&gt;% group_by(cruise = Cruise, species) %&gt;% summarize(ntot_ltabundr = n(), nsys_ltabundr = length(which(included == TRUE &amp; EffType %in% c(&#39;S&#39;,&#39;F&#39;)))) Now join these two datasets by cruise and species code: mr &lt;- full_join(abund_summ, ltabundr_summ, by=c(&#39;cruise&#39;, &#39;species&#39;)) mr %&gt;% head # A tibble: 6 × 6 # Groups: cruise [1] cruise species ntot_abund nsys_abund ntot_ltabundr nsys_ltabundr &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 901 002 3 2 3 2 2 901 013 4 4 4 4 3 901 015 2 1 2 1 4 901 018 2 1 2 1 5 901 031 1 1 1 1 6 901 032 2 0 2 0 Compare the total On-Effort sightings in both outputs: mr$ntot_abund %&gt;% sum(na.rm=TRUE) [1] 3223 mr$ntot_ltabundr %&gt;% sum(na.rm=TRUE) [1] 3225 Compare total sightings valid for use in density estimation (EffType \"S\" or \"F\" only, as well as other criteria such as Bft 0 - 6): mr$nsys_abund %&gt;% sum(na.rm=TRUE) [1] 2478 mr$nsys_ltabundr %&gt;% sum(na.rm=TRUE) [1] 2474 Let’s find the rows with discrepancies in sighting counts: bads &lt;- which(mr$nsys_abund != mr$nsys_ltabundr | mr$ntot_abund != mr$ntot_ltabundr | is.na(mr$ntot_abund) | is.na(mr$ntot_ltabundr) | is.na(mr$nsys_abund) | is.na(mr$nsys_ltabundr)) bads %&gt;% length [1] 9 Let’s look at those rows in the joined dataframe: mr[bads, ] # A tibble: 9 × 6 # Groups: cruise [6] cruise species ntot_abund nsys_abund ntot_ltabundr nsys_ltabundr &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 1004 002 3 2 2 1 2 1004 046 4 4 3 3 3 1203 015 1 0 2 0 4 1203 049 1 1 2 1 5 1607 022 8 8 8 7 6 1607 037 3 3 3 2 7 1621 015 7 7 7 6 8 1165 047 NA NA 1 1 9 1631 003 NA NA 1 0 To investigate these 4 discrepancies, we will write a helper function that returns sightings details from both outputs for a given cruise-species: sight_compare &lt;- function(abund, ltabundr, cruise, spp){ message(&#39;ABUND:&#39;) abund %&gt;% filter(CruzNo == cruise, species == paste0(&#39;SP&#39;,spp)) %&gt;% select(5, 34, 26, 29, 33, 3) %&gt;% mutate(use_sit = EffortSeg != 0) %&gt;% select(-EffortSeg) %&gt;% arrange(desc(use_sit)) %&gt;% print abund %&gt;% filter(CruzNo == 1631) %&gt;% pull(species) %&gt;% table message(&#39;\\nLTabundR:&#39;) ltabundr %&gt;% filter(Cruise == cruise, species == spp) %&gt;% select(6, 2, 13, 73, 69, 5, 9, 41, 86) %&gt;% rename(use_sit = included, use_effort = use) %&gt;% arrange(desc(use_effort)) %&gt;% tibble %&gt;% print } Discrepancies Cruise 1203, Species 015 In this case, LTabundR has a non-systematic sighting that ABUND has ignored. sight_compare(abund, ltabundr, 1203, &#39;015&#39;) ABUND: # A tibble: 1 × 6 CruzNo DateTime Beauf Mixed best use_sit &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; 1 1203 2012-5-16 11:58 4 &quot; F&quot; 74.9 TRUE LTabundR: # A tibble: 2 × 9 Cruise DateTime Bft mixed best OnEffort EffType use_effort &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;lgl&gt; 1 1203 2012-05-13 13:19:44 5 TRUE 1 TRUE N TRUE 2 1203 2012-05-16 11:58:30 4 FALSE 74.9 TRUE N TRUE # ℹ 1 more variable: use_sit &lt;lgl&gt; Looking at the sighting details from LTabundR … (ltabundr %&gt;% filter(Cruise == 1203, species == &#39;015&#39;))[1,] Event DateTime Lat Lon OnEffort Cruise Mode OffsetGMT 1 S 2012-05-13 13:19:44 11.96733 -161.1727 TRUE 1203 C -10 EffType ESWsides Course SpdKt Bft SwellHght WindSpdKt RainFog HorizSun 1 N 2 18 9.2 5 6 21 5 12 VertSun Glare Vis ObsL Rec ObsR ObsInd EffortDot EventNum 1 12 FALSE 6 073 235 280 &lt;NA&gt; TRUE 268 file_das line_num stratum_HI_EEZ stratum_OtherCNP 1 CenPac1986-2020_Final_alb.das 493021 FALSE TRUE stratum_WHICEAS year month day yday km_int km_cum ship stratum seg_id use 1 FALSE 2012 5 13 134 0 171109.1 OES OtherCNP 276 TRUE SightNo Subgroup SightNoDaily Obs ObsStd Bearing Reticle DistNm Cue Method 1 069 &lt;NA&gt; 20120513_123 235 TRUE 90 NA 0.1 3 1 Photos Birds CalibSchool PhotosAerial Biopsy CourseSchool TurtleSp TurtleGs 1 Y Y N N N NA &lt;NA&gt; NA TurtleJFR TurtleAge TurtleCapt PinnipedSp PinnipedGs BoatType BoatGs 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA &lt;NA&gt; NA PerpDistKm species best low high prob mixed ss_tot lnsstot ss_percent n_sp 1 0.1852 015 1 NaN NaN FALSE TRUE 6.956522 1.93968 NaN 2 n_obs n_best n_low n_high calibr ss_valid mixed_max spp_max included 1 1 1 1 1 TRUE FALSE FALSE &lt;NA&gt; TRUE According to ABUND, this sighting is not mixed-species, but LTabundR says it is. Looking at the raw DAS… das_file &lt;- &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39; das &lt;- das_readtext(das_file) i &lt;- which(substr(das$das, 6, 18) == &#39;131944 051312&#39;) das$das[i] [1] &quot;268S.131944 051312 N11:58.04 W161:10.36 069 235 3 1 090 0.10 N N N&quot; [2] &quot;269A.131944 051312 N11:58.04 W161:10.36 069 Y Y 033 015 &quot; [3] &quot;269C.131944 051312 N11:58.04 W161:10.36 Overall estimate for full group- never saw all at once. -EMO&quot; [4] &quot;268G.131944 051312 N11:58.04 W161:10.36 069 A 235 1 090 0.10 &quot; [5] &quot;269A.131944 051312 N11:58.04 W161:10.36 069 Y Y 033 015&quot; [6] &quot;269C.131944 051312 N11:58.04 W161:10.36 begin PC protool, first sighting is subgroup &#39;A&#39;, acoustics already tracking &quot; [7] &quot;269C.131944 051312 N11:58.04 W161:10.36 photos taken during sighting indicate Steno (015) present. Not seen or estimated during sighting. -EMO&quot; We see that this was a sighting of false killer whales during which species 015 was picked up during photo-ID. In the absence of a percent composition estimate for this sighting, it was ignored by ABUND. Cruise 1203, Species 049 In this case, LTabundR has a non-systematic sighting that ABUND has ignored. sight_compare(abund, ltabundr, 1203, &#39;049&#39;) ABUND: # A tibble: 1 × 6 CruzNo DateTime Beauf Mixed best use_sit &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; 1 1203 2012-5-3 14:21 6 &quot; F&quot; 1.16 TRUE LTabundR: # A tibble: 2 × 9 Cruise DateTime Bft mixed best OnEffort EffType use_effort &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;lgl&gt; 1 1203 2012-05-03 14:21:40 6 FALSE 1.16 TRUE S TRUE 2 1203 2012-05-07 10:54:09 7 FALSE 1 TRUE N FALSE # ℹ 1 more variable: use_sit &lt;lgl&gt; Looking at the sighting details from LTabundR … (ltabundr %&gt;% filter(Cruise == 1203, species == &#39;049&#39;))[2,] Event DateTime Lat Lon OnEffort Cruise Mode OffsetGMT 2 S 2012-05-07 10:54:09 5.960333 -162.1255 TRUE 1203 C -10 EffType ESWsides Course SpdKt Bft SwellHght WindSpdKt RainFog HorizSun 2 N 2 269 8.8 7 9 30 5 5 VertSun Glare Vis ObsL Rec ObsR ObsInd EffortDot EventNum 2 1 FALSE 4.5 238 328 073 &lt;NA&gt; TRUE 145 file_das line_num stratum_HI_EEZ stratum_OtherCNP 2 CenPac1986-2020_Final_alb.das 489954 FALSE TRUE stratum_WHICEAS year month day yday km_int km_cum ship stratum seg_id 2 FALSE 2012 5 7 128 0 170008.1 OES OtherCNP 268 use SightNo Subgroup SightNoDaily Obs ObsStd Bearing Reticle DistNm Cue 2 FALSE 052 &lt;NA&gt; 20120507_103 238 TRUE 310 10 0.52 3 Method Photos Birds CalibSchool PhotosAerial Biopsy CourseSchool TurtleSp 2 4 N N N N N NA &lt;NA&gt; TurtleGs TurtleJFR TurtleAge TurtleCapt PinnipedSp PinnipedGs BoatType BoatGs 2 NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA &lt;NA&gt; NA PerpDistKm species best low high prob mixed ss_tot lnsstot ss_percent n_sp 2 0.7377314 049 1 1 1 FALSE FALSE 1 0 1 1 n_obs n_best n_low n_high calibr ss_valid mixed_max spp_max included 2 1 1 1 1 TRUE TRUE TRUE 049 FALSE This is a sighting of a single Ziphiid whale. It appears to be squarely within the geostratum: cruzi &lt;- filter_cruz(cruz, spp=&#39;047&#39;, years = 1988) map_cruz(cruzi) Loooking at the raw DAS data … i &lt;- which(substr(das$das, 6, 18) == &#39;105409 050712&#39;) das$das[(i[1] - 10):(i[1] + 3)] [1] &quot;135V.104143 050712 N05:57.61 W162:05.66 7 09 070 30.0&quot; [2] &quot;136N.104143 050712 N05:57.61 W162:05.66 272 09.3&quot; [3] &quot;137W.104143 050712 N05:57.61 W162:05.66 5 05 01 050 4.5&quot; [4] &quot;138*.104234 050712 N05:57.62 W162:05.79&quot; [5] &quot;139*.104434 050712 N05:57.62 W162:06.09&quot; [6] &quot;140*.104634 050712 N05:57.63 W162:06.39&quot; [7] &quot;141N.104713 050712 N05:57.63 W162:06.49 269 08.8&quot; [8] &quot;142*.104834 050712 N05:57.63 W162:06.69&quot; [9] &quot;143*.105034 050712 N05:57.63 W162:06.99&quot; [10] &quot;144*.105234 050712 N05:57.63 W162:07.29&quot; [11] &quot;145S.105409 050712 N05:57.62 W162:07.53 052 238 3 4 310 10.0 0.52 N N N&quot; [12] &quot;146A.105409 050712 N05:57.62 W162:07.53 052 N N 049 &quot; [13] &quot; 1 238 1 1 1 100&quot; [14] &quot;147*.105434 050712 N05:57.62 W162:07.59&quot; It is not clear why ABUND did not include this sighting as non-systematic. Cruise 1165, Species 047 In this case, LTabundR has a systematic sighting of a pygmy sperm whale, whereas ABUND does not have anything. sight_compare(abund, ltabundr, 1165, &#39;047&#39;) ABUND: # A tibble: 0 × 6 # ℹ 6 variables: CruzNo &lt;int&gt;, DateTime &lt;chr&gt;, Beauf &lt;int&gt;, Mixed &lt;chr&gt;, # best &lt;dbl&gt;, use_sit &lt;lgl&gt; LTabundR: # A tibble: 1 × 9 Cruise DateTime Bft mixed best OnEffort EffType use_effort &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;lgl&gt; 1 1165 1988-07-30 19:28:00 0 FALSE 1.16 TRUE S TRUE # ℹ 1 more variable: use_sit &lt;lgl&gt; (ltabundr %&gt;% filter(Cruise == 1165, species == &#39;047&#39;))[1,] Event DateTime Lat Lon OnEffort Cruise Mode OffsetGMT 1 S 1988-07-30 19:28:00 26.3 -121.1167 TRUE 1165 C NA EffType ESWsides Course SpdKt Bft SwellHght WindSpdKt RainFog HorizSun 1 S 2 163 10.5 0 NA NA 1 NA VertSun Glare Vis ObsL Rec ObsR ObsInd EffortDot EventNum 1 NA NA NA 038 068 051 &lt;NA&gt; TRUE 92 file_das line_num stratum_HI_EEZ stratum_OtherCNP 1 CenPac1986-2020_Final_alb.das 50173 FALSE TRUE stratum_WHICEAS year month day yday km_int km_cum ship stratum seg_id use 1 FALSE 1988 7 30 212 0 13618.39 MAC OtherCNP 237 TRUE SightNo Subgroup SightNoDaily Obs ObsStd Bearing Reticle DistNm Cue Method 1 07 &lt;NA&gt; 19880730_9 051 TRUE 45 3.38 0.9 3 4 Photos Birds CalibSchool PhotosAerial Biopsy CourseSchool TurtleSp TurtleGs 1 &lt;NA&gt; N &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA &lt;NA&gt; NA TurtleJFR TurtleAge TurtleCapt PinnipedSp PinnipedGs BoatType BoatGs 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA &lt;NA&gt; NA PerpDistKm species best low high prob mixed ss_tot lnsstot 1 1.178606 047 1.15942 1 1.259921 FALSE FALSE 1.15942 0.1479201 ss_percent n_sp n_obs n_best n_low n_high calibr ss_valid mixed_max spp_max 1 1 1 3 3 3 3 TRUE TRUE TRUE 047 included 1 TRUE To investigate this sighting, we can filter our cruz object and take a look at a map of this sighting: cruzi &lt;- filter_cruz(cruz, spp=&#39;047&#39;, years = 1988) map_cruz(cruzi) Using that map we see that this sighting occurred just inside of the OtherCNP geostratum. It is likely that the point-in-polygon subroutines inside ABUND9 decided that this sighting was out of the study area, and therefore excluded it. The subroutines used by LTabundR, which are based in the R package sf, should not be wrong in this case. Cruise 1631, Species 003 In this case, there was a non-systematic sighting of species 003 that was found by LTabundR but not by ABUND. sight_compare(abund, ltabundr, 1631, &#39;003&#39;) ABUND: # A tibble: 0 × 6 # ℹ 6 variables: CruzNo &lt;int&gt;, DateTime &lt;chr&gt;, Beauf &lt;int&gt;, Mixed &lt;chr&gt;, # best &lt;dbl&gt;, use_sit &lt;lgl&gt; LTabundR: # A tibble: 1 × 9 Cruise DateTime Bft mixed best OnEffort EffType use_effort &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;lgl&gt; 1 1631 2006-09-02 07:35:14 5 TRUE 1 TRUE N TRUE # ℹ 1 more variable: use_sit &lt;lgl&gt; (ltabundr %&gt;% filter(Cruise == 1631, species == &#39;003&#39;)) Event DateTime Lat Lon OnEffort Cruise Mode OffsetGMT 1 S 2006-09-02 07:35:14 19.28883 -156.8227 TRUE 1631 P 10 EffType ESWsides Course SpdKt Bft SwellHght WindSpdKt RainFog HorizSun 1 N 2 155 10 5 4 18 1 10 VertSun Glare Vis ObsL Rec ObsR ObsInd EffortDot EventNum 1 2 FALSE 7 073 196 197 &lt;NA&gt; TRUE 026 file_das line_num stratum_HI_EEZ stratum_OtherCNP 1 CenPac1986-2020_Final_alb.das 405216 TRUE TRUE stratum_WHICEAS year month day yday km_int km_cum ship stratum seg_id use 1 TRUE 2006 9 2 245 0 124035.2 Mc2 WHICEAS 1275 TRUE SightNo Subgroup SightNoDaily Obs ObsStd Bearing Reticle DistNm Cue Method 1 090 &lt;NA&gt; 20060902_38 197 TRUE 59 0.3 4.12 2 4 Photos Birds CalibSchool PhotosAerial Biopsy CourseSchool TurtleSp TurtleGs 1 N N &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA &lt;NA&gt; NA TurtleJFR TurtleAge TurtleCapt PinnipedSp PinnipedGs BoatType BoatGs 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA &lt;NA&gt; NA PerpDistKm species best low high prob mixed ss_tot lnsstot ss_percent 1 6.540392 003 1 NaN NA FALSE TRUE 3.478261 1.246532 NaN n_sp n_obs n_best n_low n_high calibr ss_valid mixed_max spp_max included 1 2 1 0 1 0 TRUE FALSE FALSE &lt;NA&gt; TRUE The map indicates that this is not a geostratum boundary issue: cruzi &lt;- filter_cruz(cruz, spp=&#39;003&#39;, years = 2006) map_cruz(cruzi) Looking at the raw DAS data … das$das[405190:405220] [1] &quot; C 120000 090106 In transit to study area Sep 1. A.J. 10/18/06.&quot; [2] &quot;001B.061813 090206 N19:28.97 W156:55.51 1631 p 10 Y&quot; [3] &quot;002R.061813 090206 N19:28.97 W156:55.51 N&quot; [4] &quot;003P.061813 090206 N19:28.97 W156:55.51 197 007 277&quot; [5] &quot;004V.061813 090206 N19:28.97 W156:55.51 5 04 150 18.0&quot; [6] &quot;005N.061813 090206 N19:28.97 W156:55.51 154 10.4&quot; [7] &quot;006W.061813 090206 N19:28.97 W156:55.51 1 10 03 032 7.0&quot; [8] &quot;007C.061823 090206 N19:28.94 W156:55.50&quot; [9] &quot;008*.062019 090206 N19:28.63 W156:55.33&quot; [10] &quot;009*.063019 090206 N19:27.12 W156:54.53&quot; [11] &quot;010P.063950 090206 N19:25.68 W156:53.75 196 197 007&quot; [12] &quot;011C.063950 090206 N19:25.68 W156:53.75&quot; [13] &quot;012V.063950 090206 N19:25.68 W156:53.75 5 04 150 18.0&quot; [14] &quot;013N.063950 090206 N19:25.68 W156:53.75 151 10.1&quot; [15] &quot;014W.063950 090206 N19:25.68 W156:53.75 1 10 03 032 7.0&quot; [16] &quot;015*.064019 090206 N19:25.61 W156:53.71&quot; [17] &quot;016N.064253 090206 N19:25.22 W156:53.50 153 10.3&quot; [18] &quot;017*.065019 090206 N19:24.11 W156:52.90&quot; [19] &quot;018*.070019 090206 N19:22.62 W156:52.11&quot; [20] &quot;019*.071019 090206 N19:21.10 W156:51.30&quot; [21] &quot;020*.072019 090206 N19:19.59 W156:50.53&quot; [22] &quot;021P.072113 090206 N19:19.46 W156:50.45 073 196 197&quot; [23] &quot;022V.072113 090206 N19:19.46 W156:50.45 5 04 150 18.0&quot; [24] &quot;023N.072113 090206 N19:19.46 W156:50.45 155 10.0&quot; [25] &quot;024W.072113 090206 N19:19.46 W156:50.45 1 10 02 032 7.0&quot; [26] &quot;025*.073019 090206 N19:18.09 W156:49.75&quot; [27] &quot;026S.073514 090206 N19:17.33 W156:49.36 090 197 2 4 059 0.3 4.12 013&quot; [28] &quot;027A.073514 090206 N19:17.33 W156:49.36 090 N N 177 003&quot; [29] &quot; 1 197 3&quot; [30] &quot;028*.074019 090206 N19:16.53 W156:48.94&quot; [31] &quot;029V.074337 090206 N19:16.01 W156:48.67 4 04 150 12.0&quot; It appears that this was a multi-species sighting, but no species percentages. In the absence of a percent composition estimate for this sighting, it was ignored by ABUND. Group sizes # Format ABUND abund &lt;- SIGHTS %&gt;% tidyr::pivot_longer(cols = 31:101, names_to = &#39;species&#39;, values_to = &#39;best&#39;) %&gt;% mutate(Region = gsub(&#39; &#39;,&#39;&#39;,Region)) %&gt;% filter(best &gt; 0, ! Region %in% c(&#39;NONE&#39;, &#39;Off-Transect&#39;), EffortSeg &gt; 0) %&gt;% select(Cruise = CruzNo, TotSS, LnTotSS, species, best) %&gt;% mutate(Software=&#39;ABUND9&#39;) # Format LTabundR ltabundr &lt;- cruz$cohorts$all$sightings %&gt;% filter(OnEffort == TRUE, included == TRUE, EffType %in% c(&#39;S&#39;, &#39;F&#39;)) %&gt;% select(Cruise, TotSS = ss_tot, LnTotSS = lnsstot, species, best) %&gt;% mutate(Software=&#39;LTabundR&#39;) # Combine the datasets ss &lt;- rbind(abund, ltabundr) # Plot the datasets ggplot(ss, aes(x=best, y=factor(Cruise), col=Software, pch=Software)) + geom_point(position=ggstance::position_dodgev(height=0.5), alpha=.6) + scale_x_continuous(trans=&#39;log&#39;, breaks=c(1,2, 5,10,25,50,100,500,1000,2500,5000)) + xlab(&#39;log Estimated School Size&#39;) + ylab(&#39;Cruise&#39;) + theme_light() Effort # Format ABUND abund &lt;- EFFORT %&gt;% mutate(Region = gsub(&#39; &#39;,&#39;&#39;,Region)) %&gt;% mutate(dt = paste0(Yr, stringr::str_pad(Mo, width=2, pad=&#39;0&#39;, side=&#39;left&#39;), stringr::str_pad(Da, width=2, pad=&#39;0&#39;, side=&#39;left&#39;))) %&gt;% filter(! Region %in% c(&#39;NONE&#39;, &#39;Off-Transect&#39;), EffortSeg &gt; 0) %&gt;% select(Cruise = CruzNo, Date = dt, km = length) %&gt;% mutate(software = &#39;ABUND9&#39;) # Format LTabundR ltabundr &lt;- cruz$cohorts$all$segments %&gt;% mutate(dt = gsub(&#39;-&#39;,&#39;&#39;,substr(DateTime1, 1,10))) %&gt;% filter(OnEffort == TRUE, use == TRUE, EffType %in% c(&#39;S&#39;, &#39;F&#39;)) %&gt;% select(Cruise, Date = dt, km = dist) %&gt;% mutate(software = &#39;LTabundR&#39;) # Join the datasets eff &lt;- rbind(abund, ltabundr) %&gt;% group_by(Cruise) %&gt;% mutate(km = round(km)) %&gt;% summarize(km_abund = sum(km[software == &#39;ABUND9&#39;]), km_ltabundr = sum(km[software == &#39;LTabundR&#39;])) # Plot p &lt;- ggplot(eff, aes(x = km_abund, y = km_ltabundr, col=factor(Cruise))) + geom_abline(slope=1, intercept=0, lty=3) + geom_point(alpha=.6) + ylab(&#39;LTabundR&#39;) + xlab(&#39;ABUND9&#39;) + scale_x_continuous(breaks = seq(0, 15000, by=2500)) + scale_y_continuous(breaks = seq(0, 15000, by=2500)) + labs(title=&#39;Systematic effort per cruise&#39;, col=&#39;Cruise&#39;) + theme_light() # Make it interactive ggplotly(p) "],["maps.html", " 4 Maps Publishable maps Interactive maps Interactive dashboard", " 4 Maps To build a flexible system for mapping cruise data, we have the following functions: Publishable maps Base maps Begin with a basic map, including EEZ borders: m &lt;- map_base(region=&#39;cnp&#39;) m We also have a base map for the California Current … m &lt;- map_base(region=&#39;ccs&#39;) m And the ETP: m &lt;- map_base(region=&#39;etp&#39;) m Add strata Add your research strata to your map: m &lt;- map_base(region=&#39;cnp&#39;) m1 &lt;- map_strata(m, cruz_1720$settings, region=&#39;cnp&#39;) Add survey tracks m1 &lt;- map_effort(m, cruz_1720) m1 The defaults of map_effort() assume, for simplicity, that you want to see the segments to be included in density estimation for the first cohort specified in your settings. You can adjust this and other defaults using the function arguments. Customizing effort Inputs This map changes survey track thickness and color. m1 &lt;- map_effort(m, cruz_1720, effort_color=&#39;firebrick&#39;, effort_stroke=2.5, effort_linetype=1,) Color-code conditions Your second customization option is to add format variables to the segments slot of the cohort of interest in the cruz object. This gives you full control of line color, thickness, and line-type according to whatever specifications you wish to set, e.g., color-coding by effort type or Beaufort sea state. This is possible because the function map_effort() looks for the variables col (line color), lwd (line thickness or stroke), and lty (line type) in the columns of cruz$segments. If these columns exist, the values therein will be used instead of the function defaults. For example, color-code by Beaufort scale: # Save copy of segments to modify cruzi &lt;- cruz_1720 segments &lt;- cruzi$cohorts$all$segments # Add column `col`: color code by BFT sea state bft_colors &lt;- c(&#39;steelblue4&#39;,&#39;steelblue2&#39;,&#39;cadetblue1&#39;,&#39;grey&#39;) segments$col &lt;- bft_colors[4] segments$col[ segments$avgBft &lt;= 7 ] &lt;- bft_colors[3] # bft 5 + segments$col[ segments$avgBft &lt;= 4 ] &lt;- bft_colors[2] # bft 3 - 4 segments$col[ segments$avgBft &lt;= 2 ] &lt;- bft_colors[1] # bft 0 -2 # Update sub_segments slot in `cruz` object cruzi$cohorts$all$segments &lt;- segments # Update map m_custom2 &lt;- map_effort(m, cruzi) # Add legend using native functions from mapping package `tmap` m_custom2 &lt;- m_custom2 + tmap::tm_add_legend(&#39;line&#39;, col = bft_colors, lwd = 3, labels = c(&#39; 0 - 2&#39;, &#39; 3 - 4&#39;, &#39; 5 +&#39;, &#39; no data&#39;), title=&quot;Beaufort sea state&quot;) + tmap::tm_layout(legend.position=c(&#39;left&#39;,&#39;bottom&#39;)) # Show map m_custom2 Add sightings Use the function map_sightings() to add sightings to your map: m1 &lt;- map_sightings(m, cruz_1720) Customizing sightings To demonstrate some of the customization options, consider this map that shows sightings of false killer whales with custom dot color, shape, and size: m1 &lt;- map_sightings(m, cruz_1720, include_species = &#39;033&#39;, color_base = &#39;purple&#39;, shape_base = 18, size_base = 1) Next is a map of humpback whales and sperm whales, color-coded by species and shape-coded by whether or not the sighting will be included in the analysis: m1 &lt;- map_sightings(m, cruz_1720, include_species = c(&#39;076&#39;,&#39;046&#39;), color_code = TRUE, shape_code = TRUE) Overview Here is an overview of the steps needed to map strata, survey tracks, and sightings all together: m &lt;- map_base(&#39;cnp&#39;) m &lt;- map_strata(m, cruz_1720$settings) m &lt;- map_effort(m, cruz_1720) m &lt;- map_sightings(m, cruz_1720, size_base=.4) m Interactive maps LTabundR also has an interactive map function, which maps survey data using the leaflet package. map_cruz(cruz_1720, cohort=1, eez_show=FALSE, strata_show=FALSE, effort_show=TRUE, effort_resolution=1, sightings_show=TRUE, sightings_color = &#39;firebrick&#39;, verbose=FALSE) Note that you can also click on sightings and tracklines to see their details. Refer to the documentation for this function (?map_cruz) to see all the options available for stylizing these maps. Interactive dashboard Finally, note that LTabundR comes with an interactive data explorer app (a Shiny app) for filtering survey data according to effort scenario and species code, toggling map_cruz() settings, and reviewing summary tables of effort and sightings (including inspection of truncation distances). cruz_explorer(cruz) Screenshots from this app:           "],["filter.html", " 5 Filter &amp; combine surveys Filter Combine", " 5 Filter &amp; combine surveys You may soon encounter the need to filter a processed cruz object to only certain years, regions, or cruise numbers. You may also need to combine one processed cruz object with another. Here we will continue with the cruz object we created on the previous page. As a reminder, here is the data structure of that object: cruz_structure(cruz) &quot;cruz&quot; list structure ======================== $settings $strata --- with 11 polygon coordinate sets $survey --- with 10 input arguments $cohorts --- with 3 cohorts specified, each with 19 input arguments $strata ... containing a summary dataframe of 11 geostrata and their spatial areas ... geostratum names: HI_EEZ, OtherCNP, MHI, WHICEAS, Spotted_OU, Spotted_FI, Spotted_BI, Bottlenose_KaNi, Bottlenose_OUFI, Bottlenose_BI, NWHI $cohorts $all geostrata: WHICEAS, HI_EEZ, OtherCNP $segments --- with 1890 segments (median = 149.3 km) $das --- with 329632 data rows $sightings --- with 3934 detections $subgroups --- with 255 subgroups, 49 sightings, and 389 events $bottlenose geostrata: WHICEAS, HI_EEZ, OtherCNP, Bottlenose_BI, Bottlenose_OUFI, Bottlenose_KaNi $segments --- with 2049 segments (median = 148.6 km) $das --- with 329632 data rows $sightings --- with 523 detections $spotted geostrata: WHICEAS, HI_EEZ, OtherCNP, Spotted_OU, Spotted_FI, Spotted_BI $segments --- with 2057 segments (median = 148.5 km) $das --- with 329632 data rows $sightings --- with 527 detections Filter LTabundR lets you filter a cruz object using the function filter_cruz(). For example, in our WHICEAS case study, we processed surveys from 1986 - 2020, which we needed to do to model our detection functions, but our interest for mapping is specifically valid effort in 2017 and 2020 only, and only within the \"WHICEAS\" geostratum. cruz_1720 &lt;- filter_cruz(cruz, analysis_only = TRUE, years = c(2017, 2020), regions = &#39;WHICEAS&#39;) We will use this filtered cruz object for mapping &amp; sightings summaries downstream. save(cruz_1720,file=&#39;whiceas_cruz_1720.RData&#39;) Note that filter_cruz() has many other filter options. See ?filter_cruz() for details. Combine Say you have two processed cruz objects: one containing survey effort from the Hawaiian EEZ (HI_EEZ) geostratum area only, and one containing survey effort from everywhere else that does not include HI_EEZ effort. Let’s make those fake datasets right now, using filter_cruz(): Hawaiian EEZ-only data: cruz_hi &lt;- filter_cruz(cruz, regions = &#39;HI_EEZ&#39;, verbose = FALSE) Pelagic Hawaiian EEZ - only data: cruz_other &lt;- filter_cruz(cruz, not_regions = &#39;HI_EEZ&#39;, verbose = FALSE) Say you want to combine these datasets together in order to reconstruct the equivalent of our original cruz object. You can do this with the LTabundR function, cruz_combine(). # Make a list of cruz objects cruzes &lt;- list(cruz_hi, cruz_other) # Now combine cruz_demo &lt;- cruz_combine(cruzes) Re-constituted data structure: cruz_structure(cruz_demo) &quot;cruz&quot; list structure ======================== $settings $strata --- with 11 polygon coordinate sets $survey --- with 10 input arguments $cohorts --- with 3 cohorts specified, each with 19 input arguments $strata ... containing a summary dataframe of 11 geostrata and their spatial areas ... geostratum names: HI_EEZ, OtherCNP, MHI, WHICEAS, Spotted_OU, Spotted_FI, Spotted_BI, Bottlenose_KaNi, Bottlenose_OUFI, Bottlenose_BI, NWHI $cohorts $all geostrata: WHICEAS, HI_EEZ, OtherCNP $segments --- with 1890 segments (median = 149.3 km) $das --- with 329632 data rows $sightings --- with 3934 detections $subgroups --- with 237 subgroups, 39 sightings, and 345 events $bottlenose geostrata: WHICEAS, HI_EEZ, OtherCNP, Bottlenose_BI, Bottlenose_OUFI, Bottlenose_KaNi $segments --- with 2049 segments (median = 148.6 km) $das --- with 329632 data rows $sightings --- with 523 detections $spotted geostrata: WHICEAS, HI_EEZ, OtherCNP, Spotted_OU, Spotted_FI, Spotted_BI $segments --- with 2057 segments (median = 148.5 km) $das --- with 329632 data rows $sightings --- with 527 detections "],["summarize.html", " 6 Summarize survey Summarize effort Summarize by Beaufort Summarize sightings Summarize certain species cruz_explorer()", " 6 Summarize survey Here we will summarize the 2017 &amp; 2020 survey data we prepared on the previous page. load(&#39;whiceas_cruz_1720.RData&#39;) Summarize effort The summarize_effort() functions builds tables with total kilometers and days surveyed. effort &lt;- summarize_effort(cruz_1720, cohort=1) This function summarizes effort in several default tables: effort %&gt;% names() [1] &quot;total&quot; &quot;total_by_cruise&quot; &quot;total_by_year&quot; &quot;total_by_effort&quot; [5] &quot;total_by_stratum&quot; Total surveyed The slot $total provides the grand total distance and unique dates surveyed: library(DT) effort$total %&gt;% DT::datatable(options=list(initComplete = htmlwidgets::JS( &quot;function(settings, json) {$(this.api().table().container()).css({&#39;font-size&#39;: &#39;9pt&#39;});}&quot;) )) Total surveyed by effort The slot $total_by_effort provides the total distance and days surveyed, grouped by segments that will be included in the analysis and those that won’t: Total surveyed by stratum The slot $total_by_stratum provides the total distance and days surveyed within each stratum, again grouped by segments that will be included in the analysis and those that won’t: Summarize by Beaufort bft &lt;- summarize_bft(cruz_1720, cohort=1) This function summarizes effort by Beaufort in four default tables: bft %&gt;% names() [1] &quot;overall&quot; &quot;by_year&quot; &quot;by_stratum&quot; &quot;details&quot; Simple overall breakdown The slot $overall provides the total effort – and proportion of effort – occurring in each Beaufort state: Breakdown by year The slot $by_year provides the above for each year separately: Breakdown by stratum The slot $by_stratum provides the above for each geostratum separately: Detailed breakdown The slot $details provides the above for each cruise-year-study area-geostratum combination within the data: Summarize sightings The summarize_sightings() function builds tables summarizing the sightings within each cohort-analysis. (Eventually, we may want to include an option to merge all sightings from all cohort-analyses into a single table.) sightings &lt;- summarize_sightings(cruz_1720, cohort=1) This function summarizes sightings in four default tables: sightings %&gt;% names() [1] &quot;simple_totals&quot; &quot;analysis_totals&quot; [3] &quot;stratum_simple_totals&quot; &quot;stratum_analysis_totals&quot; Simple species totals The slot $simple_totals includes all sightings, even if they will not be inluded in analysis: Analysis totals The slot $analysis_totals only includes sightings that meet all inclusion criteria for the analysis: Simple totals for each stratum The slot $stratum_simple_totals splits the first table (simple species totals) so that sightings are tallied for each geo-stratum separately: Analysis totals for each stratum The slot $stratum_analysis_totals splits the second table (analysis totals for each species) so that sightings are tallied for each geo-stratum separately: Summarize certain species To deep-dive into details for a ceratin species (or group of species), use the function summarize_species(). species &lt;- summarize_species(spp=&#39;046&#39;, cruz_1720) This functions a list with a variety of summaries: species %&gt;% names [1] &quot;species&quot; &quot;n_total&quot; &quot;n_analysis&quot; [4] &quot;school_size&quot; &quot;yearly_total&quot; &quot;yearly_analysis&quot; [7] &quot;regional_total&quot; &quot;regional_analysis&quot; &quot;detection_distances&quot; [10] &quot;sightings&quot; The slots $n_total and $n_analysis provide the total number of sightings and the number eligible for inclusion in the analysis: species$n_total [1] 14 species$n_analysis [1] 14 School size details This table only includes the sightings eligible for analysis: Annual summaries (all sightings) Annual summaries (analysis only) Regional summaries (all sightings) Regional summaries (analysis only) Detection distances This table can be used to determine the best truncation distance to use, based on the percent truncation you wish and the number of sightings available at each option. All sightings data Finally, this last slot holds a dataframe of all sightings data for the specified species: cruz_explorer() Note that all of these summary tables can be viewed interactively using the function cruz_explorer(), which allows you to efficiently subset the data according to various filters. cruz_explorer(cruz_1720) "],["g0.html", " 7 Estimating g(0) Relative g(0) Weighted average g(0)", " 7 Estimating g(0) Detection function models assume g(0) is 1.0. In distance sampling, a “detection function” is fit to the your sighting distances to reflect the fact that animals farther out are more difficult to detect. That detection function is a model of how the probability of detection declines with increasing distance from your survey trackline. The equations for detection function models are all constructed to assume that the probability of detecting an animal on your trackline (distance = 0 km) is 1.0 – you never miss an animal on your trackline. This trackline detection probability is referred to as g(0). In reality, though, it almost never is – and it greatly impacts results. When searching for marine mammals at sea, even some of those occurring directly on your survey trackline will be missed. Real g(0) is actually less than 1.0. This technicality makes a big difference: if g(0) is actually 0.5, the assumption that g(0) is 1.0 will underestimate animal abundance by 50%. Some animals, such as pygmy and dwarf sperm whales (Genus Kogia), are very cryptic and easily missed, and thus likely have a true g(0) below 0.1. This means that estimates assuming their g(0) is still 1.0 will underestimate true abundance by 90%! Moreover, all species – whether you are a pygmy sperm whale or a blue whale – become easier to miss when sighting conditions deteriorate (e.g., Beaufort sea states 4 - 6). Therefore, g(0) must be estimated then used to scale the detection function. So g(0) matters, and the assumption of most detection functions that g(0) = 1.0 is nearly always wrong. Luckily, there is a way to handle this that avoids constructing new detection function equations or estimating even more parameters during the detection function fitting process: we use an estimate of g(0) to scale the detection function. Say the detection function predicts that the probability of detection is 1.0, 0.8, and 0.6 at distances 0 km, 1 km, and 2km, respectively. If g(0) is actually 0.5, then we can scale the detection function so that those respective predictions are now 0.5, 0.4, and 0.3. Estimating g(0) for a survey generally involves four steps: First, you estimate g(0) in perfect conditions (i.e., Beaufort sea state 0). Second, you scale that estimate downward to approximate g(0) when conditions are less than ideal (i.e., a separate g(0) estimate for each Beaufort sea state from 1 to 6). This is known as the relative trackline probability, or relative g(0), or most simply: Rg(0). Third, you determine the weighted average value of g(0) for your particular survey, based on the proportional distribution of effort in each sea state. You pass this single value to your line-transect analysis functions. Fourth, you need to determine the CV of your weighted estimate of g(0). This isn’t straightforward, because you first need to simulate a new distribution for the weighted g(0) estimate, from which you then calculate the CV. The first of these steps is typically the biggest lift analytically, and very few studies provide absolute estimates of g(0) for their species of interest. Doing so involves Bayesian simulations and special field methods (see this example from Jay Barlow, NOAA-NFMFS Southwest Fisheries Science Center). Instead, most studies assume that the absolute g(0) is in fact 1.0 – even though it’s not – and proceed directly to the second step – relative trackline probability, Rg(0). This is more common because it can be estimated directly from the survey data, thanks to an approached developed in Barlow (2015), “Inferring trackline detection probabilities, g(0), for cetaceans from apparent densities in different survey conditions” (Marine Mammal Science). Relative g(0) Archived Rg(0) estimates As of 2022, most northeast Pacific NOAA-NMFS studies still use the Rg(0) estimates from Barlow (2015), which are based on NOAA-NMFS cruises from 1986 to 2010. LTabundR includes Barlow’s results in a built-in dataset. data(barlow_2015) Here is the top of this dataset, in which each row is a Rg(0) estimate for a single species - Beaufort sea state scenario. barlow_2015 %&gt;% head(12) title scientific spp truncation 1 Delphinus spp Delphinus 005-016-017 5.5 2 Delphinus spp Delphinus 005-016-017 5.5 3 Delphinus spp Delphinus 005-016-017 5.5 4 Delphinus spp Delphinus 005-016-017 5.5 5 Delphinus spp Delphinus 005-016-017 5.5 6 Delphinus spp Delphinus 005-016-017 5.5 7 Delphinus spp Delphinus 005-016-017 5.5 8 Stenella attenuata ssp Stenella attenuata ssp 002-006-089,-090 5.5 9 Stenella attenuata ssp Stenella attenuata ssp 002-006-089,-090 5.5 10 Stenella attenuata ssp Stenella attenuata ssp 002-006-089,-090 5.5 11 Stenella attenuata ssp Stenella attenuata ssp 002-006-089,-090 5.5 12 Stenella attenuata ssp Stenella attenuata ssp 002-006-089,-090 5.5 pooling regions bft Rg0 Rg0_CV 1 none none 0 1.000 0.00 2 none none 1 1.000 0.00 3 none none 2 0.940 0.25 4 none none 3 0.722 0.25 5 none none 4 0.485 0.14 6 none none 5 0.394 0.20 7 none none 6 0.404 0.50 8 none none 0 1.000 0.00 9 none none 1 0.728 0.03 10 none none 2 0.531 0.06 11 none none 3 0.386 0.09 12 none none 4 0.282 0.12 This dataset includes Rg(0) estimates for the following species: barlow_2015$title %&gt;% unique [1] &quot;Delphinus spp&quot; &quot;Stenella attenuata ssp&quot; [3] &quot;Stenella longirostris ssp&quot; &quot;Striped dolphin&quot; [5] &quot;Rough-toothed dolphin&quot; &quot;Bottlenose dolphin&quot; [7] &quot;Risso&#39;s dolphin&quot; &quot;Short-finned pilot whale&quot; [9] &quot;Killer whale&quot; &quot;Sperm whale&quot; [11] &quot;Kogia spp&quot; &quot;Cuvier&#39;s beaked whale&quot; [13] &quot;Mesoplodon spp&quot; &quot;Dall&#39;s porpoise&quot; [15] &quot;Minke whale&quot; &quot;Sei/Bryde&#39;s&quot; [17] &quot;Fin whale&quot; &quot;Blue whale&quot; [19] &quot;Humpback whale&quot; &quot;Unidentified dolphin&quot; [21] &quot;Unidentified cetacean&quot; &quot;Pacific white-sided dolphin&quot; [23] &quot;Pygmy killer whale&quot; If your study species – or one with similar detectability – can be found on this list, then you can take this data.frame of Rg(0) values and move on to the next step. Note that if you do not have a large survey dataset, this may be the only option available to you. Estimating new Rg(0) values requires a large number of sightings (i.e., hundreds) across many Beaufort states. New Rg(0) estimates LTabundR includes a function, g0_model(), which you can use to apply the Barlow (2015) modeling methods to generate new estimates of Rg(0) based on your own survey data. To do this, you first need a cruz object in which effort has been split into short segments (5 - 10 km). If you want to work with NOAA-NMFS WinCruz data from the Pacific, you can use a built-in dataset of 1986 - 2020 surveys that is processed specifically for use in Rg(0) estimation: data(&quot;noaa_10km_1986_2020&quot;) To use this dataset in R(0) estimation, we first filter it to systematic effort within sea states 0 - 6: cruzi &lt;- filter_cruz(noaa_10km_1986_2020, analysis_only = TRUE, eff_types = &#39;S&#39;, bft_range = 0:6, on_off = TRUE) You can then estimate Rg(0) for each Beaufort sea state using the function g0_model(). For example, the code for striped dolphin (Stenella coeruleoalba) is as follows: rg0 &lt;- g0_model(spp = &#39;013&#39;, truncation_distance = 5.5, cruz = cruzi, pool_bft = NULL, jackknife_fraction = .1) The jackknife_fraction input indicates that standard error and CV will be estimated using an iterative jackknife procedure in which 10% of the data is removed in each iteration. Find more details on this process using the function documentation, ?g0_model(). The input pool_bft provides a way to specify that low Beaufort sea states, which are typically rare in open-ocean surveys, should be pooled. This step may be needed in order to achieve a monotonic decline in the g(0) ~ Bft relationship for some species, but the default is NULL, i.e., no pooling. If pool_bft is the character string \"01\", Beaufort states 1 will be pooled into state 0. If pool_bft is the character string \"012\", Beaufort states 1 and 2 will be pooled into state 0. We recommend beginning with NULL then modifying this if needed, based on the output. The chief result of this function is a $summary table: rg0$summary bft Rg0 ESW n Rg0_SE Rg0_CV ESW_SE 1 0 1.0000000 3.968033 10 0.00000000 0.0000000 0.16288323 2 1 0.9574903 3.724726 10 0.10437256 0.1090064 0.13458164 3 2 0.8507491 3.460165 10 0.16075534 0.1889574 0.10190113 4 3 0.6374127 3.180650 10 0.13055026 0.2048128 0.06967222 5 4 0.4649322 2.898497 10 0.08508186 0.1829984 0.05699997 6 5 0.4238391 2.624881 10 0.07858997 0.1854241 0.07451945 7 6 0.4534225 2.370146 10 0.10177148 0.2244517 0.10268952 The model predicts that Rg(0) declines rapidly with deteriorating sea state: plot(Rg0 ~ bft, data = rg0$summary, ylim=c(0,1), type=&#39;o&#39;, pch=16) abline(h=seq(0,1,by=.1), col=&#39;grey85&#39;, lty=3) In addition to the summary above, this function returns various details, including the details of the Generalized Additive Model (GAM) (for both the estimate and the jackknifed datasets), and the raw sightings and segments used in the model. [1] &quot;Rg0&quot; &quot;gam&quot; &quot;jackknife&quot; &quot;summary&quot; &quot;sightings&quot; &quot;segments&quot; To produce these estimates efficiently for many species, you can use the wrapper function g0_table(), as follows. First you build a list of parameters for each species/species group: species &lt;- list( list(spp = c(&#39;005&#39;, &#39;016&#39;, &#39;017&#39;), title = &#39;Delphinus spp&#39;, truncation = 5.5), list(spp = &#39;021&#39;, title = &quot;Risso&#39;s dolphin&quot;, truncation = 5.5, pool_bft = &#39;12&#39;), list(spp = &#39;046&#39;, title = &#39;Sperm whale&#39;, truncation = 5.5), list(spp = c(&#39;047&#39;, &#39;048&#39;, &#39;080&#39;), title = &#39;Kogia spp&#39;, truncation = 4.0), list(spp = &#39;061&#39;, title = &quot;Cuvier&#39;s beaked whale&quot;, truncation = 4.0), list(spp = &#39;074&#39;, title = &#39;Fin whale&#39;, truncation = 5.5, regions = &#39;CCS&#39;)) Note that we used shorter truncation distances for cryptic species. Note also that we had to pool Beaufort sea states 0-2 for Risso’s dolphins in order to maintain a monotonic decline in the Rg(0) ~ Beaufort curve. Note also that we limited the geostrata used to model the fin whale Rg(0) curve to the California Current System (‘CCS’, after Barlow 2015), so that zero-inflated segments did not confound the model. We then pass this species list to g0_table(). In this example, we are only estimating the Rg(0) relationship, without conducting jackknife estimation: rg0s &lt;- g0_table(cruzi, species, eff_types = &#39;S&#39;, jackknife_fraction = NULL) Now plot the result using a dedicated LTabundR function: g0_plot(rg0s, panes=1) Weighted average g(0) Since g(0) clearly depends upon survey conditions, and since each survey is carried out in a specific sequence of conditions, a unique, weighted g(0) value must be estimated for each species in each geostratum and year of interest. This will be done automatically by the line-transect analysis functions coming up (see LTabundR::lta()), meaning you only need a table of Rg(0) values in order to proceed to the next step. But you can also calculate weighted g(0) values separately as an isolated analysis, which we show below. Let’s say we want to estimate the average g(0) for striped dolphins during the WHICEAS survey years of 2017 and 2020. From above, we have an estimate of the Rg(0) and its CV for each Beaufort state: rg0$summary %&gt;% select(bft, Rg0, Rg0_CV) bft Rg0 Rg0_CV 1 0 1.0000000 0.0000000 2 1 0.9574903 0.1090064 3 2 0.8507491 0.1889574 4 3 0.6374127 0.2048128 5 4 0.4649322 0.1829984 6 5 0.4238391 0.1854241 7 6 0.4534225 0.2244517 We also have a processed cruz object with 2017 data: load(&#39;whiceas_cruz_1720.RData&#39;) cruz_17 &lt;- filter_cruz(cruz_1720, years = 2017, verbose=FALSE) To view the distribution of effort in this WHICEAS 2017 across sea states, we can use the function summarize_bft(): summarize_bft(cruz_17)$overall # A tibble: 6 × 3 bftr km prop &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 130. 0.0200 2 2 583. 0.0897 3 3 901. 0.139 4 4 2140. 0.329 5 5 1873. 0.288 6 6 875. 0.135 We then use the function g0_weighted_var() to compute the weighted Rg(0) for our survey as well as its CV. This function carries out an automated optimization routine to simulate a new distribution for the weighted g(0), which is then used to estimate the weighted CV. weighted_g0_2017 &lt;- g0_weighted(Rg0 = rg0$summary$Rg0, Rg0_cv = rg0$summary$Rg0_CV, cruz = cruz_17) The result: weighted_g0_2017$g0[1:2] wt.mean wt.cv 1 0.526 0.188 Now let’s do the same for WHICEAS 2020 and compare the weighted g(0) estimate: # Filter to 2020 cruz_20 &lt;- filter_cruz(cruz_1720, years = 2020, verbose=FALSE) # Summarize Bft effort summarize_bft(cruz_20)$overall # A tibble: 6 × 3 bftr km prop &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 88.2 0.0165 2 2 262. 0.0490 3 3 434. 0.0813 4 4 1482. 0.278 5 5 2004. 0.376 6 6 1067. 0.200 Note that conditions were a bit worse in 2020 compared to 2017. We therefore expect the weighted g(0) estimate to be lower: weighted_g0_2020 &lt;- g0_weighted(Rg0 = rg0$summary$Rg0, Rg0_cv = rg0$summary$Rg0_CV, cruz = cruz_20) The result: weighted_g0_2020$g0[1:2] wt.mean wt.cv 1 0.494 0.19 Confirmed: weighted g(0) for 2020 is slightly lower than in 2017, due to generally worse survey conditions. These year-specific estimates should prevent those different conditions from impacting their respective abundance estimates. "],["lta.html", " 8 Line-transect analysis Key inputs Variance estimation Other inputs Output Unusual estimate scenarios Behind the scenes", " 8 Line-transect analysis We use line-transect analysis to produce estimates of animal density and/or abundance based upon surveys. The main LTabundR function for conducting line-transect analysis is lta(), which calls for four primary arguments in addition to your cruz object: lta(cruz, Rg0, fit_filters, df_settings, estimates) Below we explain each of these inputs, discuss other optional inputs, and explore the results produced by lta(). Key inputs cruz This is the cruz object you have generated with process_surveys(). Before running lta(), ensure that this cruz object is filtered only to the years, regions, and sighting conditions you would like to use for detection function fitting. Filter your cruz object with full flexibility using LTabundR::filter_cruz(). Note that filtering for detection function fitting is typically less stringent than filtering for downstream steps for abundance estimation, since as many sightings are included as possible to combat low sample sizes, as long as sightings were observed using standard methods in an unbiased search pattern, and as long as you do not expect detectability to vary across years and regions. Here we will work with a version of the 1986-2020 Central North Pacific survey data we processed a few pages back. This version is included as a built-in dataset within LTabundR: load(&#39;whiceas_cruz.RData&#39;) As it is provided, this dataset does not need any filtering. We will use these data to estimate the abundance of striped dolphins (Stenella coeruleoalba), Fraser’s dolphins (Lagenodelphis hosei), and Melon-headed whales (Preponocephala electra) within the WHICEAS study area in 2017 and 2020. We will group these three species into a ‘species pool’ in order to gain a sufficient sample size for fitting a detection function. We will then use “Species” as a covariate within the detection function model, along with other variables including Beaufort Sea State, ship name, and log-transformed school size. Rg0 The result of LTabundR::g0_model(), which is a data.frame with Relative trackline detection probabilities, Rg(0), for each species in each Beaufort sea state. See LTabundR dataset data(\"g0_results\"), used below, as an example. This is an optional input. If not provided, g(0) will be assumed to 1.0, and its CV will be assumed to be 0. Alternatively, you can manually specify values for g(0) and its CV in the estimates argument below. Here we will use a data.frame of Rg(0) estimates based on the same survey years, 1986 - 2020, which has been provided as a built-in dataset: data(&quot;g0_results&quot;) Rg0 &lt;- g0_results fit_filters The fit_filters input specifies how to filter the data before fitting the detection function. It accepts a named list, which in our example will look like this: fit_filters = list(spp = c(&#39;013&#39;, &#39;026&#39;, &#39;031&#39;), pool = &#39;Multi-species pool 1&#39;, cohort = &#39;all&#39;, truncation_distance = 5, other_species = &#39;remove&#39;) spp: A character vector of species codes. Using multiple species codes may be useful when you have low sample sizes for a cohort of similar species. cohort: The cohort containing these species, provided as a name or a number indicating which slot in cruz$cohorts should be referenced. truncation_distance: The truncation distance to apply during model fitting. The remaining inputs are optional (i.e., they all have defaults): pool: A character string, providing a title for this species pool. If not specified, the species codes used will be concatenated to produce a title automatically. other_species: A character vector with four recognized values: If \"apply\" (the default if not specified), the species code will be changed to \"Other\" for sightings in which the species was in a mixed-species school but was not the species with the largest percentage of the total school size. In those cases, the species was not as relevant to the detection of the school as the other species were, which may bias the detection function. This creates a factor level for the detection function to use (when \"species\" is a covariate) to distinguish between cue-relevant species that are within the specified pool and those that are not. The second option for other_species is \"ignore\", which does not reassign species codes to \"Other\", and ignores whether the species of interest held the plurality for a mixed species detection. The third option is \"remove\": any species re-assigned to \"Other\" will be removed before the detection function is fit; this can be useful if only a small number of species are re-assigned to \"Other\", which would then obviate species as a viable covariate (since the sample size of all species levels would be unlikely to exceed df_settings$covariates_n_per_level – see below). The fourth and final option is coerce, which forces all species codes to \"Other\" for the purposes of detection function fitting and abundance estimation. This is effectively the same as removing ‘species’ from the list of covariates, but this option can be a convenience if you want to quickly toggle the use of species as a covariate for a specific species pool, and/or produce abundance estimates for unidentified taxa (e.g., an ‘Unidentified dolphins’ species pool that includes multiple species codes).   df_settings The df_settings input specifies how to fit a detection function to the filtered data. It accepts a named list, which in our example will look like this: df_settings = list(covariates = c(&#39;bft&#39;,&#39;lnsstot&#39;,&#39;cruise&#39;,&#39;year&#39;,&#39;ship&#39;,&#39;species&#39;), covariates_factor = c(FALSE, FALSE, TRUE, TRUE, TRUE, TRUE), covariates_levels = 2, covariates_n_per_level = 10, detection_function_base = &#39;hn&#39;, base_model = &#39;~1&#39;, delta_aic = 2) (Note that all of these inputs have defaults, and are therefore optional.) covariates Covariates you wish to include as candidates in detection function models, provided as a character vector. The covariates must match columns existing within cruz$cohorts$&lt;cohort_name&gt;$sightings. Note that the function will ignore case, coercing all covariates to lowercase. Default: no covariates. covariates_factor A Boolean vector, which must be the same length as covariates, indicating whether each covariate should be treated as a factor instead of a numeric. Default: NULL. covariates_levels The minimum number of levels a factor covariate must have in order to be included as an eligible covariate. Default: 2. covariates_n_per_level The minimum number of observations within each level of a factor covariate. If this condition is not met, the covariate is excluded from the candidates. Default: 10. detection_function_base The base key for the detection function, provided as a character vector. Accepted values are \"hn\" (half-normal key, the default, which exhibits greater stability when fitting to cetacean survey data; Gerrogette and Forcada 2005), \"hr\" (hazard-rate), or c(\"hn\", \"hr), which will loop through both keys and attempt model fitting. base_model The initial model formula, upon which to build using candidate covariates. If not provided by the user, the default is \"~ 1\". delta_aic The AIC difference between the model yielding the lowest AIC and other candidate models, used to define the best-fitting models. Typically, AIC differences of less than 2 (the default) indicate effectively equal model performance. If this value is not zero, then model averaging will be done: if multiple models are within delta_aic of the model with the lowest AIC, all “best” models will be used in subsequent steps and their results will be averaged. See Details below. estimates The estimates input specifies which estimates of density and abundance to produce based on the fitted detection function. This input accepts a list of sub-lists, which in our example will look something like this: estimates &lt;- list(list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2017, regions = &#39;WHICEAS&#39;)) This example shows only a single sub-list, specifying how to generate a density/abundance estimate for striped dolphins (species code 013) within the “WHICEAS” geostratum for 2017. spp: A character vector of species codes. title: A title for this abundance estimate, given as a character vector, ’ e.g., \"Striped dolphin - pelagic\". If left blank, the species code(s) will be concatenated to use as a title. years: A numeric vector of years, used to filter data to include only effort/sightings from these years. regions: A character vector of geostratum names, used to filter the data. Any segment or sighting occurring within any (but not necessarily all) of the provided regions will be returned. This holds true for nested regions: for example, in analyses from the Central North Pacific, in which the Hawaii EEZ geostratum (\"HI-EEZ\") is nested within the larger geostratum representing the entire CNP study area (\"OtherCNP\"), an input of regions = \"OtherCNP\" will return segments/sightings both inside the Hawaii EEZ and outside of it. You also have the option of manually specifying other filters &amp; arguments. Each of these sub-lists accepts the following named slots: cruises: An optional numeric vector of cruise numbers, used to filter data to include effort/sighting from only certain cruises. Ignored if NULL. regions_remove: A character vector of geostratum names, similar to above. Any segment or sighting occurring within any of these not_regions will not be returned. Using the example above, if regions = \"OtherCNP\" and not_regions = \"HI-EEZ\", only segments occuring within OtherCNP and outside of HI-EEZ will be returned. This can be particularly useful for abundance estimates for pelagic stock that exclude nested insular stocks. g0: If left as the default NULL, the lta() function will automatically estimate the weighted trackline detection probability (g0) according to the distribution of Beaufort sea states contained within the survey years/regions for which density/abundance is being estimated (this is done using the LTabundR function g0_weighted()). This will only be done if the Rg0 input above is not NULL; if it is and you do not provide g(0) values here, g0 will be coerced to equal 1. To coerce g(0) to a certain value of your own choosing, you can provide a numeric vector of length 1 or 2. If length 1, this value represents g(0) for all schools regardless of size. If length 2, these values represent g(0) for small and large school sizes, as defined by g0_threshold below. g0_cv: Similar to g0 above: if left NULL, the CV of the g(0) estimate will be automatically estimated based on weighted survey conditions. Alternatively, you can manually specify a CV here, using a numeric vector of length 1 or 2. If you do not specify a value and Rg0 input is NULL, g0_cv will be coerced to equal 0. g0_threshold: The school size threshold between small and large groups. alt_g0_spp: An alternate species code to use to draw Relative g(0) values from the Rg0 input. This is useful in the event that Rg(0) was not estimated for the species whose density/abundance you are estimating, but there is a similarly detectable species whose Rg(0) parameters have been estimated. combine_g0: A Boolean, with default FALSE. If TRUE, weighted g0 estimates will be produced separately for each species code provided (specifically, for each unique row in the Rg0 table that is found after filtering by the species codes you provide in this estimate), THEN average those estimates together. This can be useful when you do not have a Rg(0) estimates for a certain species, but you can approximate g0 by averaging together estimates from multiple species (e.g., averaging together weighted g(0) from across rorqual species in order to get a weighted g(0) estimate for ‘Unidentified rorquals’). region_title: An optional character vector indicating the title you would like to give to the region pertaining to this estimate. This can be useful if you have a complicated assemblage of regions you are combining and/or removing. If not supplied, the function will automatically generate a region_title based on regions and regions_remove. forced_effort: If this is a single numeric value instead of NULL (NULL is the default), this value will be used as the survey effort, in km, in a brute-force method; this same value will be used for every year and region. This is only helpful if you are looking for a relatively easy way to compare results from your own analysis to another (e.g., comparing LTabundR results to reports from NOAA reports prior to 2021, in which effort was calculated slightly differently). area: If this is a single numeric value instead of NULL (NULL is the default), this value will be used as the area of the region in which abundance is being estimated, in square km, in a brute-force approach. If left NULL, the function will calculate the final area of the survey area resulting from the regions and regions_remove filters above. remove_land: A Boolean, with default TRUE, indicating whether or not land area should be removed from the survey area before calculating its area for abundance estimation. This term is only referenced if area is not specified manually. Here is the full estimates list for all the species-year-geostratum combinations for which we want to estimate density/abundance: estimates &lt;- list( list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2017, regions = &#39;WHICEAS&#39;), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2020, regions = &#39;WHICEAS&#39;), list(spp = &#39;026&#39;, title = &#39;Frasers dolphin&#39;, years = 2017, regions = &#39;WHICEAS&#39;), list(spp = &#39;026&#39;, title = &#39;Frasers dolphin&#39;, years = 2020, regions = &#39;WHICEAS&#39;), list(spp = &#39;031&#39;, title = &#39;Melon-headed whale&#39;, years = 2017, regions = &#39;WHICEAS&#39;), list(spp = &#39;031&#39;, title = &#39;Melon-headed whale&#39;, years = 2020, regions = &#39;WHICEAS&#39;)) Each of these sub-lists specifies the details for a single estimate of density/abundance, making it possible to produce multiple estimates from the same detection function model. Generally, there needs to be a sub-list for each species-region-year combination of interest. You can imagine that building up these sub-lists can get tedious. It can also introduce the possibility of error or inconsistencies across estimates of multiple species. To address that issue, LTabundR includes the function lta_estimates(), which makes your code for preparing an estimates object much more efficient. That function is demonstrated in the Case Studies chapters. Quickly review results: results$estimate title species Region Area year segments km 1 Striped dolphin 013 (WHICEAS) 402948.7 2017 29 3040.009 2 Striped dolphin 013 (WHICEAS) 402948.7 2020 34 4585.386 3 Frasers dolphin 026 (WHICEAS) 402948.7 2017 29 3040.009 4 Frasers dolphin 026 (WHICEAS) 402948.7 2020 34 4585.386 5 Melon-headed whale 031 (WHICEAS) 402948.7 2017 29 3040.009 6 Melon-headed whale 031 (WHICEAS) 402948.7 2020 34 4585.386 Area_covered ESW_mean n g0_est ER_clusters D_clusters N_clusters size_mean 1 11003.442 3.619543 3 0.388 0.0009868393 0.0003538356 142.57760 31.95079 2 16960.781 3.698878 3 0.364 0.0006542525 0.0002447343 98.61536 53.95865 3 NA NA 0 0.341 0.0000000000 0.0000000000 0.00000 NA 4 16897.976 3.685181 2 0.313 0.0004361683 0.0001896659 76.42563 132.51143 5 9538.118 3.137530 2 0.425 0.0006578929 0.0002482908 100.04846 185.98103 6 15957.100 3.479991 3 0.393 0.0006542525 0.0002414466 97.29061 226.13289 size_sd ER D N 1 0.5780968 0.03153029 0.01131967 4561.247 2 2.7402089 0.03530258 0.01319739 5317.873 3 NA 0.00000000 0.00000000 0.000 4 182.1747087 0.05779729 0.02376260 9575.109 5 96.4369707 0.12235559 0.04481712 18059.002 6 175.5620746 0.14794801 0.05687956 22919.545 More details on the lta() output are provided below. Variance estimation By default, the lta() function produces a single estimate of the detection function and a single estimate of density/abundance estimate for each sub-list within estimates(). However, you can obtain the coefficient of variation (CV) of those estimates by activating the function’s bootstrap variance estimation feature. To do this, add bootstraps as an input specifying a large number of iterations (1,000 iterations is standard, but we suggest first testing your code with 5 - 10 bootstraps before committing; the function typically requires ~1 hour per 100 bootstraps.). For the purposes of example only, we will just use 10 bootstrap iterations here: results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, bootstraps = 10) This command will first produce official estimates of the detection function and density/abundance, then it will repeat the analysis for the number of iterations you have specified. In each iteration, survey segments are re-sampled according to standard bootstrap variance estimation methods (see more details below, in “Behind the Scenes”). Other inputs There are a few other optional input arguments that lend further control over the lta() procedure. lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, ss_correction = 1, bootstraps = 10 toplot = TRUE, verbose = TRUE,) use_g0: A Boolean, with default TRUE, indicating whether or not to use custom g(0) value(s). If FALSE, the assumed g(0) value will be 1. This is an easy way to toggle on-and-off automated g(0) estimation and/or ignore manually supplied g(0) values. ss_correction: Should a correction be applied to school sizes? School sizes will be scaled by this number. The default, 1, means no changes will occur. toplot: A Boolean, with default TRUE, indicating whether detection function plots (Distance::plot.ds()) should be displayed as the candidate models are tested. verbose: A Boolean, with default TRUE, indicating whether or not updates should be printed to the Console. Output During processing While lta() is running, it will print things to the Console (if verbose is TRUE), plot diagnostic plots of how the study area is being calculated (if toplot is TRUE), and plot detection function model candidates (if toplot is TRUE). To demonstrate this, we will run the estimate for striped dolphins in 2017 only, without variance bootstrapping. To expedite processing, we will manually supply g(0) values from Bradford et al. (2021) (this saves about 3 minutes per estimate): new_estimates &lt;- list( list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2017, regions = &#39;WHICEAS&#39;, g0 = 0.35, g0_cv = 0.19)) # Run it: demo &lt;- lta(cruz, Rg0, fit_filters, df_settings, new_estimates) Cruise DateTime SightNoDaily line_num species Bft lnsstot 41 1164 1988-08-08 08:05:00 19880808_9 41585 026 NA 6.856462 72 1268 1989-08-23 17:45:00 19890823_12 60352 013 NA 2.995732 82 1268 1989-11-25 06:10:00 19891125_22 66719 013 NA 4.716132 387 2001 2020-01-26 07:51:40 20200126_129 609065 013 NA 3.676782 Cruise DateTime SightNoDaily line_num species best ss_valid 334 1604 2016-07-07 07:15:57 20160707_105 512744 Other 1 FALSE Additionally, windows will appear showing details for the detection function models and details of the density/abundance estimate. Outputs The lta() function returns a list of objects. To demonstrate this output, we will pull back in the dataset representing the result of the analysis above, for all three species in both years (with 5 bootstrap iterations): This list of results has five slots: names(results) [1] &quot;pool&quot; &quot;inputs&quot; &quot;estimate&quot; &quot;df&quot; &quot;bootstrap&quot; pool: The species pool pertaining to these estimates. inputs: A list of the inputs used to produce these estimates. estimate: A table of density/abundance estimates for each species/region/year combination specified in the estimates input. results$estimate title species Region Area year segments km 1 Striped dolphin 013 (WHICEAS) 402948.7 2017 29 3040.009 2 Striped dolphin 013 (WHICEAS) 402948.7 2020 34 4585.386 3 Frasers dolphin 026 (WHICEAS) 402948.7 2017 29 3040.009 4 Frasers dolphin 026 (WHICEAS) 402948.7 2020 34 4585.386 5 Melon-headed whale 031 (WHICEAS) 402948.7 2017 29 3040.009 6 Melon-headed whale 031 (WHICEAS) 402948.7 2020 34 4585.386 Area_covered ESW_mean n g0_est ER_clusters D_clusters N_clusters size_mean 1 11003.442 3.619543 3 0.388 0.0009868393 0.0003538356 142.57760 31.95079 2 16960.781 3.698878 3 0.366 0.0006542525 0.0002433969 98.07648 53.95865 3 NA NA 0 0.345 0.0000000000 0.0000000000 0.00000 NA 4 16897.976 3.685181 2 0.315 0.0004361683 0.0001884617 75.94039 132.51143 5 9538.118 3.137530 2 0.425 0.0006578929 0.0002482908 100.04846 185.98103 6 15957.100 3.479991 3 0.396 0.0006542525 0.0002396175 96.55356 226.13289 size_sd ER D N 1 0.5780968 0.03153029 0.01131967 4561.247 2 2.7402089 0.03530258 0.01312528 5288.814 3 NA 0.00000000 0.00000000 0.000 4 182.1747087 0.05779729 0.02361173 9514.315 5 96.4369707 0.12235559 0.04481712 18059.002 6 175.5620746 0.14794801 0.05644865 22745.912 df: A named list with details for the detection function. results$df %&gt;% names [1] &quot;best_models&quot; &quot;all_models&quot; &quot;best_objects&quot; &quot;sightings&quot; &quot;sample_size&quot; [6] &quot;curve&quot; results$df$best_models Model Key_function Formula Pmean AIC 1 11 hn ~1 + ship + bft + lnsstot + species 0.5357372 1041.309 2 9 hn ~1 + ship + bft + lnsstot 0.5431524 1042.481 $\\\\Delta$AIC Covariates tested pool 1 0.000 bft, lnsstot, ship, species Multi-species pool 1 2 1.172 bft, lnsstot, ship, species Multi-species pool 1 bootstrap: If bootstrap variance estimation was carried out, the output would also include bootstrap, a named list with results from the bootstrap process, only returned if the bootstraps input is greater than 1. results$bootstrap %&gt;% names [1] &quot;summary&quot; &quot;details&quot; &quot;df&quot; results$bootstrap$summary %&gt;% head # A tibble: 6 × 18 # Groups: title, Region [3] title Region year species iterations ESW_mean g0_mean g0_cv km ER &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Frasers d… (WHIC… 2017 026 10 NaN 0.390 0.713 3103. 0 2 Frasers d… (WHIC… 2020 026 10 3.52 0.390 0.654 4689. 0.0902 3 Melon-hea… (WHIC… 2017 031 10 3.10 0.422 0.275 3103. 0.181 4 Melon-hea… (WHIC… 2020 031 10 3.50 0.425 0.251 4689. 0.180 5 Striped d… (WHIC… 2017 013 10 3.66 0.425 0.148 3103. 0.0435 6 Striped d… (WHIC… 2020 013 10 3.69 0.367 0.179 4689. 0.0425 # ℹ 8 more variables: D &lt;dbl&gt;, size &lt;dbl&gt;, Nmean &lt;dbl&gt;, Nmedian &lt;dbl&gt;, # Nsd &lt;dbl&gt;, CV &lt;dbl&gt;, L95 &lt;dbl&gt;, U95 &lt;dbl&gt; Unusual estimate scenarios Most line-transect estimates in most areas are relatively straightforward: you want an estimate for a single species in a single year, and your geostrata do not overlap, nor do they need to be stratified or combined in funny ways. But there will also be unusual and slightly more complicated scenarios. We outline some of those below and demonstrate how they can be handled within the LTabundR framework. Species combinations &amp; g(0) Multi-species schools Multi-species schools can confound detection function model fitting, since your species of interest may not be the predominant species in the group, which means that the other species present may be having a greater influence over the detection function. You can decide how to account for this using the other_species slot in your fit_filters list. See the details on this discussed above. Species pools When you don’t have enough sightings of individual species to model a detection function effectively, it can be useful to pool sightings from multiple species who have similar detection characteristics. This is a common tactic in the Central North Pacific. When you do this, you typically need to make the following changes to a “normal” lta() call: In your df_settings list, consider adding species as a covariate, and ensure that you specify that it should be treated as a factor. This may improve detection function model fit. In your fit_filters list, specify multiple species codes and name your species pool accordingly (e.g., “Multi-species pool 1”). In your fit_filters list, specify how to handle “Other” species (see above). In your estimates list, add a sub-list for each species-region-year for which you want a density/abundance estimate. We took all of these steps in the example above with striped dolphins, Fraser’s dolphins, and melon-headed whales. Use that code as a guide. Pooling similar species Species that can be confused with one another may need to be pooled together for abundance estimation. For example, in the northeast Pacific, sei whales, Bryde’s whales, and fin whales can co-occur but they are difficult to distinguish in the field. They are also relatively uncommon, and may need to be pooled with other species in order to obtain a sound detection function model. To account for this, we want to estimate the density/abundance in the WHICEAS study area of all detections of sei, Bryde’s, fin whales together. To handle this, we will follow all the steps taken for a multi-species pool, as discussed above. Additionally, in our estimates sub-list(s), we will specify (1) that multiple species should be included in the estimate, and (2) that the weighted g(0) estimates for each of the individual species should be averaged together: estimates &lt;- list( list(spp = c(&#39;072&#39;, &#39;073&#39;, &#39;099&#39;, &#39;074&#39;), title = &quot;Sei/Bryde&#39;s/Fin whale&quot;, years = 2017, regions = &#39;WHICEAS&#39;, combine_g0 = TRUE)) Rare unidentified taxa A similar problem occur when you have species codes for unidentified taxa that have been identified down to a family- or genus-level. For example, “Unidenfitied Mesoplodon” is a species code (the code is \"050\") for any beaked whale that is definitely in the genus Mesoplodon. There are plenty of these sightings, which means g(0) and its CV can be estimated just fine without referring to other species codes. Other unidentified taxa, however, are less common. For example, in Hawaiian studies, density/abundance is estimated for “Unidentified rorquals”. In the field there is a species code for this group, \"070\", but it is rarely used – there are enough sightings to model the detection function, but not nearly enough sightings to estimate g(0) or its CV. In this case, we need to combine g(0) from more common species codes in order to estimate the unidentified rorqual’s g(0). To do this, we fit a detection function using species code \"070\" … fit_filters &lt;- list(spp = c(&#39;070&#39;), pool = &#39;Unidentified rorqual&#39;, cohort = &#39;all&#39;, truncation_distance = 5.5) … then, in our estimates list, we specify some alternate g(0) species designations. estimates &lt;- list( list(spp = &#39;070&#39;, title = &#39;Unidentified rorqual&#39;, years = 2017, regions = &#39;WHICEAS&#39;, alt_g0_spp = c(&#39;071&#39;,&#39;099&#39;,&#39;074&#39;,&#39;075&#39;), combine_g0 = TRUE)) Geostratum combinations In your estimates sub-lists, the regions and regions_remove slots give you control of the geographic scope of (1) the weighted g(0) and CV used in density estimation, (2) the effort and sightings used to estimate density, (3) and the area used to calculate abundance. A note on cohort geostrata Recall that, when processing your survey data to create a cruz object, you provide a list of geostrata as an argument in your process_surveys() call. You also have the option to specify a subset of those geostrata for each species cohort (see load_cohort_settings()), which is an option that you should almost always use. Selecting a subset of geostrata is important because that subset is used to “segmentize” your survey data – i.e., break effort into discrete sections that can be bootstrap-sampling during the lta() variance estimation routine – and the segmentizing procedure always breaks segments when a survey passes from one geostratum to another. This matters because the lta() bootstrapping routine will re-sample survey segments in a way that preserves the proportion of segments occurring in each geostratum, to ensure that all geostrata are represented in the same proportion as the original estimate. When segments are unncessarily broken into small segments by irrelevant geostrata that have been included in the analysis, the bootstrap estimate of the CV is likely to be too large. For example: in the Central North Pacific, there are about 11 geostrata commonly used. These include the Hawaiian EEZ geostratum, the Main Hawaiian Islands geostratum, and the larger CNP geostratum that represents the maximum range of the study area. These three geostrata are typically all you need for most density estimates for most species. However, a few species – e.g., bottlenose dolphin, pantropical spotted dolphin, and false killer whale – have special geostrata that represent insular stock boundaries and/or pelagic stock boundaries. If those insular geostrata are used in density estimates for which they do not apply, they will confound the bootstrap estimate of density/abundance CV. Punchline: be sure to specify only the relevant geostrata in each cohort’s settings. Combining disparate geostrata For example, in Hawaii bottlenose dolphins belong to a pelagic stock as well as several insular stocks. If you wished to estimate the abundance of all insular stocks together, you simply provide their respective geostratum names in the regions slot of your estimates sub-list: estimates &lt;- list(list(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;, years = 2017, regions = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;))) Removing insular geostrata Conversely, you may wish to estimate density/abundance for pelagic bottlenose dolphins only, ignoring the insular stocks. You can substract geostrata using the regions_remove slot: estimates &lt;- list(list(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;, years = 2017, regions = &#39;HI_EEZ&#39;, regions_remove = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;))) Combine partially overlapping geostrata Say you want to estimate the density/abundance for a set of geostrata that partially overlap. An example of this is that the Northwest Hawaiian Islands geostratum overlaps slightly with the Main Hawaiian Islands geostratum. This is not an issue; when study area is calculated within lta() (actually, that function calls another function, strata_area(), to do this. That function is demonstrated below), overlap among strata is accounted for. estimates &lt;- list(list(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;, years = 2017, regions = c(&#39;MHI&#39;,&#39;NWHI&#39;))) Regionally stratified analysis Field surveys are sometimes stratified such that trackline design and/or density can differ substantially across regions. Also, analysts may sometimes wish to estimate density/abundance for individual regions separately, regardless of design stratification. In lta(), you can accommodate a stratified analysis by providing an estimates sub-list for each geostratum. For example, in 2002 the Hawaiian EEZ was surveyed with different effort intensity in the Main Hawaiian Islands region compared to pelagic waters. For that reason, density/abundance estimates ought to be stratified by region: estimates &lt;- list( list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2002, regions = &#39;MHI&#39;), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2002, regions = &#39;HI_EEZ&#39;, regions_remove = &#39;MHI&#39;)) Here we have one 2002 estimate for the Main Hawaiian Islands, and a second for the pelagic Hawiian EEZ, achieved by subtracting the \"MHI\" stratum from the \"HI_EEZ\" stratum. Once lta() processing is complete, you can summarize and plot the results for each study area separately. The next step is to combine the stratified estimates to generate a grand estimate for the entire EEZ. This is achieved using the LTabundR functions lta_enlist() and lta_destratify(). We discuss this further in a later chapter. Subgroup-based analyses After 2010, Pacific Islands Fisheries Science Center (PIFSC) began a sub-group protocol referred to as the “PC Protocol” after the scientific name for false killer whales, Pseudorca crassidens, which was the species for which the protocol was designed. False killer whales are rare and occur in dispersed subgroups, which complicates conventional distance sampling approaches to line-transect analysis. To handle this, a separate, subgroup-based analytical approach was developed in 2014 - 2017. This approach could theoretically be used for other species that occur in subgroups. We cover this on a separate page. Other scenarios Study periods that span years, such as a December - January survey. This is not yet handled well within the LTabundR framework. More will go here. Behind the scenes Area estimation Unless you manually specify the study area in your estimates list, lta() will calculate your study area for you based on the geostrata you provide. It does so by calling the LTabundR function strata_area(), which you can use on your own to explore geostratum combination options. This function was designed using the sf package to handle complex polygon combinations, and it uses Natural Earth datasets to remove land within your study area (this is a feature you can turn off, if you want). Here are some examples of how strata_area() handles complex scenarios. Say you want to estimate abundance in the ‘WHCEAS’ study area, but you want to make sure the study area estimate is accurately removing land: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;WHICEAS&#39;), verbose = FALSE) Say you want to estimate abundance in the pelagic Hawaiian EEZ, ignoring effort and sightings within the Main Hawaiian Islands stratum and accurately removing small islands in northwest Hawaii: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;HI_EEZ&#39;), strata_remove = c(&#39;MHI&#39;), verbose = FALSE) Say you want to estimate abundance of pelagic bottlenose dolphins within the WHICEAS study area, ignoring the insular stocks: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;WHICEAS&#39;), strata_remove = c(&#39;Bottlenose_KaNi&#39;,&#39;Bottlenose_OUFI&#39;,&#39;Bottlenose_BI&#39;), verbose = FALSE) Say you want to estimate abundance for only the insular bottlenose stocks: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;Bottlenose_KaNi&#39;,&#39;Bottlenose_OUFI&#39;,&#39;Bottlenose_BI&#39;), verbose = FALSE) Say you want to estimate abundance for false killer whales within the Northwest Hawaiian Islands and Main Hawaiian Islands study areas combined, but those geostrata partially overlap: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;MHI&#39;,&#39;NWHI&#39;), verbose = FALSE) Say you want to estimate abundance for the Hawaiian EEZ outside of those partially overlapping geostrata: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = &#39;HI_EEZ&#39;, strata_remove = c(&#39;MHI&#39;,&#39;NWHI&#39;), verbose = FALSE) g(0) estimation If you want lta() to calculate a weighted g(0) estimate (and associated CV) that is specific to the conditions associated with your estimates sub-list parameters, all you need to do is provide the Rg0 input. When you do this, the lta() function will find the Rg0 values associated with the species code(s) in your estimates sub-list, then calculate weighted g(0) and its CV using the LTabundR function, g0_weighted(), which we discussed and demonstrated on the previous page. If lta() can’t find your species code in the Rg0 table you provide, it will give up and assume that g(0) is 1.0 and that g0_cv is 0.0. If your estimates sub-list has a alt_g0_spp slot, lta() will use that species code instead to filter the Rg0 table. If your estimates sub-list has a combine_g0 slot that is TRUE, lta() will filter the Rg0 table using all species codes you provide. If that filtration results in multiple Rg0 species being found, weighted g(0) will be calculated for each of those species separately, then those g(0) estimates will be combined using a geometric mean (using the LTabundR function g0_combine()). If combine_g0 is FALSE, only the first species code provided in your estimates sub-list will be used to filter Rg0. If you want to supply a weighted g(0) estimate and its CV yourself, you can add the g0 and g0_cv slots to your estlimates sublist, as explained above. If you want to coerce g(0) to be assumed to be 1.0 (with CV = 0.0), you can either (1) not supply the Rg0 input, or (2) manually specify the g0 and g0_cv slots in your estimates sub-list accordingly. Covariates in detection function estimation Before detection functions are modelled, any covariates supplied by the user and specified as a factor are first tested for eligibility. Only factors with at least two levels (or whatever you specified with df_settings$covariates_levels) and 10 observations in each level (or whatever you specified with df_settings$covariates_n_per_level) are eligible for inclusion. Fitting a detection function The detection function is estimated using functions in the package mrds, primarily the main function mrds::ddf(), which uses a Horvitz-Thompson-like estimator to predict the probability of detection for each sighting. If multiple base key functions (e.g., half-normal or hazard-rate) are provided, and/or if covariates are specified, model fitting is done in a forward stepwise procedure: In the first round, the base model (no covariates, i.e., \"~1\") is fit first. In the second round, each covariate is added one at a time; at the end of the round, the covariate, if any, that produces the lowest AIC below the AIC from the previous round is added to the formula. This process is repeated in subsequent rounds, adding a new covariate term in each round, until the AIC no longer improves. If a second base key is provided, the process is repeated for that second key. All models within delta_aic of the model with the lowest AIC qualify as best-fitting models. The best-fitting model(s) is(are) then used to estimate the Effective Strip half-Width (ESW) based on the covariates associated with each sighting. If multiple best-fitting models occur, we will find the average ESW for each sighting across all models, using a weighted mean approach in which we weight according to model AIC. To turn off this model averaging step, set delta_aic to 0 to avoid passing multiple models to the abundance estimation stage. Note that if LnSsTot is included as a covariate, the function will (1) check to see if the sightings dataframe has a column named ss_valid (all cruz objects do), then, if so, (2) filter sightings only to rows where ss_valid is TRUE, meaning the school size estimate for that sighting is a valid estimate. This stage of the lta() command is executed within a backend function, LTabundR::fit_df(), which has its own documentation for your reference. Estimating density &amp; abundance Estimates are produced for various combinations of species, regions, and years, according to the arguments specified in your estimates list(s). Before these estimates are produced, we filter the data used to fit the detection function to strictly systematic (design-based) effort (i.e., EffType = \"S\"), in which standard protocols are in use (i.e., OnEffort = TRUE) and the Beaufort sea state is less than 7 (though these controls can be modified using the lta() inputs abund_eff_types and abund_bft_range (see above). Note that if sightings has a column named ss_valid (all standard cruz objects do) and any of the rows in that column are FALSE, those rows will have their best school size estimate (which will be NA or 1, since they are invalid) replaced by the mean best estimate for their respective species. This stage of the lta() command is executed within a back-end function, LTabundR::abundance(), which has its own documentation for your reference. Bootstrap variance estimation If the bootstraps input value is greater than 1, bootstrap variance estimation will be attempted. In each bootstrap iteration, survey segments are re-sampled with replacement before fitting the detection function and estimating density/abundance. Re-sampling is done in a routine that preserves the proportion of segments from each geostratum. Note that the entire process is repeated in each bootstrap: step-wise fitting of the detection function, averaging of the best-fitting models, and density/abundance estimation for all species/region/year combinations specified in your estimates input. At the end of the bootstrap process, results are summarized for each species/region/year combination. 95% confidence intervals are calculated using the BCA method (package coxed, function bca()). g(0) values during bootstrapping When conducting the non-parametric bootstrap routine to estimate the CV of density and abundance, uncertainty is incorporated into the g(0) value in each iteration using a parametric bootstrapping subroutine: First, a logit-transformed distribution is modeled based upon the mean and CV of g(0) provided by the user in the estimates input (see documentation for LTabundR::g0_optimize() for details on this step). This modeled distribution is used to randomly draw a g(0) value for each iteration of the density/abundance bootstrap routine. In this way, the uncertainty in g(0) is propagated into uncertainty in density/abundance. "],["destratify.html", " 9 Stratified analysis", " 9 Stratified analysis Field surveys are sometimes stratified such that trackline design and/or density can differ substantially across regions. Also, analysts may sometimes wish to estimate density/abundance for individual regions separately, regardless of design stratification. On the previous page, we demonstrated how to accommodate a stratified analysis by providing an estimates sub-list for each geostratum. For example, in 2002 the Hawaiian EEZ was surveyed with different effort intensity in the Main Hawaiian Islands region compared to pelagic waters. Here is the code that generates density/abundance estimates of striped dolphins in 2002 (stratified) and 2010 (unstratified), with only 10 bootstrap iterations: # Survey data data(&quot;cnp_150km_1986_2020&quot;) cruz &lt;- cnp_150km_1986_2020 # Rg0 table data(&quot;g0_results&quot;) Rg0 &lt;- g0_results # Detection function filters fit_filters &lt;- list(spp = c(&#39;013&#39;, &#39;026&#39;, &#39;031&#39;), pool = &#39;Multi-species pool 1&#39;, cohort = &#39;all&#39;, truncation_distance = 5, other_species = &#39;remove&#39;) # Detection function settings df_settings &lt;- list(covariates = c(&#39;bft&#39;,&#39;lnsstot&#39;,&#39;cruise&#39;,&#39;year&#39;,&#39;ship&#39;,&#39;species&#39;), covariates_factor = c(FALSE, FALSE, TRUE, TRUE, TRUE, TRUE), covariates_levels = 2, covariates_n_per_level = 10, detection_function_base = &#39;hn&#39;, base_model = &#39;~1&#39;, delta_aic = 2) # Estimates estimates &lt;- list( list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2002, regions = &#39;MHI&#39;), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2002, regions = &#39;HI_EEZ&#39;, regions_remove = &#39;MHI&#39;), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2010, regions = &#39;HI_EEZ&#39;)) # Run it results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, bootstraps = 10) # Save it locally saveRDS(results, file=&#39;lta/multispecies_pool_1.RData&#39;) Let’s read these results back in using the LTabundR function lta_enlist(), which stores LTA results in a flexible list structure. ltas &lt;- lta_enlist(&#39;lta/&#39;) As these results stand, 2002 estimates are stratified into 2 separate regions: (ltas %&gt;% lta_report(verbose = FALSE))$table4 2002 (HI_EEZ) - (MHI) 2002 1 Species or category Density Abundance CV 95% CI Density 2 Striped dolphin 9.22 20,855 0.26 21,638-44,648 8.44 (MHI) 2010 (HI_EEZ) 1 Abundance CV 95% CI Density Abundance CV 95% CI 2 1,790 0.81 0-3,854 14.36 35,527 0.27 35,038-77,911 Now let’s process these LTA results through an LTabundR function, lta_destratify(), which will combine the separate regional estimates from 2002 into a single estimate for the year. ltas_2a &lt;- lta_destratify(ltas, years = 2002, combine_method = &#39;arithmetic&#39;, new_region = &#39;(HI_EEZ)&#39;) The new_region argument specifies how to refer to the combined region. In this case we want the 2002 study area to be named the same as the unstratified 2010 study area, hence \"(HI_EEZ)\". The combine_method argument is explained below. Now let’s re-check the summary table: (ltas_2a %&gt;% lta_report(verbose=FALSE))$table4 2002 (HI_EEZ) 2010 (HI_EEZ) 1 Species or category Density Abundance CV 95% CI Density Abundance 2 Striped dolphin 9.15 22,646 0.74 6,176-83,039 14.36 35,527 1 CV 95% CI 2 0.27 35,038-77,911 There is now only one set of columns for 2002, and the values therein are combinations of the stratified regions. The “de-stratification” routine within lta_destratify() sums abundance estimates across regions to get combined abundance. To estimate density in the combined regions, the function uses weighted averaging in which the area of a geostratum serves as its weight. To estimate CV and confidence interval of the combined result, the function uses one of two combine_methods (this is an argument in the function). The default method is \"arithmetic\", which uses classic formulae to estimate the combined variance and the corresponding confidence interval. This is done in a way that allows multiple geostrata to be combined, not just two. The second option, \"bootstrap\", uses an iterative method that draws bootstrap samples, with replacement, from the bootstrap estimate of density within each stratified region. ltas_2b &lt;- lta_destratify(ltas, years = 2002, combine_method = &#39;bootstrap&#39;, new_region = &#39;(HI_EEZ)&#39;) (ltas_2b %&gt;% lta_report(verbose=FALSE))$table4 2002 (HI_EEZ) 2010 (HI_EEZ) 1 Species or category Density Abundance CV 95% CI Density Abundance 2 Striped dolphin 9.15 22,646 0.23 22,484-48,617 14.36 35,527 1 CV 95% CI 2 0.27 35,038-77,911 Note that use of lta_destratify() only makes sense if the stratified regions have zero overlap. "],["trend-analysis.html", " 10 Trend analysis", " 10 Trend analysis When a species’ density appears to change dramatically from one survey year to the next, it could be due to several factors: the species’ abundance may have changed; its range may have shifted; or the timing of its migratory movements may have shifted. This apparent change could also be due solely to random chance: you can sample the exact same population in two different surveys, and you are liable to produce different abundance estimates due simply to random variation in how often you encounter your target species. In other words, random variation in the encounter rate may lead you to estimate a change in abundance, when in fact there is no change. For this reason, whenever you suspect that abundance has changed between years – i.e., whenever the confidence intervals for two years do not overlap – it is good practice to carry out follow-up tests. One such test was developed in (Bradford et al. 2020 and Bradford et al. (2021). That test has been provided in LTabundR with the function er_simulator(), which refers to a simulation-based test of random variation in the encounter rate (ER). This function uses randomization simulations to test for the probability that year-to-year changes observed in a species’ encounter rate are due to random sampling variation (and not actual change in the encounter rate). More specifcially, this function uses bootstrap sampling of survey segments to see if random variation in sampling could possibly produce an apparent but immaterial change in encounter rate across years. You will find full analytical details in the Appendix to Bradford et al. (2020) for analytical details, but briefly: in each bootstrap iteration, survey segments are resampled in a way that preserves the proportion of effort occurring within each geostratum in the data. The resampled data are used to calculate the overall ER across all survey years, since the null hypothesis is that the ER does not change across years. This overall ER is used to predict the number of sightings in each year, based on the distance covered by the resampled segments in each year. This process is repeated (typically hundreds to thousands of times) to produce a distribution of predicted sighting counts in each year. This distribution reflects the range of ERs that could be possible due simply to random variation and not to underlying changes in abundance. These distributions are compared to the actual number of sightings observed in their respective year. The fraction of simulated sightings counts that are more extreme than the observed count reflects the probability that the observed count is due to random sample variation alone. For example, Bradford et al. (2021) found non-overlapping confidence intervals in their estimates of Bryde’s whale abundance in 2002, 2010, and 2017. To test for the significance of these trends, they carried out the “ER simulator” routine described above. In LTabundR, we would carry out the same analysis as follows: Take your processed data: data(cnp_150km_1986_2020) cruz &lt;- cnp_150km_1986_2020 Filter it to systematic effort in the years of interest: cruzi &lt;- filter_cruz(cruz, analysis_only = TRUE, years = c(2002, 2010, 2017), regions = &#39;HI_EEZ&#39;, eff_types = &#39;S&#39;, bft_range = 0:6) Conduct the ER simulation, passing the species code for the Bryde’s whale (\"072\"). For the purposes of this example, we will only use 100 iterations. er_results &lt;- er_simulator(spp = &#39;072&#39;, cruz = cruzi, iterations = 100) This routine provides a list with two slots: er_results %&gt;% names [1] &quot;summary&quot; &quot;details&quot; The summary slot returns the p-value for each year, i.e., the chances that the observed number of sightings was due purely to random variation in the encounter rate. er_results$summary years observed p 1 2002 158 0.96 2 2010 210 0.00 3 2017 146 0.97 In this example, the encounter rates observed in 2002 and 2010 are very likely due to some process other than random variation in the encounter rate, such as range shifts, seasonal movement timing shifts, and/or changes in abundance. However, the observed encounter rate in 2017 could easily be explained by random variation in the ER. The details slot returns the simulation predictions for each year, which can be used to replicate the histograms that are printed when the function is run. A dataframe with a row for each year. Columns provide the number of observations of the species of interest during systematic effort, and the p-value of the test. The p-value represents the fraction of simulated encounter rates that exceed the observed encounter rate. "],["subgroups.html", " 11 Subgroup-based analysis Inputs Outputs Behind the scenes", " 11 Subgroup-based analysis False killer whales (Pseudorca crassidens) are rare and occur in dispersed subgroups, which complicates conventional distance sampling approaches to line-transect analysis. To better estimate their abundance, in 2010 the Pacific Islands Fisheries Science Center (PIFSC) initiated a sub-group protocol referred to as the “PC Protocol”, a reference to the species’ scientific name. To conduct line-transect analysis with this sub-group-based protocol, a method was developed in 2014 - 2017 To handle this, a separate, subgroup-based analytical approach was developed in 2014 - 2017, then updated in 2020 (Bradford et al. 2020). An additional complication is that false killer whales in Hawaiian waters belong to two discrete populations – the Northwest Hawaiian Islands (NWHI) population and a pelagic population – whose ranges partially overlap, which means that population assignment cannot always be based simply on the geographic location of sightings. When geographic inference of population is not possible, biopsy-sampled genetics, photo-identification, and acoustics are used to assign each sighting to a population post-hoc. To accommodate these special circumstances with an appropriate balance of flexibility and efficiency, LTabundR includes a function named lta_subgroup(), whose use will look something like this: lta_subgroup(df_sits, truncation_distance, ss, cruz10, g0_spp, g0_truncation, g0_pool_bft, g0_jackknife_fraction = 0.1, density_segments, density_das, density_sightings, abundance_area, iterations = 1000, output_dir = &#39;../test_code/subgroup/&#39;, toplot = TRUE, verbose = TRUE, density_bootstraps = 10000) We will step through each of these inputs below, using a case study in which we estimate false killer whale abundance in the Hawaiian EEZ for 2017. Inputs df_sits This is a data.frame of sightings you want to use to fit the detection function model. For false killer whales in Bradford et al. (2020), this is a combination of systematic sightings prior to 2010 and Phase 1 sightings from 2010 onwards (using the PC protocol). No filtering will be applied to these sightings within this function, so make sure you provide the data pre-filtered. Bradford et al. (2020) used a single detection function for all populations of false killer whale. LTabundR has a built-in dataset for processed Central North Pacific surveys, 1986-2020, using 150-km segments. We will use that here: data(&quot;cnp_150km_1986_2020&quot;) cruz &lt;- cnp_150km_1986_2020 The code used to generate this dataset can be seen by pulling up the help documentation: ?noaa_10km_1986_2020. As mentioned above, for 1986 - 2010, all detections are assumed to be ‘Phase 1’ sightings, and therefore usable in detection function estimation. Here we draw those sightings from the above cruz object, filtering as needed (the species code for false killer whales is \"033\"), and to simplify we will select only a few key columns. sits1 &lt;- cruz$cohorts$all$sightings %&gt;% filter(OnEffort == TRUE, year &lt; 2011, Lat &gt;= 5, Lat &lt;= 40, Lon &gt;= -185, Lon &lt;= -120, species == &#39;033&#39;, mixed == FALSE) %&gt;% select(DateTime, Lat, Lon, Cruise, PerpDistKm) sits1 %&gt;% nrow [1] 33 sits1 %&gt;% head DateTime Lat Lon Cruise PerpDistKm 1 1986-11-13 09:43:00 10.466667 -139.2833 990 1.17493742 2 1987-08-19 15:30:00 12.050000 -133.3000 1080 2.24543067 3 1987-12-01 09:23:00 8.266667 -122.5500 1080 0.42589077 4 1989-08-22 06:45:00 11.800000 -141.7333 1268 0.40838431 5 1989-08-22 16:39:00 12.716667 -143.1000 1268 0.68815211 6 1989-09-10 17:18:00 7.350000 -129.5333 1268 0.07751339 For 2011 onwards, we will use Phase 1 subgroup detections from the PC protocol, making sure that the column holding detection distances is named PerpDistKm: sits2 &lt;- cruz$cohorts$all$subgroups$subgroups %&gt;% filter(OnEffort == TRUE, lubridate::year(DateTime) &gt;= 2011, Lat &gt;= 5, Lat &lt;= 40, Lon &gt;= -185, Lon &lt;= -120, Species == &#39;033&#39;, Phase == 1) %&gt;% select(DateTime, Lat, Lon, Cruise, PerpDistKm = PerpDist) sits2 %&gt;% nrow [1] 53 sits1 %&gt;% head DateTime Lat Lon Cruise PerpDistKm 1 1986-11-13 09:43:00 10.466667 -139.2833 990 1.17493742 2 1987-08-19 15:30:00 12.050000 -133.3000 1080 2.24543067 3 1987-12-01 09:23:00 8.266667 -122.5500 1080 0.42589077 4 1989-08-22 06:45:00 11.800000 -141.7333 1268 0.40838431 5 1989-08-22 16:39:00 12.716667 -143.1000 1268 0.68815211 6 1989-09-10 17:18:00 7.350000 -129.5333 1268 0.07751339 To create df_sits for detection function fitting, we combine these datasets together: df_sits &lt;- rbind(sits1, sits2) df_sits %&gt;% nrow [1] 86 truncation_distance The truncation distance, in km, will be applied during detection function model fitting. Typically the farthest 5 - 10% of sightings are truncated, but this needs to be balanced by sample size considerations. Get candidate distances: truncation_options &lt;- quantile(df_sits$PerpDistKm, c(0.90,0.91,0.92,0.93,0.94,.95)) truncation_options 90% 91% 92% 93% 94% 95% 4.249637 4.350266 4.403387 4.486396 4.881672 5.217843 Plot these options: hist(df_sits$PerpDistKm, main=&#39;Detection distances&#39;) abline(v=truncation_options, col=&#39;red&#39;, lty=3) Get sample size remaining for each candidate distance: data.frame(km = truncation_options, n = sapply(truncation_options, function(x){length(which(df_sits$PerpDistKm &lt;= x))})) km n 90% 4.249637 77 91% 4.350266 78 92% 4.403387 79 93% 4.486396 80 94% 4.881672 80 95% 5.217843 81 Based on these results, we will choose a truncation distance of 4.5 km. truncation_distance &lt;- 4.5 ss This is a numeric vector of subgroup school sizes. The function will find this vector’s geometric mean and bootstrapped CV. In Bradford et al. (2020), school size data come from all Phase 1 and Phase 2 estimates of subgroup sizes from 2010 onwards. In the processed cruz object, each of those estimates is the geometric mean of repeat estimates from separate observers. ss &lt;- cruz$cohort$all$subgroups$subgroups %&gt;% filter(lubridate::year(DateTime) &gt;= 2011, Lat &gt;= 5, Lat &lt;= 40, Lon &gt;= -185, Lon &lt;= -120, Species == &#39;033&#39;) %&gt;% pull(GSBest_geom) ss %&gt;% length [1] 183 ss [1] 5.00 7.00 1.00 1.00 1.00 1.33 2.00 2.75 2.67 1.00 6.00 1.00 [13] 2.00 18.50 1.00 4.00 5.00 2.00 1.00 4.00 4.50 2.50 2.00 2.00 [25] 1.00 1.00 5.00 1.00 4.00 1.00 1.00 1.00 1.00 1.00 1.00 2.00 [37] 2.00 1.00 4.00 9.83 2.50 4.75 8.25 3.00 2.00 3.00 3.67 3.50 [49] 4.50 2.00 1.00 1.00 2.00 1.00 1.00 1.00 1.00 1.00 2.00 1.00 [61] 1.00 1.00 2.00 3.00 2.00 1.50 2.67 1.33 2.00 6.00 4.33 2.00 [73] 1.00 3.00 5.00 5.80 4.00 1.00 2.00 2.00 4.00 4.50 2.00 2.00 [85] 2.00 4.50 2.00 1.00 2.00 1.00 1.00 2.00 2.00 1.50 4.50 1.00 [97] 1.00 3.33 3.00 2.00 2.00 7.50 5.00 4.00 3.67 2.00 3.00 2.00 [109] 8.50 8.00 4.00 2.00 1.00 1.00 4.00 4.00 2.00 1.00 2.00 6.00 [121] 2.00 2.00 2.50 1.00 1.00 1.00 1.00 1.00 2.00 2.00 1.00 1.00 [133] 1.00 1.00 1.00 1.00 1.00 2.00 1.00 2.00 2.00 2.00 1.00 4.00 [145] 7.50 1.50 2.00 1.00 1.00 1.00 1.00 2.00 1.00 2.00 3.00 3.00 [157] 2.00 3.00 1.50 1.00 1.00 3.00 2.00 4.00 1.00 1.00 1.00 2.00 [169] 2.00 4.00 2.00 2.00 1.00 1.00 3.00 1.00 3.00 2.50 2.00 2.00 [181] 1.00 1.00 1.00 cruz10 This is a processed cruz object with short segment lengths, ideally 10 km or less (hence the 10 in the input name). This cruz object will be used to estimate Rg(0), i.e., the relative trackline detection probability (see its chapter). LTabundR comes with a built-in dataset we can use for this purpose: data(&quot;noaa_10km_1986_2020&quot;) cruz10 &lt;- noaa_10km_1986_2020 The code used to generate this dataset can be seen by pulling up the help documentation: ?noaa_10km_1986_2020. g0_spp This is a character vector of species code(s) to use to estimate Rg(0). In most cases this will be a single species, e.g., \"033\" for false killer whales. g0_spp &lt;- &#39;033&#39; g0_truncation The truncation distance to use when estimating Rg(0). In Bradford et al. (2020) this is 5.5 km. g0_truncation &lt;- 5.5 g0_pool_bft A way to specify that low Beaufort sea states, which are typically rare in open-ocean surveys, should be pooled. This step may be needed in order to achieve a monotonic decline in the g(0) ~ Bft relationship, but the default is NULL, i.e., no pooling. If g0_pool_bft is the character string \"01\", Beaufort states 1 will be pooled into state 0. If g0_pool_bft is the character string \"012\", Beaufort states 1 and 2 will be pooled into state 0. g0_pool_bft = NULL g0_jackknife_fraction The proportion of data to leave out within each jackknife permutation. The default is 0.1 (i.e., 10% of the data, yielding 10 jackknife loops), after Barlow (2015). g0_jackknife_fraction = 0.1 density_segments The survey segments to be used in density/abundance estimation. For example, Bradford et al. (2020) used 150-km segments to estimate false killer whale density in the Hawaiian EEZ in 2017. For this we can use the 1986-2020 dataset we loaded above. Note that no filtering will be applied to these segments by the lta_subgroup() function, so w need to filter them ourselves first: we want only systematic segments for the Hawaiian EEZ in 2017 (specially, just cruises 1705 and 1706). cruzi &lt;- filter_cruz(cruz = cruz, analysis_only = TRUE, years = 2017, cruises = c(1705, 1706), regions = &#39;HI_EEZ&#39;, bft_range = 0:6, eff_types = &#39;S&#39;, on_off = TRUE) From this filtered cruz object, we will isolate the segments data: density_segments &lt;- cruzi$cohorts$all$segments Since we do not want to stratify our analysis by smaller geostrata, such as the Main Hawaiian Islands, we will go ahead and coerce all stratum assignments to the Hawaiian EEZ geostratum: density_segments$stratum &lt;- &#39;HI_EEZ&#39; density_das This is the complete survey data corresponding to the above segments. These data will be used to determine the proportion of survey effort occurring in each Beaufort sea state during estimation of Relative g(0). density_das &lt;- cruzi$cohorts$all$das density_sightings These are the encounters to use in density/abundance estimation. In Bradford et al. (20120), these were the Phase 1 detections of false killer whale subgroups within the population-region-year of interest, e.g., Northwest Hawaiian Island population sightings within the Hawaiian EEZ in 2017. No filtering is applied to these sightings within lta_subgroups(), so make sure only the sightings you wish to use are included and nothing more. In this example, since we do not have population information on hand, we will not filter detections to a specific population. Instead, we will estimate abundance of all false killer whales within the Hawaiian EEZ: density_sightings &lt;- cruz$cohorts$all$subgroups$subgroups %&gt;% filter(EffType == &#39;S&#39;, OnEffort == TRUE, lubridate::year(DateTime) == 2017, PerpDist &lt;= truncation_distance, Species == &#39;033&#39;, Phase == 1) density_sightings %&gt;% nrow [1] 32 density_sightings %&gt;% head Cruise Date DateTime Lat Lon OnEffort Bft EffType 1 1705 2017-09-21 2017-09-21 14:17:27 20.04283 -161.9177 TRUE 5 S 2 1705 2017-09-21 2017-09-21 14:39:40 20.06017 -161.9840 TRUE 5 S 3 1705 2017-09-29 2017-09-29 13:22:08 23.55750 -175.8527 TRUE 1 S 4 1705 2017-09-29 2017-09-29 13:26:05 23.56050 -175.8643 TRUE 1 S 5 1705 2017-09-29 2017-09-29 13:26:43 23.56083 -175.8662 TRUE 1 S 6 1705 2017-09-29 2017-09-29 13:32:33 23.56517 -175.8832 TRUE 1 S SightNo Species SubGrp Angle RadDist seg_id PerpDist GSBest GSH GSL 1 064 033 A 14 5.0374102 1602 1.2186598 3.00 4 2.00 2 064 033 B 24 3.1483814 1602 1.2805621 3.67 6 2.33 3 073 033 A 49 3.3706200 1610 2.5438392 2.00 3 1.00 4 073 033 B 68 0.6852359 1610 0.6353397 1.00 2 1.00 5 073 033 C 40 0.3703978 1610 0.2380871 1.00 1 1.00 6 073 033 E 89 2.9817023 1610 2.9812482 1.00 2 1.00 GSBest_geom GSH_geom GSL_geom use stratum_HI_EEZ stratum_OtherCNP 1 3.00 4 2.00 TRUE TRUE TRUE 2 3.67 6 2.33 TRUE TRUE TRUE 3 2.00 3 1.00 TRUE TRUE TRUE 4 1.00 2 1.00 TRUE TRUE TRUE 5 1.00 1 1.00 TRUE TRUE TRUE 6 1.00 2 1.00 TRUE TRUE TRUE stratum_MHI stratum Phase 1 FALSE HI_EEZ 1 2 FALSE HI_EEZ 1 3 FALSE HI_EEZ 1 4 FALSE HI_EEZ 1 5 FALSE HI_EEZ 1 6 FALSE HI_EEZ 1 As above, let’s make sure the geostratum assignments for these sightings are simple: density_segments$stratum &lt;- &#39;HI_EEZ&#39; abundance_area This is the area, in square km, of the region of interest. The density estimate will be scaled by this area. We have two options for finding this area. The first is to draw the area from our cohort$strata slot: cruz$strata$area[cruz$strata$stratum == &#39;HI_EEZ&#39;] [1] 2474596 The second is to calculate it ourselves using the LTabundR function strata_area(). This second option will be useful if your study area is a complicated combination/substraction of multiple geostrata. data(strata_cnp) abundance_area &lt;- strata_area(strata_all = strata_cnp, strata_keep = &#39;HI_EEZ&#39;)$km2 abundance_area [1] 2474596 Remaining inputs iterations: Number of iterations to use in the various CV bootstrapping procedures occurring throughout this function, specifically: Effective Strip Half-Width CV estimation, school size CV estimation, weighted g(0) CV estimation, and encounter rate estimation. output_dir: The path in which results RData files should be stored. If left ““, the current working directory will be used. toplot: A Boolean, with default FALSE, indicating whether to plot various aspects of the analysis. verbose: A Boolean, with default TRUE, indicating whether to print status updates to the Console. density_bootstraps: Number of bootstrap iterations to use for the CV estimate of density and abundance specifically. This input allows this final step to use a different (typically larger) iteration size than the iterations input above. save(results_subgroup, file=&#39;results_subgroup.RData&#39;) load(&#39;results_subgroup.RData&#39;) Outputs The function returns a list with many slots, including estimates of density and abundance – along with estimates of intermediate parameters – with a CV derived from a bootstrapping routine. To demonstrate this output, we will use results from a call with only 10 bootstrap iterations. results_subgroup %&gt;% names [1] &quot;D&quot; &quot;D_CV&quot; &quot;D_L95&quot; [4] &quot;D_U95&quot; &quot;N&quot; &quot;N_CV&quot; [7] &quot;N_L95&quot; &quot;N_U95&quot; &quot;ER&quot; [10] &quot;ss&quot; &quot;n&quot; &quot;L&quot; [13] &quot;n_segments&quot; &quot;g0&quot; &quot;g0_cv&quot; [16] &quot;g0_details&quot; &quot;df&quot; &quot;bootstraps&quot; [19] &quot;iterations&quot; &quot;density_bootstraps&quot; Most of these slots hold best-estimates of parameters or sample size details: results_subgroup[c(1:15, 19:20)] $D 1 0.001663263 $D_CV 1 1.344861 $D_L95 [1] 0.0005004478 $D_U95 [1] 0.01547307 $N [1] 4116 $N_CV [1] 1.344829 $N_L95 [1] 1238.357 $N_U95 [1] 38290 $ER [1] 0.001860071 $ss [1] 2.466557 $n [1] 32 $L [1] 17203.64 $n_segments [1] 136 $g0 [1] 0.571 $g0_cv [1] 0.365 $iterations [1] 10 $density_bootstraps [1] 10000 The g0_details slot includes the results from the g0_model() and g0_weighted() functions called internally by lta_subgroup(). See those functions’ documentation pages for details. results_subgroup$g0_details %&gt;% names [1] &quot;Rg0&quot; &quot;gam&quot; &quot;jackknife&quot; &quot;summary&quot; &quot;sightings&quot; &quot;segments&quot; The df slot includes details of the detection function fit. See the documentation for df_plot() for details. results_subgroup$df %&gt;% names [1] &quot;best_models&quot; &quot;all_models&quot; &quot;best_objects&quot; &quot;sightings&quot; The bootstraps slot has the bootstrapped values for various parameters, in case they are useful for troubleshooting, subsequent analyses, and/or plotting: results_subgroup$bootstraps %&gt;% names [1] &quot;esw&quot; &quot;ss&quot; &quot;g0&quot; &quot;er&quot; &quot;D&quot; &quot;N&quot; Some examples: Behind the scenes This function performs the following operations: Fits a detection function to df_sits without covariates, using the LTabundR function df_fit(), in order to estimate the effective strip half-width (ESW). Conducts bootstrap re-sampling of the detection function fitting routine in order to estimate the CV of ESW. Estimates the geometric mean of subgroup school size based on the ss input. Creates a bootstrap-resampled distribution of subgroup school sizes, with which CV is estimated. Models the Relative g(0) in different survey conditions using the LTabundR function g0_model(). This function also estimates the CV of the Rg(0) estimate in each Beaufort sea state using jackknife resampling. Estimates the encounter rate (subgroup detections / trackline surveyed). Creates a bootstrap-resampled distribution of encounter rate estimates. Calculates a weighted g(0) estimate according to the proportion of effort occurring in each Beaufort sea state, then uses an automated parameter optimization routine (see details in LTabundR function g0_weighted()) to estimate the CV of the weighted g(0) estimate. Creates a bootstrap-resampled distribution of the weighted g(0) estimate. Estimates density using the best estimates of effective strip half-width, school size, g(0), and the encounter rate. Estimates abundance by scaling the density estimate by the provided abundance_area. Creates a bootstrap-resampled distribution of the density estimate by iteratively drawing values from the resampled distributions of the constituent parameters of the density equation. Creates a bootstrap-resampled distribtion of the abundance estimate by scaling the density distribution by abundance_area. Note that this approach could theoretically be used for other species that occur in subgroups. "],["tables.html", " 12 Tables Summary tables Appendix tables", " 12 Tables To demonstrate how LTA results can be summarized, tabularized, and plotted, we will use a built-in LTabundR dataset, which has denisty/abundance estimates for the Hawaiian EEZ in 2010 and 2017 for striped dolphins, Fraser’s dolphins, and melon-headed whales, ran with only 20 iterations: data(lta_result) We created these LTA results using the following built-in dataset: data(cnp_150km_1986_2020) Summary tables To summarize lta() results using the standard table format provided in recent NOAA stock assessment reports, use the function lta_report(). tables &lt;- lta_report(lta_result, cruz = cnp_150km_1986_2020, verbose = TRUE) Providing the cruz object is not required, but if it is not provided, one of the five summary tables ($table1a below) will not be returned. tables %&gt;% names [1] &quot;table1a&quot; &quot;table1b&quot; &quot;table2&quot; &quot;table3&quot; &quot;table4&quot; &quot;tableA1&quot; &quot;tableA2&quot; Table 1 in reports: Sample sizes Table 1a If cruz was provided, $table1a 1a will include total sighting counts for all species in the years from lta_result, broken down by region. The Ntot column holds all sightings, regardless of effort status or Beaufort sea state. Nsys holds counts of systematic-only sightings (i.e., EffType = “S” and Bft &lt;= 6), which may still include sightings that are beyond the species-specific truncation distance and were therefore excluded from density/abundance estimation. These counts are provided separately from the $table1b slot below, since those counts are based on the lta_result object, and will not include sightings for species that did not have a specific LTA estimate specified when it was made. We also include this separately so as to give the user full flexibility in how they summarize sighting counts by region/population/stock. tables$table1a %&gt;% DT::datatable(options=list(initComplete = htmlwidgets::JS( &quot;function(settings, json) {$(this.api().table().container()).css({&#39;font-size&#39;: &#39;9pt&#39;});}&quot;) )) Table 1b This table holds sighting counts used in estimates of density/abundance. The rows match the rows for Tables 3 and 4. In this table, columns are still prepared for total sightings (Ntot) and systematic sightings (Nsys), but they are left blank, since it is not clear how sightings from multiple regions in $table1a would be concatenated for this table. The user can fill in those gaps accordingly. tables$table1b %&gt;% DT::datatable(options=list(initComplete = htmlwidgets::JS( &quot;function(settings, json) {$(this.api().table().container()).css({&#39;font-size&#39;: &#39;9pt&#39;});}&quot;) )) Table 2 in reports: Detection functions Table 3: Parameter estimates Table 4: Density/abundance Appendix tables Table A1: Study areas Table A2: Effort totals (parsed by Beaufort sea state) tables$tableA2 $`2010` # A tibble: 1 × 10 Species Stratum Effort B0 B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 all HI_EEZ 16166 0.00124 0.0127 0.0383 0.117 0.480 0.303 0.0474 $`2017` # A tibble: 1 × 10 Species Stratum Effort B0 B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 all HI_EEZ 15913 0.000828 0.0102 0.0370 0.117 0.312 0.350 0.173 "],["plots.html", " 13 Plots Abundance plots Detection function plots", " 13 Plots Abundance plots To plot the abundance estimate with error bars representing the 95% confidence interval, use the function lta_plot(): lta_plot(lta_result, nrows=1) Detection function plots To plot the best-fit detection model for a species pool, use the df_plot() function. df_plot(lta_result) This function provides various stylization options, including the option to show multiple best-fitting models atop a single histogram of detections: df_plot(lta_result, model_colors=RColorBrewer::brewer.pal(n = 4, name = &quot;Dark2&quot;), model_pch = 16, pt_show=2, pt_alpha=.3, bootstrap_show = FALSE, legend_show=TRUE, legend_x=2.8) "],["whiceas.html", " 14 WHICEAS Data processing Data exploration Rg0 Density &amp; abundance Results Validation", " 14 WHICEAS Here we demonstrate code that reproduces the Bradford et al. (2022) WHICEAS report within the new LTabundR framework. This study estimates cetacean abundance for Hawaiian WHICEAS study area for 2017 and 2020. Here we use survey data from 1986 to 2020 to estimate Relative g(0) and detection functions. Currently, coefficients of variation (CV) of density and abundance are estimated using only 100 bootstrap iterations (the publication uses 1,000) to reduce processing time. library(dplyr) library(LTabundR) library(ggplot2) Data processing Settings Survey-wide settings data(species_codes) data(ships) data(group_size_coefficients) edits &lt;- readRDS(&#39;cnp_1986_2020_edits.RData&#39;) survey &lt;- load_survey_settings( out_handling = &#39;remove&#39;, max_row_interval = Inf, segment_method = &quot;equallength&quot;, segment_target_km = 150, segment_max_interval = 24, segment_remainder_handling = c(&quot;segment&quot;), ship_list = ships, species_codes = species_codes, group_size_coefficients = group_size_coefficients, smear_angles = FALSE ) Geostrata data(strata_cnp) Cohort-specific settings Cohort 1: all species all_species &lt;- load_cohort_settings( id = &quot;all&quot;, # * species = NULL, strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;), # * probable_species = FALSE, sighting_method = 0, cue_range = 0:7, school_size_range = c(0, 10000), school_size_calibrate = TRUE, calibration_floor = 0, use_low_if_na = TRUE, io_sightings = 0, geometric_mean_group = TRUE, truncation_km = 7.5, # * beaufort_range = 0:6, abeam_sightings = TRUE, strata_overlap_handling = c(&quot;smallest&quot;), distance_types = c(&#39;S&#39;,&#39;F&#39;,&#39;N&#39;), distance_modes = c(&#39;P&#39;,&#39;C&#39;), distance_on_off = TRUE ) Cohort 2: bottlenose dolphins bottlenose &lt;- load_cohort_settings( id = &quot;bottlenose&quot;, species = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;, &#39;Bottlenose_BI&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_KaNi&#39;), truncation_km = 7.5) Cohort 3: pantropical spotted dolphins spotted &lt;- load_cohort_settings( id = &quot;spotted&quot;, species = &#39;002&#39;, strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;, &#39;Spotted_OU&#39;,&#39;Spotted_FI&#39;,&#39;Spotted_BI&#39;), truncation_km = 7.5) Process settings &lt;- load_settings(strata = strata_cnp, survey = survey, cohorts = list(all_species, bottlenose, spotted)) cruz &lt;- process_surveys(das_file = &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39;, settings = settings, edits = edits) save(cruz, file=&#39;whiceas/whiceas_cruz.RData&#39;) load(&#39;whiceas/whiceas_cruz.RData&#39;) Data exploration Processed data structure cruz_structure(cruz) Map of sightings For the entire cruz object: map_cruz(cruz, sightings_color = &#39;firebrick&#39;) For just the surveys of interest: # Filter cruz1720 &lt;- filter_cruz(cruz, years = c(2017, 2020), regions = &#39;WHICEAS&#39;, eff_types = &#39;S&#39;, bft_range = 0:6) # Map, including survey tracks map_cruz(cruz1720, sightings_color = &#39;firebrick&#39;, effort_show = TRUE, effort_resolution = 3, effort_weight = 4, effort_opacity = .4) Interactive dashboard Use this to determine truncation distances. Note them in your lta() code below: cruz_explorer(cruz) Rg0 We can use the built-in dataset, data(g0_results), which has Beaufort-specific Relative g(0) estimates for most species based on 1986-2020 surveys. data(&quot;g0_results&quot;) Rg0 &lt;- g0_results # Plot the results: g0_plot(Rg0, panes = 3) To explore the effects of LTabundR’s g(0) estimation routines on our abundance estimates, we will be running the analyses below with three different g(0) scenarios. In scenario 1, we will manually specify the weighted g(0) and its CV for each species in 2020; these results should be nearly exact replicates of those from Bradford et al. (2021), and we can be sure that any discrepancy between the two sets of results are not attributable the g(0) aspect of the analysis. Second, we will allow LTabundR to calculate the weighted g(0) and its CV using the same Relative g(0) values that were used in Bradford et al. (2021) (i.e., the results of Barlow (2015), which used surveys from 1986 to 2010 and is provided in LTabundR as a built-in dataset), such that any difference between scenarios 1 and 2 are likely due to the weighted g(0) subroutines. Finally, in scenario 3, we will use the Relative g(0) esitmates produced by LTabundR using a more extensive survey dataset (1986-2020). For each g(0) scenario, our Rg0 object and results_path will differ: # Specify scenario number here g0_scenario &lt;- 2 # Set Rg0 source and results path if(g0_scenario == 1){ # Using manually-specified g0 and its CV Rg0 &lt;- NULL results_path &lt;- &#39;whiceas/lta_manual/&#39; } if(g0_scenario == 2){ # Using Rg0 estimates from Barlow 2015 data(barlow_2015) Rg0 &lt;- barlow_2015 results_path &lt;- &#39;whiceas/lta_barlow/&#39; } if(g0_scenario == 3){ # New LTabundR estimates of Rg0 data(g0_results) Rg0 &lt;- g0_results results_path &lt;- &#39;whiceas/lta/&#39; } Density &amp; abundance First we can define common values that will be constant across all estimates we produce: bootstraps &lt;- 200 years &lt;- 1986:2020 fit_regions &lt;- NULL fit_not_regions &lt;- NULL toplot = TRUE verbose = TRUE df_settings &lt;- list(covariates = c(&#39;bft&#39;,&#39;lnsstot&#39;,&#39;cruise&#39;,&#39;year&#39;,&#39;ship&#39;,&#39;species&#39;), covariates_factor = c(FALSE, FALSE, TRUE, TRUE, TRUE, TRUE), covariates_levels = 2, covariates_n_per_level = 10, simplify_cue = TRUE, simplify_bino = TRUE, detection_function_base = &#39;hn&#39;, base_model = &#39;~1&#39;, delta_aic = 2) For most species, we want to estimate density/abundance for the same set of year-region scenarios. To reduce code redundancy, as well as the risk of typing errors (and our work!), we can use the LTabundR function lta_estimates() to economize how we prepare our estimates input. For most species, these are the year-region scenarios for which we want estimates: scenarios &lt;- list(list(years = 2017, regions = &#39;WHICEAS&#39;), list(years = 2020, regions = &#39;WHICEAS&#39;)) The lta_estimates() function will generate a custom function that makes it easy to create a set of estimates sub-lists for each species of interest: estimator &lt;- lta_estimates(scenarios) That result, estimator, is actually a function. Here’s an example of how this function will work, using the first species pool as an example: estimates &lt;- c(estimator(spp = &#39;013&#39;, title = &quot;Striped dolphin&quot;), estimator(spp = &#39;026&#39;, title = &quot;Fraser&#39;s dolphin&quot;, alt_g0_spp = &#39;013&#39;), estimator(spp = &#39;031&#39;, title = &quot;Melon-headed whale&quot;, alt_g0_spp = &#39;013&#39;)) estimates [[1]] [[1]]$years [1] 2017 [[1]]$regions [1] &quot;WHICEAS&quot; [[1]]$spp [1] &quot;013&quot; [[1]]$title [1] &quot;Striped dolphin&quot; [[2]] [[2]]$years [1] 2020 [[2]]$regions [1] &quot;WHICEAS&quot; [[2]]$spp [1] &quot;013&quot; [[2]]$title [1] &quot;Striped dolphin&quot; [[3]] [[3]]$years [1] 2017 [[3]]$regions [1] &quot;WHICEAS&quot; [[3]]$spp [1] &quot;026&quot; [[3]]$title [1] &quot;Fraser&#39;s dolphin&quot; [[3]]$alt_g0_spp [1] &quot;013&quot; [[4]] [[4]]$years [1] 2020 [[4]]$regions [1] &quot;WHICEAS&quot; [[4]]$spp [1] &quot;026&quot; [[4]]$title [1] &quot;Fraser&#39;s dolphin&quot; [[4]]$alt_g0_spp [1] &quot;013&quot; [[5]] [[5]]$years [1] 2017 [[5]]$regions [1] &quot;WHICEAS&quot; [[5]]$spp [1] &quot;031&quot; [[5]]$title [1] &quot;Melon-headed whale&quot; [[5]]$alt_g0_spp [1] &quot;013&quot; [[6]] [[6]]$years [1] 2020 [[6]]$regions [1] &quot;WHICEAS&quot; [[6]]$spp [1] &quot;031&quot; [[6]]$title [1] &quot;Melon-headed whale&quot; [[6]]$alt_g0_spp [1] &quot;013&quot; The output of estimator() is a list of sub-lists specifying a set of density/abundance estimates you want to produce based on the detection function for a single species pool. Here is the full code for producing those estimates for all species from Bradford et al. (2021): Multi-species pool 1 # Striped dolphin (013), Fraser&#39;s dolphin (026), Melon-headed whale (031) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;013&#39;, &#39;026&#39;, &#39;031&#39;), pool = &#39;Multi-species pool 1&#39;, cohort = &#39;all&#39;, truncation_distance = 5, other_species = &#39;remove&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;013&#39;, title = &quot;Striped dolphin&quot;), estimator(spp = &#39;026&#39;, title = &quot;Fraser&#39;s dolphin&quot;, alt_g0_spp = &#39;013&#39;), estimator(spp = &#39;031&#39;, title = &quot;Melon-headed whale&quot;, alt_g0_spp = &#39;013&#39;)) estimates # Manually specify g0 and its CV -- only for g0_scenario 1 # Two specifications per species, one for 2017 and one for 2020 if(g0_scenario==1){ # Striped estimates[[1]]$g0 &lt;- 0.35; estimates[[1]]$g0_cv &lt;- 0.19 estimates[[2]]$g0 &lt;- 0.31; estimates[[2]]$g0_cv &lt;- 0.22 # Fraser&#39;s estimates[[3]]$g0 &lt;- 0.35; estimates[[3]]$g0_cv &lt;- 0.10 estimates[[4]]$g0 &lt;- 0.31; estimates[[4]]$g0_cv &lt;- 0.22 # Melon-headed estimates[[5]]$g0 &lt;- 0.35; estimates[[5]]$g0_cv &lt;- 0.19 estimates[[6]]$g0 &lt;- 0.31; estimates[[6]]$g0_cv &lt;- 0.22 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 2 # Rough-toothed dolphin (15), Common bottlenose dolphin (18), Risso&#39;s (21), # Pygmy killer whale (32) # Notes # Bottlenose abundance is estimated in a separate cohort, but included here for DF fitting if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), pool = &#39;Multi-species pool 2&#39;, cohort = &#39;all&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;015&#39;, title = &quot;Rough-toothed dolphin&quot;), estimator(spp = &#39;021&#39;, title = &quot;Risso&#39;s dolphin&quot;), estimator(spp = &#39;032&#39;, title = &quot;Pygmy killer whale&quot;)) if(g0_scenario==1){ # Rough-toothed estimates[[1]]$g0 &lt;- 0.09; estimates[[1]]$g0_cv &lt;- 0.45 estimates[[2]]$g0 &lt;- 0.07; estimates[[2]]$g0_cv &lt;- 0.51 # Risso&#39;s estimates[[3]]$g0 &lt;- 0.57; estimates[[3]]$g0_cv &lt;- 0.18 estimates[[4]]$g0 &lt;- 0.52; estimates[[4]]$g0_cv &lt;- 021 # Pygmy killer estimates[[5]]$g0 &lt;- 0.14; estimates[[5]]$g0_cv &lt;- 0.25 estimates[[6]]$g0 &lt;- 0.11; estimates[[6]]$g0_cv &lt;- 0.28 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 3 # Short-finned pilot whale (036), Longman&#39;s beaked whale (065) # No Rg(0) available for Longman&#39;s -- will use SF pilot whale instead to estimate its weighted g0 if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;036&#39;, &#39;065&#39;), pool = &#39;Multi-species pool 3&#39;, cohort = &#39;all&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;036&#39;, title = &quot;Short-finned pilot whale&quot;), estimator(spp = &#39;065&#39;, title = &quot;Longman&#39;s beaked whale&quot;, alt_g0_spp = &#39;036&#39;)) if(g0_scenario==1){ # Short-finned estimates[[1]]$g0 &lt;- 0.58; estimates[[1]]$g0_cv &lt;- 0.15 estimates[[2]]$g0 &lt;- 0.52; estimates[[2]]$g0_cv &lt;- 0.19 # Longman&#39;s estimates[[3]]$g0 &lt;- 0.58; estimates[[3]]$g0_cv &lt;- 0.15 estimates[[4]]$g0 &lt;- 0.52; estimates[[4]]$g0_cv &lt;- 0.19 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 4 # Killer whale (37), sperm whale (46) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;037&#39;, &#39;046&#39;), pool = &#39;Multi-species pool 4&#39;, cohort = &#39;all&#39;, truncation_distance = 5.5, other_species = &#39;remove&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;037&#39;, title = &quot;Killer whale&quot;), estimator(spp = &#39;046&#39;, title = &quot;Sperm whale&quot;)) if(g0_scenario==1){ # Killer (no sightings in ALB et al 2021) estimates[[1]]$g0 &lt;- 1; estimates[[1]]$g0_cv &lt;- 0 estimates[[2]]$g0 &lt;- 1; estimates[[2]]$g0_cv &lt;- 0 # Sperm estimates[[3]]$g0 &lt;- 0.63; estimates[[3]]$g0_cv &lt;- 0.34 estimates[[4]]$g0 &lt;- 0.61; estimates[[4]]$g0_cv &lt;- 0.37 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 5 # Pygmy sperm whale (47), dwarf sperm whale (48), UNID Kogia (80), # Blainville&#39;s beaked whale (59), Cuvier&#39;s beaked whale (61), # UNID Mesoplodon (51), UNID beaked whale (49), Minke whale (71) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;047&#39;, &#39;048&#39;, &#39;080&#39;, &#39;059&#39;, &#39;061&#39;, &#39;051&#39;, &#39;049&#39;, &#39;071&#39;), pool = &#39;Multi-species pool 5&#39;, cohort = &#39;all&#39;, truncation_distance = 4.5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;047&#39;, title = &quot;Pygmy sperm whale&quot;), estimator(spp = &#39;048&#39;, title = &quot;Dwarf sperm whale&quot;), estimator(spp = &#39;080&#39;, title = &quot;Unidentified Kogia&quot;), estimator(spp = &#39;059&#39;, title = &quot;Blainville&#39;s beaked whale&quot;), estimator(spp = &#39;061&#39;, title = &quot;Cuvier&#39;s beaked whale&quot;), estimator(spp = &#39;051&#39;, title = &quot;Unidentified Mesoplodon&quot;), estimator(spp = &#39;049&#39;, title = &quot;Unidentified beaked whale&quot;, alt_g0_spp = c(&#39;061&#39;,&#39;051&#39;), combine_g0 = TRUE), estimator(spp = &#39;071&#39;, title = &quot;Minke whale&quot;)) # Note Barlow2015 provides absolute estimates for Cuviers and UNID Mesop if(g0_scenario==1){ # Pygmy sperm (no sightings in ALB, using other kogia values) estimates[[1]]$g0 &lt;- 0.005; estimates[[1]]$g0_cv &lt;- 0.15 estimates[[2]]$g0 &lt;- 0.004; estimates[[2]]$g0_cv &lt;- 0.15 # Dwarf sperm (no ALB sightings in 2017, using 2020 g0) estimates[[3]]$g0 &lt;- 0.005; estimates[[3]]$g0_cv &lt;- 0.15 estimates[[4]]$g0 &lt;- 0.004; estimates[[4]]$g0_cv &lt;- 0.15 # UNID Kogia (no ALB sightings 2020, using 2017 g0) estimates[[5]]$g0 &lt;- 0.005; estimates[[5]]$g0_cv &lt;- 0.15 estimates[[6]]$g0 &lt;- 0.004; estimates[[6]]$g0_cv &lt;- 0.15 # Blainville&#39;s (no ALB sightings 2017, using 2020 g0) estimates[[7]]$g0 &lt;- 0.11; estimates[[7]]$g0_cv &lt;- 0.30 estimates[[8]]$g0 &lt;- 0.11; estimates[[6]]$g0_cv &lt;- 0.30 # Cuvier&#39;s (no sightings in ALB -- using unid beaked g0) estimates[[9]]$g0 &lt;- 0.13; estimates[[9]]$g0_cv &lt;- 0.20 estimates[[10]]$g0 &lt;- 0.11; estimates[[10]]$g0_cv &lt;- 0.21 # UNID Mesop (no ALB sightings 2017, using 2020 g0) estimates[[11]]$g0 &lt;- 0.11; estimates[[11]]$g0_cv &lt;- 0.30 estimates[[12]]$g0 &lt;- 0.11; estimates[[12]]$g0_cv &lt;- 0.30 # UNID beaked estimates[[13]]$g0 &lt;- 0.13; estimates[[13]]$g0_cv &lt;- 0.20 estimates[[14]]$g0 &lt;- 0.11; estimates[[14]]$g0_cv &lt;- 0.21 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 6 # Bryde&#39;s whale (72), Sei whale (73), Fin whale (74), Blue whale (75), # Sei/Bryde&#39;s (99), Fin/Sei/Bryde&#39;s (72, 73, 74, 99) # Bryde&#39;s, Sei&#39;s, and Sei/Bryde&#39;s all use same Rg0 (title = &quot;Sei/Bryde&#39;s&quot;) # Sei/Bryde&#39;s/Fin use an average of Fin and Sei/Bryde&#39;s. if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;072&#39;, &#39;073&#39;, &#39;074&#39;,&#39;075&#39;,&#39;099&#39;), pool = &#39;Multi-species pool 6&#39;, cohort = &#39;all&#39;, truncation_distance = 5.0, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;072&#39;, title = &quot;Bryde&#39;s whale&quot;), estimator(spp = &#39;073&#39;, title = &quot;Sei whale&quot;), estimator(spp = &#39;074&#39;, title = &quot;Fin whale&quot;), estimator(spp = &#39;075&#39;, title = &quot;Blue whale&quot;), estimator(spp = &#39;099&#39;, title = &quot;Sei/Bryde&#39;s whale&quot;), estimator(spp = c(&#39;072&#39;, &#39;073&#39;, &#39;099&#39;, &#39;074&#39;), title = &quot;Sei/Bryde&#39;s/Fin whale&quot;, combine_g0 = TRUE)) if(g0_scenario==1){ # Brydes (no ALB sightings -- using sei values) estimates[[1]]$g0 &lt;- 0.38; estimates[[1]]$g0_cv &lt;- 0.21 estimates[[2]]$g0 &lt;- 0.38; estimates[[2]]$g0_cv &lt;- 0.21 # Sei (no ALB sightings in 2017, using 2020 g0) estimates[[3]]$g0 &lt;- 0.38; estimates[[3]]$g0_cv &lt;- 0.21 estimates[[4]]$g0 &lt;- 0.38; estimates[[4]]$g0_cv &lt;- 0.21 # Fin (no ALB sightings in 2017, using 2020 g0) estimates[[5]]$g0 &lt;- 0.30; estimates[[5]]$g0_cv &lt;- 0.29 estimates[[6]]$g0 &lt;- 0.30; estimates[[6]]$g0_cv &lt;- 0.29 # Blue (no ALB sightings, using fin values) estimates[[7]]$g0 &lt;- 0.30; estimates[[7]]$g0_cv &lt;- 0.29 estimates[[8]]$g0 &lt;- 0.30; estimates[[8]]$g0_cv &lt;- 0.29 # Sei / Bryde&#39;s (no ALB sightings in 2017, using 2020 g0) estimates[[9]]$g0 &lt;- 0.38; estimates[[9]]$g0_cv &lt;- 0.21 estimates[[10]]$g0 &lt;- 0.38; estimates[[10]]$g0_cv &lt;- 0.21 # Sei / Bryde&#39;s / Fin (no ALB sightings in 2017, using 2020 g0) estimates[[11]]$g0 &lt;- 0.34; estimates[[11]]$g0_cv &lt;- 0.17 estimates[[12]]$g0 &lt;- 0.34; estimates[[12]]$g0_cv &lt;- 0.17 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Humpback whale if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;076&#39;), pool = &#39;Humpback whale&#39;, cohort = &#39;all&#39;, truncation_distance = 5.5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;-c(estimator(spp = &#39;076&#39;, title = &quot;Humpback whale&quot;)) if(g0_scenario==1){ # No ALB sightings in 2017, using 2020 g0 estimates[[1]]$g0 &lt;- 0.68; estimates[[1]]$g0_cv &lt;- 0.36 estimates[[2]]$g0 &lt;- 0.68; estimates[[2]]$g0_cv &lt;- 0.36 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Unidentified rorquals # UNID rorquals (70) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;070&#39;), pool = &#39;Unidentified rorqual&#39;, cohort = &#39;all&#39;, truncation_distance = 5.5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;070&#39;, title = &quot;Unidentified rorqual&quot;, alt_g0_spp = c(&#39;071&#39;,&#39;099&#39;,&#39;074&#39;,&#39;075&#39;), combine_g0 = TRUE)) if(g0_scenario==1){ estimates[[1]]$g0 &lt;- 0.35; estimates[[1]]$g0_cv &lt;- 0.18 estimates[[2]]$g0 &lt;- 0.32; estimates[[2]]$g0_cv &lt;- 0.20 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) results$bootstrap$summary %&gt;% as.data.frame # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Unidentified dolphins # UNID dolphin (177, 277, 377, 77) if(TRUE){ # toggle spp &lt;- c(&#39;177&#39;,&#39;277&#39;,&#39;377&#39;,&#39;077&#39;) pool_title &lt;- &#39;Unidentified dolphin&#39; # Detection function specifications fit_filters &lt;- list(spp = c(&#39;177&#39;,&#39;277&#39;,&#39;377&#39;,&#39;077&#39;), pool = pool_title, cohort = &#39;all&#39;, truncation_distance = 5.5, other_species = &#39;coerce&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- estimator(spp = spp, title = pool_title, alt_g0_spp = c(&#39;002&#39;,&#39;013&#39;,&#39;018&#39;,&#39;015&#39;, &#39;036&#39;, &#39;021&#39;), combine_g0 = TRUE) if(g0_scenario==1){ estimates[[1]]$g0 &lt;- 0.33; estimates[[1]]$g0_cv &lt;- 0.08 estimates[[2]]$g0 &lt;- 0.29; estimates[[2]]$g0_cv &lt;- 0.10 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Unidentified cetaceans # UNID cetacean (78, 79, 98, 96) if(TRUE){ # toggle spp &lt;- c(&#39;078&#39;,&#39;079&#39;,&#39;098&#39;,&#39;096&#39;) pool_title &lt;- &#39;Unidentified cetacean&#39; # Detection function specifications fit_filters &lt;- list(spp = spp, pool = pool_title, cohort = &#39;all&#39;, truncation_distance = 5.5, other_species = &#39;coerce&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- estimator(spp = spp, title = pool_title, g0=1.0, g0_cv = 0.0) if(g0_scenario==1){ estimates[[1]]$g0 &lt;- 1.0; estimates[[1]]$g0_cv &lt;- 0.0 estimates[[2]]$g0 &lt;- 1.0; estimates[[2]]$g0_cv &lt;- 0.0 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Bottlenose dolphin # Bottlenose dolphin (018) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), pool = &#39;Bottlenose dolphin&#39;, cohort = &#39;bottlenose&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan scenarios &lt;- list( list(years = 2017, regions = &#39;WHICEAS&#39;, regions_remove = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;), region_title = &#39;(WHICEAS)&#39;), list(years = 2020, regions = &#39;WHICEAS&#39;, regions_remove = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;), region_title = &#39;(WHICEAS)&#39;)) estimator &lt;- lta_estimates(scenarios) estimates &lt;- estimator(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;) if(g0_scenario==1){ # No ALB sightings in 2017, using 2020 g0 estimates[[1]]$g0 &lt;- 0.24; estimates[[1]]$g0_cv &lt;- 0.38 estimates[[2]]$g0 &lt;- 0.24; estimates[[2]]$g0_cv &lt;- 0.38 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Pantropical spotted dolphin # Pantropical spotted dolphin (002) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;002&#39;), pool = &#39;Pantropical spotted dolphin&#39;, cohort = &#39;spotted&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan scenarios &lt;- list( list(years = 2017, regions = &#39;WHICEAS&#39;, regions_remove = c(&#39;Spotted_OU&#39;, &#39;Spotted_FI&#39;, &#39;Spotted_BI&#39;), region_title = &#39;(WHICEAS)&#39;), list(years = 2020, regions = &#39;WHICEAS&#39;, regions_remove = c(&#39;Spotted_OU&#39;, &#39;Spotted_FI&#39;, &#39;Spotted_BI&#39;), region_title = &#39;(WHICEAS)&#39;)) estimator &lt;- lta_estimates(scenarios) estimates &lt;- estimator(spp = &#39;002&#39;, title = &#39;Pantropical spotted dolphin&#39;) if(g0_scenario==1){ estimates[[1]]$g0 &lt;- 0.28; estimates[[1]]$g0_cv &lt;- 0.11 estimates[[2]]$g0 &lt;- 0.25; estimates[[2]]$g0_cv &lt;- 0.13 } # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Results To review results, we will use g(0) scenario 3 (new auto-generated estimates of Relative g(0) from LTabundR). # Load results ltas &lt;- lta_enlist(&#39;whiceas/lta/&#39;) Tables Generate report: reporti &lt;- lta_report(ltas, cruz) Table 1. Sample sizes. The lta_report() function above attempts to generate sample size tables based on the cruz object and ltas results (see $table1a and $table1b outputs of lta_report()), but this is difficult to generalize into an automatic function, especially when cohort-specific geostrata are involved. To determine sample sizes with more control, we can write a quick helper function: sample_size &lt;- function(cruz, spp, cohort, years, td, in_region=NULL, region_remove = NULL){ suppressMessages({ sits &lt;- cruz$cohorts[[cohort]]$sightings %&gt;% filter(species %in% spp) %&gt;% filter(year %in% years) %&gt;% filter(stratum %in% in_region) %&gt;% filter(! stratum %in% region_remove) %&gt;% mutate(species = paste(spp, collapse=&#39;/&#39;)) %&gt;% group_by(species, year) %&gt;% summarize(ntot = n(), nsys = length(which(use == TRUE &amp; Bft &lt;= 6 &amp; EffType == &#39;S&#39;)), nest = length(which(use == TRUE &amp; included == TRUE &amp; Bft &lt;= 6 &amp; EffType == &#39;S&#39; &amp; PerpDistKm &lt;= td))) %&gt;% tidyr::pivot_longer(cols = ntot:nest) %&gt;% tidyr::pivot_wider(id_cols = species, names_from = year:name, values_from = value) }) return(sits) } Now we can use this function to generate sample size totals for each species/stock of interest: # Save years to re-use in the lines below years &lt;- c(2017, 2020) # Spotted dolphin sample_size(cruz, spp = &#39;002&#39;, cohort = 3, years, td = 5, in_region = &#39;WHICEAS&#39;, region_remove = c(&#39;Spotted_OU&#39;,&#39;Spotted_FI&#39;,&#39;Spotted_BI&#39;)) # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 002 10 7 6 6 4 # ℹ 1 more variable: `2020_nest` &lt;int&gt; # More efficient code for remaining species: sample_size(cruz, &#39;013&#39;, 1, years, 5, &#39;WHICEAS&#39;) # striped dolphin # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 013 4 3 3 8 3 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;015&#39;, 1, years, 5, &#39;WHICEAS&#39;) # rough-toothed dolphin # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 015 18 4 4 7 4 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;018&#39;, 2, years, 5, &#39;WHICEAS&#39;, # bottlenose dolphin c(&#39;Bottlenose_KaNi&#39;,&#39;Bottlenose_OUFI&#39;,&#39;Bottlenose_BI&#39;)) # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 018 1 0 0 3 3 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;021&#39;, 1, years, 5, &#39;WHICEAS&#39;) # risso&#39;s dolphin # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 021 5 2 2 5 4 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;026&#39;, 1, years, 5, &#39;WHICEAS&#39;) # fraser&#39;s dolphin # A tibble: 1 × 4 # Groups: species [1] species `2020_ntot` `2020_nsys` `2020_nest` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 026 3 2 2 sample_size(cruz, &#39;031&#39;, 1, years, 5, &#39;WHICEAS&#39;) # melon-headed whale # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 031 4 2 2 6 3 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;032&#39;, 1, years, 5, &#39;WHICEAS&#39;) # pygmy killer whale # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 032 2 1 1 3 3 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;036&#39;, 1, years, 5, &#39;WHICEAS&#39;) # short-finned pilot whale # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 036 27 1 1 6 5 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;037&#39;, 1, years, 5.5, &#39;WHICEAS&#39;) # killer whale # A tibble: 0 × 1 # Groups: species [0] # ℹ 1 variable: species &lt;chr&gt; sample_size(cruz, &#39;046&#39;, 1, years, 5.5, &#39;WHICEAS&#39;) # sperm whale # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 046 5 1 1 14 10 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;047&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # pygmy sperm whale # A tibble: 0 × 1 # Groups: species [0] # ℹ 1 variable: species &lt;chr&gt; sample_size(cruz, &#39;048&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # dwarf sperm whale # A tibble: 1 × 4 # Groups: species [1] species `2020_ntot` `2020_nsys` `2020_nest` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 048 1 1 1 sample_size(cruz, &#39;080&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # UNID Kogia # A tibble: 1 × 4 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 080 2 1 1 sample_size(cruz, &#39;049&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # UNID beaked whale # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 049 4 1 1 4 4 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;051&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # UNID Mesoplodon # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 051 1 0 0 3 2 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;059&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # Blainville&#39;s beaked whale # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 059 3 0 0 2 0 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;061&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # Cuvier&#39;s beaked whale # A tibble: 1 × 4 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 061 3 0 0 sample_size(cruz, &#39;065&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # Longman&#39;s beaked whale # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 065 3 2 2 1 1 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;070&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # UNID rorqual # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 070 2 1 1 15 4 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;071&#39;, 1, years, 4.5, &#39;WHICEAS&#39;) # Minke whale # A tibble: 1 × 4 # Groups: species [1] species `2020_ntot` `2020_nsys` `2020_nest` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 071 1 1 0 sample_size(cruz, c(&#39;199&#39;), 1, years, 5, &#39;WHICEAS&#39;) # Sei/Bryde&#39;s/Fin # A tibble: 1 × 4 # Groups: species [1] species `2020_ntot` `2020_nsys` `2020_nest` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 199 1 1 1 sample_size(cruz, &#39;073&#39;, 1, years, 5, &#39;WHICEAS&#39;) # Sei whale # A tibble: 1 × 4 # Groups: species [1] species `2020_ntot` `2020_nsys` `2020_nest` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 073 5 3 3 sample_size(cruz, &#39;074&#39;, 1, years, 5, &#39;WHICEAS&#39;) # Fin whale # A tibble: 1 × 4 # Groups: species [1] species `2020_ntot` `2020_nsys` `2020_nest` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 074 1 1 1 sample_size(cruz, &#39;076&#39;, 1, years, 5.5, &#39;WHICEAS&#39;) # Humpback whale # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 076 3 0 0 164 85 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, &#39;099&#39;, 1, years, 5, &#39;WHICEAS&#39;) # Sei/Brydes # A tibble: 1 × 4 # Groups: species [1] species `2020_ntot` `2020_nsys` `2020_nest` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 099 5 4 4 sample_size(cruz, c(&#39;177&#39;,&#39;277&#39;,&#39;377&#39;,&#39;077&#39;), 1, years, 5.5, &#39;WHICEAS&#39;) # UNID dolphin # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 177/277/377/077 18 4 3 23 11 # ℹ 1 more variable: `2020_nest` &lt;int&gt; sample_size(cruz, c(&#39;078&#39;,&#39;079&#39;,&#39;098&#39;,&#39;096&#39;), 1, years, 5.5, &#39;WHICEAS&#39;) # UNID cetacean # A tibble: 1 × 7 # Groups: species [1] species `2017_ntot` `2017_nsys` `2017_nest` `2020_ntot` `2020_nsys` &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 078/079/098/096 6 1 1 23 13 # ℹ 1 more variable: `2020_nest` &lt;int&gt; To expedite building up this sample size table, consider copying and pasting the table produced by lta_report()$table1b, then filling in the blanks with values from above: Table 1b. Sample sizes of sightings used in density esitmation. Table 2. Detection functions for cetacean species and taxonomic categories. reporti$table2 Table 3. Estimates of line-transect parameters for cetacean species and taxonomic categories. reporti$table3 Table 4. Estimates of density (individuals per 1,000 km2) and abundance for cetacean species and taxonomic categories sighted while on systematic survey effort. reporti$table4 Table A1. Study areas. reporti$tableA1 Table A2. Effort totals and by Beaufort sea-state, in each survey year. reporti$tableA2 $`2017` # A tibble: 3 × 9 Species Stratum Effort B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 spotted WHICEAS 2787 0.0125 0.103 0.114 0.304 0.368 0.0987 2 bottlenose WHICEAS 2939 0.0119 0.0911 0.114 0.312 0.380 0.0910 3 all WHICEAS 3040 0.0115 0.0942 0.111 0.320 0.370 0.0934 $`2020` # A tibble: 3 × 9 Species Stratum Effort B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 spotted WHICEAS 4079 0.0147 0.0382 0.0710 0.274 0.401 0.201 2 bottlenose WHICEAS 4417 0.0174 0.0390 0.0715 0.281 0.407 0.185 3 all WHICEAS 4585 0.0170 0.0406 0.0695 0.281 0.407 0.184 Plots lta_plot(species = NULL, lta_result = ltas, years = c(2017, 2020)) Validation To validate our results and the routines within LTabundR, we will compare the LTabundR WHICEAS results to those from A.L. Bradford et al. (2022). To do so, we first bring in their results, which we’ve staged in a GoogleSheet provided in the code chunk below: library(gsheet) url &lt;- &#39;https://docs.google.com/spreadsheets/d/1S94I9I0R589Z8PFP41lYOF8hM6yetap7RFlPdBeet1M/edit?usp=sharing&#39; alb &lt;- gsheet2tbl(url) Next we format these data to facilitate joining and analyzing alongside our new WHICEAS results within a tidyverse framework: library(dplyr) library(tidyr) alb &lt;- alb %&gt;% mutate(title=gsub(&quot;&#39;&quot;,&quot;&quot;,species)) %&gt;% tidyr::pivot_longer(3:ncol(alb), names_to=&#39;column&#39;, values_to=&#39;value&#39;, values_transform = as.character) %&gt;% mutate(source=&#39;ALB et al. (2022)&#39;, title = species, code = stringr::str_pad(code,width=3, side=&#39;left&#39;, pad=&#39;0&#39;), year = ifelse(grepl(&#39;17&#39;, column), 2017, 2020) %&gt;% as.numeric) %&gt;% select(-species) %&gt;% rename(species = code) %&gt;% mutate(column = gsub(&#39;17&#39;,&#39;&#39;,column)) %&gt;% mutate(column = gsub(&#39;20&#39;,&#39;&#39;,column)) %&gt;% group_by(title, species, year) %&gt;% summarize(g0 = value[column == &#39;g0&#39;][1] %&gt;% as.numeric, g0_cv = value[column == &#39;gcv&#39;][1] %&gt;% as.numeric, ESW = value[column == &#39;esw&#39;] %&gt;% as.numeric, ss = value[column == &#39;ss&#39;] %&gt;% as.numeric, D = value[column == &#39;d&#39;] %&gt;% as.numeric, N = gsub(&#39;,&#39;,&#39;&#39;, value[column == &#39;n&#39;]) %&gt;% as.numeric, CV = value[column == &#39;cv&#39;] %&gt;% as.numeric, L95 = gsub(&#39;,&#39;,&#39;&#39;, value[column == &#39;lci&#39;]) %&gt;% as.numeric, U95 = gsub(&#39;,&#39;,&#39;&#39;, value[column == &#39;uci&#39;]) %&gt;% as.numeric) %&gt;% filter(!is.na(D)) %&gt;% mutate(N = round(N)) # Modify names names(alb)[4:ncol(alb)] &lt;- paste0(&#39;alb_&#39;,names(alb)[4:ncol(alb)] ) Review these results: alb LTabundR results We will compare the ALB et al. (2022) results to all three versions of the WHICEAS analysis we have produced: (1) manually specified g(0) parameters, (2) Rg(0) estimates from Barlow et al. (2015), and (3) new Rg(0) estimates produced by LTabundR automatically. We will bring those results back in and join each to the results from ALB et al. (2022), using a custom helper function, whiceas_join(): whiceas_join &lt;- function(alb, ltas){ # Combine core info from each LTA list into a dataframe ltabundr &lt;- data.frame() for(i in 1:length(ltas)){ lti &lt;- ltas[[i]] ltabundi &lt;- left_join(lti$estimate, lti$bootstrap$summary %&gt;% select(title, Region, year, species, iterations, g0_cv, CV, L95, U95), by=c(&#39;title&#39;, &#39;Region&#39;, &#39;year&#39;, &#39;species&#39;)) ltabundr &lt;- rbind(ltabundr, ltabundi) } # Format ltabundr data ltabundr &lt;- ltabundr %&gt;% mutate(year = as.numeric(year), D = D*1000, N = round(N)) %&gt;% select(title, species, year, g0=g0_est, g0_cv, ESW = ESW_mean, ss = size_mean, D, N, CV, L95, U95) # Join ALB to LTabundR mr &lt;- left_join(ltabundr, alb, by=c(&#39;title&#39;, &#39;year&#39;)) return(mr) } Read in, format, and join the results: # Manually-specified g0 ltas &lt;- lta_enlist(&#39;whiceas/lta_manual/&#39;) mr1 &lt;- whiceas_join(alb, ltas) %&gt;% mutate(scenario = &#39;ALB et al. (2022)&#39;) # Barlow (2015) Rg0 ltas &lt;- lta_enlist(&#39;whiceas/lta_barlow/&#39;) mr2 &lt;- whiceas_join(alb, ltas) %&gt;% mutate(scenario = &#39;Barlow (2015)&#39;) # New auto-generated Rg(0) ltas &lt;- lta_enlist(&#39;whiceas/lta/&#39;) mr3 &lt;- whiceas_join(alb, ltas) %&gt;% mutate(scenario = &#39;LTabundR&#39;) # Join them together mr &lt;- rbind(mr1, mr2, mr3) %&gt;% mutate(scenario = factor(scenario, levels = c(&#39;ALB et al. (2022)&#39;, &#39;Barlow (2015)&#39;, &#39;LTabundR&#39;))) Now we can compare LTabundR estimates to those from ALB et al. (2021): library(ggplot2) library(plotly) # Density ggplot(mr, aes(x=alb_D, y=D, color=title, shape=factor(year))) + geom_point() + scale_x_continuous(trans=&#39;log&#39;, limits=c(0.1, 50), breaks = c(0.01, 0.1, 0.25, 0.5, 1.0, 2.5, 5, 10, 25, 50), labels = function(x)round(x, 2)) + scale_y_continuous(trans=&#39;log&#39;, limits=c(0.1, 50), breaks = c(0.01, 0.1, 0.25, 0.5, 1.0, 2.5, 5, 10, 25, 50), labels = function(x)round(x, 2)) + geom_abline(slope = 1, intercept = 0, lty=3, alpha=.6) + ylab(&#39;LTabundR&#39;) + xlab(&#39;ALB et al. (2022)&#39;) + facet_wrap(~scenario, nrow=3) + labs(title=&#39;Density (cetaceans per 1,000 km2)&#39;, shape=&#39;Year&#39;, color = &#39;Species&#39;) + theme_light()   # Abundance ggplot(mr, aes(x=alb_N, y=N, color=title, shape=factor(year))) + geom_point() + scale_x_continuous(trans=&#39;log&#39;, limits=c(10, 30000), breaks = c(10, 25, 100, 250, 1000, 2500, 10000, 30000), labels = function(x)round(x)) + scale_y_continuous(trans=&#39;log&#39;, limits=c(10, 30000), breaks = c(10, 25, 100, 250, 1000, 2500, 10000, 30000), labels = function(x)round(x)) + geom_abline(slope = 1, intercept = 0, lty=3, alpha=.6) + ylab(&#39;LTabundR&#39;) + xlab(&#39;ALB et al. (2022)&#39;) + facet_wrap(~scenario, nrow=3) + labs(title=&#39;WHICEAS abundance&#39;, shape=&#39;Year&#39;, color = &#39;Species&#39;) + theme_light()   # g(0) estimate ggplot(mr, aes(x=alb_g0, y=g0, color=title, shape=factor(year))) + geom_point() + geom_abline(slope = 1, intercept = 0, lty=3, alpha=.6) + ylab(&#39;LTabundR&#39;) + xlab(&#39;ALB et al. (2022)&#39;) + facet_wrap(~scenario, nrow=3) + labs(title=&#39;Estimates of g(0)&#39;, shape=&#39;Year&#39;, color = &#39;Species&#39;) + theme_light()   # g(0) CV estimate ggplot(mr, aes(x=alb_g0_cv, y=g0_cv, color=title, shape=factor(year))) + geom_point() + geom_abline(slope = 1, intercept = 0, lty=3, alpha=.6) + ylab(&#39;LTabundR&#39;) + xlab(&#39;ALB et al. (2022)&#39;) + facet_wrap(~scenario, nrow=3) + labs(title=&#39;Estimates of CV of g(0)&#39;, shape=&#39;Year&#39;, color = &#39;Species&#39;) + theme_light() "],["stratagallery.html", " 15 Strata gallery Central North Pacific California Current ETP", " 15 Strata gallery This packages comes with several built-in datasets of geographic strata that are commonly used in NOAA/NMFS surveys. The functions strata_explore() and strata_select() were developed to help you explore those built-in options. Central North Pacific strata_explore(&#39;cnp&#39;) To acquire the filepath to one of these strata, pass the index (or indices) printed in the map titles above to the function strata_select(): strata &lt;- strata_select(selections = c(2,3), region = &#39;cnp&#39;) This function returns a named list that can be passed directly to the strata argument in load_settings(). strata %&gt;% names [1] &quot;OtherCNP&quot; &quot;MHI&quot; strata $OtherCNP Lon Lat 1 -131 40.00 2 -126 32.05 3 -120 25.00 4 -120 -5.00 5 -185 -5.00 6 -185 40.00 7 -131 40.00 $MHI Lon Lat 1 -156.00 22.00 2 -154.40 20.60 3 -153.50 19.20 4 -154.35 18.55 5 -155.20 17.75 6 -157.00 18.25 7 -157.50 19.20 8 -161.00 21.84 9 -160.00 23.00 10 -157.00 22.50 11 -156.00 22.00 California Current strata_explore(&#39;ccs&#39;) ETP You can do the same for the Eastern Tropical Pacific (ETP); there are about 70 polygons built-in to LTabundR for this region. We will just show a few of them here, using the start_at argument. strata_explore(&#39;etp&#39;, start_at = 64) "],["segmentizing.html", " 16 Segmentizing Approach Defaults Day vs Equal Length Contiguous vs. non-contiguous effort Segment remainder handling Typical settings", " 16 Segmentizing The package’s segmentize() function and its associated settings were designed to give researchers full control over how data are segmented, be it for design-based density analysis (which tend to use long segments of 100 km or more and allow for non-contiguous effort to be included in the same segment) or for habitat modeling (which tend to use short segments of 5 - 10 km and disallow non-contiguous effort to be pooled into the same segment). Approach Segments are built and stored separately for each cohort of species, since each cohort has specific settings for segmentizing. Within each cohort, the survey is first grouped into blocs of data that all share the same “effort scenario”, i.e., all rows share the same Cruise number, study area status (in or out), geographic stratum, and year. Since a survey may leave a stratum then return to it many days hence, it is normal for these blocs to contain non-contiguous data with large spatial gaps. These gaps will be addressed a few steps down. The blocs are split a final time according to whether the effort scenario meets inclusion criteria for the analysis. These inclusion criteria are controlled by the cohort-specific settings such as distance_types, distance_modes, and distance_on_off. Rows of data that meet the inclusion criteria are relegated to their own data bloc, and given a new column, use, with the value TRUE. Data that do not meet the criteria are relegated to their own bloc as well (column use is FALSE). This means that, at the end of this process, we will have segments that will be used in the density/detection function analysis, and segments that will not. (The excluded segments are not deleted or transformed in any other way; they can still be used in summarizing detections, etc.) Next, the segmentize() function loops through each of these blocs of effort and parses its data into segments according to the segment_method. If segmentizing by \"day\", this is straightforward: all data occurring on a unique date are assigned to its own segment. Segmentizing by \"equallength\" is a bit more complicated in terms of coding: segments are built up one line of data at a time; if the segment_target_km is reached or the segment_max_interval is exceeded, a new segment begins. At the end of this process, you have lists of data sorted into their segments, each with a unique seg_id, as well as a summary dataframe that provides the distance (km); time and coordinates for the beginning, middle, and end of the segment; and the weighted averages of sighting conditions and weather data contained in the segment. Setting up this demo The demonstration of segmentize() on in Processing chapter relies on the settings list that is attached as a slot in the cruz object. But you can override those settings with direct function inputs in segmentize(), which gives us a chance to explore segmentization options. First we load the demo data and carry out initial processing: # Load built-in settings example data(example_settings) settings &lt;- example_settings # Set path to DAS file das_file &lt;- &#39;data/surveys/HICEASwinter2020.das&#39; # First steps of formatting das &lt;- das_load(das_file, perform_checks = FALSE, print_glimpse = FALSE) cruz &lt;- process_strata(das, settings, verbose=FALSE) cruz &lt;- das_format(cruz) And this is the histogram function we will be using to display the results of each run of segmentize(): segment_histogram &lt;- function(cruz, cohort=1, by_day=FALSE){ (settings &lt;- cruz$settings) (segs &lt;- cruz$cohorts[[cohort]]$segments) (segmax &lt;- max(segs$dist,na.rm=TRUE)*1.1) if(by_day){ main_use &lt;- paste0(&#39;Segments (use) | by day&#39;) main_exclude &lt;- paste0(&#39;Segments (exclude) | by day&#39;) }else{ (main_use &lt;- paste0(&#39;Segments (use) | target: &#39;,settings$survey$segment_target_km,&#39; km&#39;)) (main_exclude &lt;- paste0(&#39;Segments (exclude) | target: &#39;,settings$survey$segment_target_km,&#39; km&#39;)) } par(mfrow=c(1,2)) par(mar=c(4.2,4.2,2.5,.5)) hist(segs$dist[segs$use], breaks = seq(0,segmax,by=segmax/40), xlab=&#39;Segment lengths (km)&#39;, main=main_use, cex.main = .8, cex.axis = .8, cex.lab = .8) hist(segs$dist[!segs$use], breaks = seq(0,segmax,by=segmax/40), xlab=&#39;Segment lengths (km)&#39;, main=main_exclude, cex.main = .8, cex.axis = .8, cex.lab = .8) par(mfrow=c(1,1)) } Defaults Here is the segmentize() function parameterized with the “factory default” settings from load_settings(). cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 30, segment_max_interval = 48, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), distance_types = c(&#39;S&#39;,&#39;F&#39;,&#39;N&#39;), distance_modes = c(&#39;P&#39;,&#39;C&#39;), distance_on_off = c(TRUE), verbose=FALSE) # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow [1] 273 # Plot segment_histogram(cruz_demo) Day vs Equal Length By day cruz_demo &lt;- segmentize(cruz, segment_method = &#39;day&#39;, segment_target_km = 30, segment_max_interval = 48, verbose=FALSE) # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow [1] 113 # Plot segment_histogram(cruz_demo, by_day = TRUE) By target length of 150 km cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 150, segment_max_interval = 48, verbose=FALSE) # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow [1] 68 # Plot segment_histogram(cruz_demo, by_day = TRUE) Contiguous vs. non-contiguous effort The example above allows for non-contiguous effort; a segment is allowed to contain effort separated by gaps as large as 24 hours (settings$max_interval). To coerce segments to represent only contiguous effort, make that setting very small: cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 30, segment_max_interval = .1, verbose=FALSE) # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow [1] 596 # Plot segment_histogram(cruz_demo, by_day = TRUE) You can see that many contiguous periods of effort were much shorter than the target length of 30 km. This is why allowing for non-contiguous effort can be advantageous for target segment lengths larger than 5 - 10 km. Segment remainder handling The default setting for segment_remainder_handling, c('append','segment'), means that remainders less than half the target length will be randomly appended to another segment, while remainders more than half will be treated as their own segment (and will be placed randomly along the trackline). If you don’t want that level of complexity, you can simply assign a single setting: 'append' will append the remainder in all cases, regardless of remainder length relative to the target length. The same idea goes for 'segment'. The other possible setting is 'disperse', which disperses the remainder evenly across all segments. To demonstrate, let’s use a target length of 100 km. cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 100, segment_max_interval = 48, verbose=FALSE) # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow [1] 94 # Plot segment_histogram(cruz_demo, by_day = TRUE) Note that most segments are longer than the target length, due to the impact of dispersing the remainder. If you wanted, you could combat this by making the target length slightly smaller: cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 90, segment_max_interval = 48, verbose=FALSE) # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow [1] 102 # Plot segment_histogram(cruz_demo, by_day = TRUE) But in general, the disperse option may be more appropriate for shorter segment lengths. Typical settings Design-based line transect analysis To replicate methods used in density estimation analyses, use large segment lengths (100 km or more) or simply segmentize by day. (See the examples above.) Remember that long segment lengths won’t work well unless you allow for non-contiguous effort. Habitat modeling To replicate the methods used in typical habitat modeling studies, use smaller segment lengths of contiguous effort. cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 5, segment_max_interval = .1, verbose=FALSE) # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow [1] 1819 # Plot segment_histogram(cruz_demo, by_day = TRUE) "],["abund9_compare.html", " 17 LTabundR vs. ABUND9 Differences in total effort Differences in on-effort distance Differences in total sightings Differences in on-effort sightings Differences in school size estimation", " 17 LTabundR vs. ABUND9 We have tried to develop LTabundR with the flexibility either to replicate ABUND results or to produce customizable results that could potentially vary from ABUND quite significantly (e.g., formatted for habitat modeling). However, even when we use LTabundR settings intended to replicate ABUND results, there are likely to be some small differences. These are detailed below: Differences in total effort After loading the data, LTabundR removes rows with invalid Cruise numbers, invalid times, and invalid coordinates. As far as we can tell, ABUND does not remove such missing data. This is a relatively minor point; in processing the 1986-2020 data (623,640 rows), 287 rows are missing Cruise info; 1,430 are missing valid times; and 556 are missing valid coordinates, for a total of 2,273 rows removed out of more than 625,000 (0.3% of rows). Many of these rows with missing data have the same coordinates as complete rows nearby (since WinCruz can sometimes produce multiple lines at the same time when setting up metadata for the research day). In ABUND, custom functions are used to calculate whether DAS coordinates occur within geostrata are difficult to validate, and it is possible that they differ from the functions used in R for the same purpose. LTabundR uses functions within the well-established sf package to do these same calculations. Both ABUND and LTabundR calculate the distance surveyed based on the sum of distances between adjacent rows in the DAS file. They do this differently (see below), based on the way they loop through the data, which may yield minor differences in segment track lengths. ABUND loops through the data one row at a time, calculating distance traveled at the same time as allocating effort to segments and processing sightings. It calculates the distance between each new row and the beginning of a segment of effort. That beginning location (object BEGTIME in the Fortran code) is reset with various triggers (including a new date), and the distance traveled is calculated using a subroutine (DISTRAV). For surveys occurring after 1991, the distance between a new coordinate and the BEGTIME coordinate is calculated using a subroutine named GRCIRC (great-circle distance). Prior to 1991, the ship speed and the time since BEGTIME is used to estimate distance traveled. After 1991, the function calculates distance based on coordinates. For all years, the distance calculation only happens if the time gap in time is at least 1.2 minutes (line 405 in ABUND9.FOR), otherwise the distance is returned as 0 km. This function also seems to allow for large gaps between subsequent rows within a single day of effort. The subroutine prints a warning message when the gap is greater than 30 km, but does not modify its estimate of distance traveled. This allows for the possibility that, in rare cases, estimates of distance surveyed will be spuriously large. LTabundR processes data using a modular approach rather than a single large loop. Prior to the segmentizing stage, it calculates the distance between rows of data. Its approach is to calculate the distance between each row and its subsequent row (it does so using the swfscDAS function distance_greatcircle(), which is a nearly-exact recode of the ABUND subroutine GRCIRC for R. There are two important differences that LTabundR applies: (1) In anticipation of WinCruz surveys that operate on much smaller scales with more frequent position updates, we calculate distances for time gaps as small as 30 seconds, not 1.2 minutes. This may generate minor differences in the total length of tracks; (2) If the distance between rows is greater than 30 km, then it is assumed that effort has stopped and the distance is changed to 0 km (that distance can be modified by the user; see the LTabundR function load_survey_settings(). This approach should avoid the misinterpretation of large gaps in effort as large periods of effort. Differences in on-effort distance LTabundR works with DAS data that are loaded and formatted using swfscDAS:das_read() and das_process(). It is possible that these functions categorize events as On- or Off-Effort slightly differently than ABUND, or apply other differences that would be difficult for us to know or track. While ABUND uses a minimum length threshold to create segments, such that full-length segments are never less than that threshold and small remainder segments always occur at the end of a continuous period of effort, LTabundR uses an approach more similar to the effort-chopping functions in swfscDAS: it looks at continuous blocs of effort, determines how many full-length segments can be defined in each bloc, then randomly places the remainder within that bloc according to a set of user-defined settings (see load_survey_settings(). This process produces full-length segments whose distribution of exact lengths is centered about the target length, rather than always being greater than the target length. To control the particularities of segmentizing, LTabundR uses settings such as segment_max_interval, which controls how discontinuous effort is allowed to be pooled into the same segment. These rules may produce slight differences in segment lengths. Note that, since ABUND is a loop-based routine while LTabundR is modular, segments identified by the two program will never be exactly identical, and a 1:1 comparison of segments produced by the two programs is not possible. Differences in total sightings In ABUND9, only sightings that occur while OnEffort == TRUE are returned; in contrast, LTabundR does not remove any sightings (it just flags them differently, using the included column variable). But we can easily filter LTabundR sightings to emulate ABUND9 output. Differences in on-effort sightings LTabundR includes an additional criterion for inclusion in analysis: the sighting must occur at or forward of the beam (this can be deactivated in load_survey_settings(). Since geostratum handling is different in the two programs, it is possible that sightings occurring near stratum margins may be included/excluded differently. Differences in school size estimation If an observer is not included in the Group Size Calibration Coefficients .DAT file, ABUND applies a default coefficient (0.8625) to scale group size estimates; however, it applies this calibration to group sizes of all sizes, including solo animals or small groups of 2-3. In LTabundR, users can choose to restrict calibrations for unknown observers to group size estimates of any size (see load_cohort_settings()) Note that ABUND9 calibrates school sizes slightly differently than ABUND7. The ABUND9 release notes mention a bug in previous versions that incorrectly calibrated school size. LTabundR corresponds perfectly with ABUND9 school size calibrations, but not with ABUND8 or earlier. "],["spp_codes.html", " 18 NOAA/NMFS species codes Table species_translator()", " 18 NOAA/NMFS species codes Table LTabundR provides the standard table of NOAA/NMFS species codes as a built-in dataset: This version of the species codes table was provided by Amanda Bradford (Pacific Islands Fisheries Science Center) in 2021. For an easier way to find a species of interest, look into the species_translator() function on the Miscellaneous functions page. species_translator() To streamline the management of species codes, scientific names, common names, etc., in the functions throughout this package, we have developed a “translator” function that returns the various identifiers for a species according to a variety of search terms. You can search by species code: # source(&#39;R/species_translator.R`) species_translator(id = &#39;035&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common_name1 &quot;Long-finned pilot whale&quot; common_name2 &quot;Atlantic pilot whale&quot; common_name3 &quot;blackfish&quot; common_name4 &quot;pothead&quot; By the short code name: species_translator(id = &#39;LONG_PILOT&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common_name1 &quot;Long-finned pilot whale&quot; common_name2 &quot;Atlantic pilot whale&quot; common_name3 &quot;blackfish&quot; common_name4 &quot;pothead&quot; By the scientific name: species_translator(id = &#39;Globicephala melas&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common_name1 &quot;Long-finned pilot whale&quot; common_name2 &quot;Atlantic pilot whale&quot; common_name3 &quot;blackfish&quot; common_name4 &quot;pothead&quot; Or by one of the species’ common names: species_translator(id = &#39;Long-finned pilot whale&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common_name1 &quot;Long-finned pilot whale&quot; common_name2 &quot;Atlantic pilot whale&quot; common_name3 &quot;blackfish&quot; common_name4 &quot;pothead&quot; The function will return any species for which there is a partial match: species_translator(id = &#39;megap&#39;) %&gt;% t() 76 code &quot;076&quot; short_name &quot;HUMPBACK_W&quot; scientific_name &quot;Megaptera novaeangliae&quot; common_name1 &quot;Humpback whale&quot; common_name2 &quot;&quot; common_name3 &quot;&quot; common_name4 &quot;&quot; 70 code &quot;070&quot; short_name &quot;UNID_RORQL&quot; scientific_name &quot;Balaenopterid sp&quot; common_name1 &quot;Unidentified rorqual (Balaenoptera; Megaptera)&quot; common_name2 &quot;&quot; common_name3 &quot;&quot; common_name4 &quot;&quot; species_translator(id = &#39;killer&#39;) code short_name scientific_name common_name1 32 032 PYGMY_KLLR Feresa attenuata Pygmy killer whale 33 033 FALSE_KLLR Pseudorca crassidens False killer whale 37 037 KILLER_WHA Orcinus orca Killer whale 110 110 Orcinus orca Transient killer whale 111 111 Orcinus orca Resident killer whale 112 112 Orcinus orca Offshore killer whale 113 113 Orcinus orca Type A Antarctic killer whale 114 114 Orcinus orca Type B Antarctic killer whale 115 115 Orcinus orca Type C Antarctic killer whale common_name2 common_name3 common_name4 32 slender blackfish 33 37 110 111 112 113 114 115 Note that if species_codes is NULL, as in the examples above, the list of codes used in ABUND9 will be used as a default. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
