[["index.html", "User’s guide for LTabundR Overview", " User’s guide for LTabundR Overview The R package LTabundR offers tools that facilitate and standardize the typical workflow for estimating cetacean abundance from NOAA-NMFS ship surveys in the eastern Pacific. That workflow typically involves four stages: (1) Data processing This step involves reading in &amp; processing raw DAS files, breaking effort into discrete segments for variance estimation, correcting school size estimates according to calibration models, then averaging together school size estimates for each sighting. Most importantly, this step standardizes data structure in a way that all downstream analyses depend upon. The name we will use for this standardized data object is a cruz object. The key LTabundR functions you will use in this stage are load_settings() and process_surveys(). (2) Data exploration This step involves summarizing effort and sightings totals, determining the appropriate truncation distances for each species based on their sample size, and producing maps. The key functions in this stage are cruz_explorer() (a Shiny dashboard for data exploration) and the summarize... functions, such as summarize_species() and summarize_effort(). (3) Data analysis This step involves estimating Beaufort-specific “Relative” trackline detection probabilities – i.e., g(0) estimates; estimating density/abundance with detection functions and determining the CV of that estimate; handling stratified analyses; and testing for the significance of interannual trends. The key LTabundR functions in this stage are g0_table() and lta(). Most analyses are group-based analyses, but false killer whales (Pseudorca crassidens) are analyzed differently using a subgroup-based approach. For this exception the function lta_subgroup() will be used. (4) Reports &amp; plots This step involes produces standardly-formatted results tables of your line-transect estimates; plots the best-fitting detection function model; and plots abundance estimates (and their CV). They key LTabundR functions in this stage are lta_report(), df_plot(), and lta_plot(). This user’s guide is structured around these four workflow stages. Those pages are followed by a set of case studies with full-fledged example code. The guide concludes with appendices that offer further details and minutiae on certain aspects of the package. Throughout this user’s guide, we will primarily be using example data from recent 2017 &amp; 2020 wintertime surveys in the region of the main Hawaiian Islands. That study area has been referred to “WHICEAS”. Installation Once the package is public, install details will go here. Credits This R package is a consolidation of code developed over many years by many NOAA-NMFS scientists, primarily the four J’s: Jay Barlow, Jim Gerrodette, Jeff Laake, and Jeff Moore. This package was developed by Eric Keen and Amanda Bradford. "],["install.html", " 1 Install LTabundR Option 1: Install from GitHub Option 2: Install locally", " 1 Install LTabundR Option 1: Install from GitHub Since the package is currently private, this is more complicated than it will be once the package is released. The first step is creating an R environment variable containing your GitHub personal access token. Complete these instructions only once: # Open your .Renviron file library(usethis) usethis::edit_r_environ() # Add your GitHub personal access token as a variable in `.Renviron` GITHUB_TOKEN = &#39;your_token_goes_here_as_a_character_string&#39; # Save and close your .Renviron file # Reload .Renviron readRenviron(&#39;~/.Renviron&#39;) Then run this code every time you want to reinstall / update the package: library(devtools) github_token &lt;- Sys.getenv(&quot;GITHUB_TOKEN&quot;) devtools::install_github(repo = &#39;amandalbradford/LTabundR-dev&#39;, subdir=&#39;LTabundR&#39;, auth_token=github_token, force=TRUE) library(LTabundR) Option 2: Install locally For now you can also install or update the package locally, using this code: library(devtools) # Remove the package, if you have an earlier version of it on your machine if(&#39;LTabundR&#39; %in% installed.packages()){remove.packages(&#39;LTabundR&#39;)} # Specify the path to the package&#39;s project folder. path_to_package &lt;- &#39;../LTabundR-dev/LTabundR&#39; # Update function documentation document(path_to_package) # Install package and any dependencies you need install(path_to_package) library(LTabundR) "],["settings.html", " 2 Settings Survey strata Survey-wide settings Cohort-specific settings Example code", " 2 Settings To customize the way data are processed and included in your analysis, use the load_settings() function. This function emulates and expands upon the settings file, ABUND.INP, that was used to run ABUND7/9 in FORTRAN. This function allows you to use ‘factory defaults’ if you don’t wish to specify anything special such as strata or study area polygons: settings &lt;- load_settings() If you do not want to use all the defaults, you can provide load_settings() with custom inputs. The function accepts three arguments: strata: dataframe(s) of coordinates survey: settings that will apply universally to the analysis cohorts: settings that are specific to groups of species. By providing cohort-specific settings, the code for a single analysis becomes simpler and more easily reproduced, since the code only needs to be run once without modification. The output of load_settings() is a named list with a slot for each of these arguments: settings %&gt;% names [1] &quot;strata&quot; &quot;survey&quot; &quot;cohorts&quot; Survey strata Stratum polygons can be provided as a named list of data.frame objects. Each data.frame must have Lat and Lon as the first two columns, providing coordinates in decimal degrees in which South and West coordinates are negative. Other columns are allowed, but the first two need to be Lon and Lat. The name of the slot holding the data.frame will be used as a reference name for the stratum. If strata is NULL, abundance will not be estimated; only density within the searched area (i.e., the total segment length x effective strip width). While users are welcome to upload polygons of their own, the package comes with built-in polygons for strata that are commonly used in the main NMFS study regions: the Central North Pacific (CNP, including Hawaii) … data(strata_cnp) names(strata_cnp) [1] &quot;HI_EEZ&quot; &quot;OtherCNP&quot; &quot;MHI&quot; &quot;WHICEAS&quot; [5] &quot;Spotted_OU&quot; &quot;Spotted_FI&quot; &quot;Spotted_BI&quot; &quot;Bottlenose_KaNi&quot; [9] &quot;Bottlenose_OUFI&quot; &quot;Bottlenose_BI&quot; &quot;NWHI&quot; …the California Current System (CCS) … data(strata_ccs) names(strata_ccs) [1] &quot;CCS&quot; &quot;Southern_CA&quot; &quot;Central_CA&quot; &quot;Nothern_CA&quot; &quot;OR_WA&quot; … and the Eastern Tropical Pacific (ETP): data(strata_etp) names(strata_etp) [1] &quot;MOPS_AreaCoreM&quot; &quot;MOPS_AreaIn&quot; &quot;MOPS_AreaIn1&quot; [4] &quot;MOPS_AreaIn2&quot; &quot;MOPS_AREAINS&quot; &quot;MOPS_AREAMID&quot; [7] &quot;MOPS_AreaMid1&quot; &quot;MOPS_AreaMid2&quot; &quot;MOPS_AreaMOPS&quot; [10] &quot;MOPS_AREANORS&quot; &quot;MOPS_AreaOuterM&quot; &quot;MOPS_AreaSou&quot; [13] &quot;MOPS_AREASOUS&quot; &quot;MOPS_AreaSpin&quot; &quot;MOPS_AreaSpinS&quot; [16] &quot;MOPS_AREAWES&quot; &quot;PODS_93STRAT1&quot; &quot;PODS_93STRAT2&quot; [19] &quot;PODS_Area92&quot; &quot;PODS_AREA92RS&quot; &quot;PODS_Area92s&quot; [22] &quot;PODS_AREA93&quot; &quot;PODS_AREA93A&quot; &quot;PODS_AREA93AR&quot; [25] &quot;PODS_AREA93AS&quot; &quot;PODS_AREA93BR&quot; &quot;PODS_AREA93M&quot; [28] &quot;PODS_AREA93MS&quot; &quot;PODS_AREA93R&quot; &quot;PODS_AREA93R1&quot; [31] &quot;PODS_AREA93R2&quot; &quot;PODS_AREA93RS&quot; &quot;PODS_AREA93S&quot; [34] &quot;PODS_AREANCOR&quot; &quot;PODS_GOCpoly&quot; &quot;Pre1986_Area79ES1&quot; [37] &quot;Pre1986_Area79ES1s&quot; &quot;Pre1986_Area79ES2&quot; &quot;Pre1986_Area79ES2s&quot; [40] &quot;Pre1986_Area79NE1&quot; &quot;Pre1986_Area79NE1s&quot; &quot;Pre1986_Area79NE2&quot; [43] &quot;Pre1986_Area79NE2s&quot; &quot;Pre1986_Area79NE3&quot; &quot;Pre1986_Area79NE3s&quot; [46] &quot;Pre1986_AreaCal&quot; &quot;Pre1986_AreaCals&quot; &quot;Pre1986_AreaMid&quot; [49] &quot;Pre1986_AreaMidS&quot; &quot;Pre1986_AreaNorth&quot; &quot;Pre1986_AreaNorthS&quot; [52] &quot;Pre1986_AreaSouth&quot; &quot;Pre1986_AreaSouthS&quot; &quot;STAR_Area98a&quot; [55] &quot;STAR_Area98b&quot; &quot;STAR_AreaCore&quot; &quot;STAR_AreaCore2&quot; [58] &quot;STAR_AreaCoreS&quot; &quot;STAR_AreaNCoast&quot; &quot;STAR_AreaNCstS&quot; [61] &quot;STAR_AreaOuter&quot; &quot;STAR_AreaOuter00&quot; &quot;STAR_AreaSCoast&quot; [64] &quot;STAR_AreaSCstS&quot; &quot;STAR_AreaSPn&quot; &quot;STAR_AreaSPs&quot; [67] &quot;STAR_AreaSTAR&quot; &quot;STAR_AreaSTAR2&quot; &quot;STAR_AreaSTARlite&quot; [70] &quot;STAR_Dcaparea&quot; The package includes functions for visualizing and selecting from these strata. See the Strata Gallery appendix. Survey-wide settings Survey-wide settings apply universally to all species in the analysis. Defaults settings$survey $out_handling [1] &quot;remove&quot; $max_row_interval [1] 900 $segment_method [1] &quot;day&quot; $segment_target_km [1] 150 $segment_max_interval [1] 48 $segment_remainder_handling [1] &quot;segment&quot; $ship_list NULL $species_codes NULL $group_size_coefficients NULL $smear_angles [1] FALSE Defaults for the survey argument list are built up efficiently using the function load_survey_settings() (see example code at bottom). Details The survey_settings input accepts a list with any of the following named slots. out_handling: the first slot allows you to specify how data occurring outside of geo-strata should be handled. If this is set to \"remove\", those rows will be filtered out of the data early in the process. This reduces memory usage, speeds up processing, and gives you geographic control of how effort and sightings will be summarize. If this is set to \"stratum\", those data will be assigned to a fake geo-stratum, named \"out\". Effort in the \"out\" stratum will not be segmentized, but \"out\" sightings will be processed and retained in the final datasets. This setting might be useful if you want to use \"out\" data for survey summaries and/or detection function estimation. The default is \"remove\", since that saves the most time and memory. segment_method: This and the next few slots are devoted to controlling how effort will be “segmentized”, or chopped into discrete sections for the purposes of estimating the variabce of the abundance estimate. The two method options are \"day\" – all effort within the same Cruise-StudyArea-Stratum-Year-Effort scenario will be binned into segments by calendar date – and \"equallength\" – effort within each unique effort scenario (Cruise-StudyArea-etc.) will be divided into segments of approximately equal length. See the Appendix on segmentizing for details. segment_target_km: if segmenting by \"equallength\", this field allows you to specify what that target length is, in km. segment_max_km_gap: the segmentizing function works by calculating the distance between each row of DAS data. If that distance exceeds the length specified here (e.g., 5 km), the function will assume that there was a break in effort. segment_max_interval: if segmentizing by \"equallength\", this setting allows you to specify the time gaps in effort that are allowed to be contained within a single segment. For example, if your goal is a few large segments of equal length (e.g., 150-km segments, for bootstrap estimation of density variance), you are probably willing for discrete periods of effort to be concatenated into a single segment, even if the gaps between effort are as large as 1 or 2 days, in which case you would set segment_max_interval to 24 or 48 (hours), respectively. However, if your goal is many smaller segments (e.g., 5-km segments, for habitat modeling), you want to ensure that effort is contiguous so that segment locations can be accurately related to environmental variables, in which case you would set segment_max_interval to be very small (e.g., .2 hours, or 12 minutes). Setting this interval to a small number, such as 0.2, also allows the segmentizing function overlook momentary breaks in effort, such as when an unofficial observer logs a sighting. segment_remainder_handling: if segmentizing by \"equallength\", periods of effectively-contiguous effort (as specified by segment_max_interval) are unlikely to be perfectly divisible by your segment_target_km; there is going to be a remainder. You can handle this remainder in three ways: (1) \"disperse\" allows the function to adjust segment_target_km so that there is in fact no remainder, effectively dispersing the remainder evenly across all segments within that period of contiguous effort; (2) \"append\" asks the function to append the remainder to a randomly selected segment, such that most segments are the target length with the exception of one longer one; or (3) \"segment\" asks the function to simply place the remainder in its own segment, placed randomly within the period of contiguous effort. This setting also has a second layer of versatility, because it can accept a one- or two-element character vector. If a two-element vector is provided (e.g., c(\"append\",\"segment\")), the first element will be used in the event that the remainder is less than or equal to half your segment_target_km; if the remainder is more than half that target length, the second element will be used. This feature allows for replication of the segmentizing methods in Becker et al. (2010). The remaining slots in survey_settings pertain to various datasets and settings used in data processing: ship_list: A data.frame containing a list of ship names. If not provided the default version, which was current as of the release of ABUND9 in 2020, will be used (data(ships)). Supplied data.frames must match the column naming structure of data(ships). species_codes: A data.frame containing species codes. If not provided the default version, which was current as of the release of ABUND9 in 2020, will be used (data(species_codes)). Supplied data.frames must match the column naming structure of data(species_codes). group_size_coefficients: A data.frame of calibration factors. Find details in the subsection on processing sightings and estimating school size. smear_angles: If TRUE (the default is FALSE), bearing angles to a group of animals will be “smeared” by adding a uniformly distributed random number between -5 and +5 degrees. This has not been used in any recent analyses because observers have not been rounding angles as much as they used to. It was suggested by Buckland as a method for dealing with rounding which is especially influential when rounding to zero places many sightings at zero perpendicular distance. Cohort-specific settings Cohort-specific settings apply only to a group of species. Since you can add as many cohorts to a settings object as you need, this allows you to stage your entire analysis and run your code once without modifying code or creating multiple versions of your code for each analysis of each cohort. Defaults The default is to use a single cohort for all species: settings$cohorts %&gt;% names [1] &quot;default&quot; Default values for the default cohort: settings$cohorts$default $id [1] &quot;default&quot; $species NULL $strata NULL $probable_species [1] FALSE $sighting_method [1] 0 $cue_range [1] 0 1 2 3 4 5 6 7 $school_size_range [1] 0 10000 $school_size_calibrate [1] TRUE $calibration_floor [1] 0 $use_low_if_na [1] FALSE $io_sightings [1] 0 $geometric_mean_group [1] TRUE $truncation_km [1] 5.5 $beaufort_range [1] 0 1 2 3 4 5 6 $abeam_sightings [1] FALSE $strata_overlap_handling [1] &quot;smallest&quot; &quot;largest&quot; &quot;each&quot; $distance_types [1] &quot;S&quot; &quot;F&quot; &quot;N&quot; $distance_modes [1] &quot;P&quot; &quot;C&quot; $distance_on_off [1] TRUE Defaults for the cohorts argument list are built up efficiently using the function load_cohort_settings() (see example code at bottom). Details The cohort_settings input accepts a list of any length. Each slot in that list can contain settings for a different cohort. Each cohort list can have any of the following named slots: id: An informal identifier for this cohort, to help you keep track of which cohort is which. For example, settings for a cohort of large whales species could be named \"big whales\"; settings for small delphinids and phocoenids could be named \"small_odontocetes\"; settings for beaked whales could be named \"beakers\". species: A character vector of species codes to include in this cohort. If NULL (the default), all species will be included. Note that any species to be used in modeling a detection function for this cohort must be included here. For example, in Hawaii the bottlenose dolphin is analyzed as part of a multi-species pool along with the rough-toothed dolphin, Risso’s dolphin, and pygmy killer whale. However, the bottlenose dolphin has insular populations that need to be analyzed distinctly from their pelagic counterpart, which requires some special geostratum handling that behooves the preparation of a dedicated cohort for bottlenose dolphin. Even so, in the cohort_settings object for the bottlenose dolphin cohort, the species codes for the rough-toothed, Risso’s, and pygmy-killer-whale dolphins need to be provided in this species argument. Conversely, in the cohort_settings object that holds most other species, including the rough-toothed, Risso’s, and pygmy-killer-whale dolphins, the bottlenose dolphin’s code still needs to be included in this species argument. strata: A character vector of geostratum names. These must match the names listed in the strata slot of your survey settings (see documentation for load_survey_settings()). If NULL (the default), all geostrata in your survey settings will be used. This argument is an opportunity to subset the geostrata used for a cohort. For example, as discussed above, certain dolphin species in Hawaiian waters have unique geostrata that apply only to their insular/pelagic populations, and should only have a role in breaking effort segments in the bootstrap variance analysis for these specific species. Those dolphins should be given their own cohort, and those insular/pelagic geostrata should be included in this strata argument. Conversely, all other species should be placed in a separate cohort and only the generic geostrata should be included in this strata argument. See the WHICEAS example below for a demonstration. probable_species: If TRUE (default is FALSE), the “probable” species identifications will be used in place of the “unidentified” categories. sighting_method: A coded integer which determines which sightings will be included based on how they were first seen. Allowable codes are 0=any method, 1=with 25X only, 2=neither with 25x binoculars nor from the helicopter (i.e., naked eyes and 7x binoculars only). These codes match those used in ABUND7/9. cue_range: Numeric vector of acceptable “observation cues” for sightings used in estimates of abundance. (0=this detail is missing in the data, 1=associated birds, 2=splashes, 3=body of the marine mammal, 4=associated vessel, 5=?, 6=blow / spout, 7=associated helicopter). These codes match those used in ABUND7/9. school_size_range: Minimum and maximum group sizes to be included in estimates of abundance. This is the overall group size, not the number of the given species that are present in a group. school_size_calibrate: A logical (TRUE or FALSE) specifying whether or not to carry out school size adjustments according to the calibration table provided in survey$group_size_coefficients (if that table is provided). This setting allows you to toggle the survey-wide setting for certain cohorts. For example, perhaps you want to carry out calibration for a cohort of dolphin species, but not for a cohort of large whales whose group sizes tend to be smaller and easier to estimate accurately. calibration_floor: A numeric indicating the minimum school size estimate for which school size calibration will be attempted. This pertains only to observers who do no have an entry in the group_size_coefficients table provided in load_survey_settings() (that table has a calibration floor for each observer). The default is 0, meaning that calibration will be attempted for all school size estimates, regarding of the raw estimate. use_low_if_na: If this setting is TRUE, an observer does not make a best estimate of group size, mean group size will be calculated from “low” estimates. This will be done only if no observer has a “best” estimate. io_sightings: A coded integer which specifies how sightings by the independent observer will be handled. Allowable codes, which are inherited from those used in ABUND7/9, are \"_1\"=include independent observer sightings wih all other sightings, \"0\"=ignore sightings by independent observer, \"1\"=use only sightings made by regular observer team WHEN an independent observer was present, \"2\"=include only sightings made by the independent observer. IO sightings are typically used only for making g(0) estimates, otherwise IO sightings are usually ignored (code = \"0\"). geometric_mean_group: This logical variable specifies whether to use a weighted geometric mean when calculating mean group size. Barlow, Gerrodette, and Perryman (1998) found that this gave slightly better performance than a straight mean group size. Default is TRUE, but it will only be done if group_size_coefficients is not NULL. truncation_km: Specifies the maximum perpendicular distance for groups that could potentially be included for abundance estimation. Also determines the bins used for grouped perpendicular distances. This is not the stage at which you set the truncation distance for detection function modeling; it is simply a preliminary cutoff for very distant sightings that whose distance is unlikely to be measured accurately from shipboard reticle readings. For that reason, the default is set to be quite far (5.5 km). beaufort_range: Vector of Beaufort sea states (integers) that are acceptable in estimating the detection function and density. Beaufort data with a decimal place will be rounded to the nearest integer to evaluate for inclusion. abeam_sightings: = If TRUE, sightings that occur aft of beam are included in estimating the detection function and densities. Default is FALSE: all abeam sightings will be ignored. strata_overlap_handling: In the event that survey strata overlap, this setting tells R how to handle it. The options are \"smallest\" (the default), in which effort can belong to only a single stratum, and the smallest of overlapping strata will be used (e.g., an insular polygon nested within a larger EEZ polygon); \"largest\", in which effort can belong to only a single stratum and the largest of overlapping strata will be used (we are not sure what use case this would serve, but we offer it as an option for niche analyses); and \"each\", in which each effort is allowed to belong to two or more strata at once and all analyses will be conducted for each overlapping polygon separately (e.g., this may be appropriate for nested strata in which you want to estimate density in the entirety of the larger stratum in addition to estimating density for the nested stratum). distance_types: A character vector of the effort types that will be included in detection function estimation and density estimation, and therefore considered in effort segmentizing. Accepted values are \"S\" (systematic/standard effort), \"F\" (fine-scale effort), and \"N\" (non-systematic/non-standard effort, in which systematic protocols are being used but effort is not occurring along design-based transect routes). The default values are c(\"S\",\"F\",\"N\"). distance_modes: The effort modes that will be included in detection function estimation and density estimation, and therefore considered in effort segmentizing. Accepted values are \"P\" (passing) and \"C\" (closing), and the default values are c(\"P\",\"C\"). distance_on_off: The value(s) of OnEffort (On Effort is TRUE, Off Effort is FALSE) that will be included in detection function estimation and density estimation, and therefore considered in effort segmentizing. Default is TRUE only. (We don’t expect FALSE or c(TRUE,FALSE) to be used much, if at all, but we make this option available). Example code Use settings defaults No strata or group size calibration. settings &lt;- load_settings() Use settings defaults, but with strata # Load strata dataframes data(strata_cnp) settings &lt;- load_settings(strata = strata_cnp) Customize survey, but not cohorts This code will process survey data such that effort segments are 5 km in length, with any effort falling outside of the provided geostrata relegated to a virtual geostratum named \"out\". Since a cohort is not specified, the default values will be used. # Load built-in datasets data(strata_cnp) data(group_size_coefficients) data(ships) data(species_codes) # Survey settings survey &lt;- load_survey_settings(out_handling = &#39;stratum&#39;, max_row_interval = 700, segment_method = &#39;equallength&#39;, segment_target_km = 5, segment_max_interval = .3, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), ship_list = ships, species_codes = species_codes, group_size_coefficients = group_size_coefficients, smear_angles = FALSE) # Load settings settings &lt;- load_settings(strata = strata_cnp, survey) Fully custom: WHICEAS case study These are the settings we will use in the remainder of the tutorial. Survey-wide settings To emulate the analysis done in Bradford et al. (2021), we want to process effort with 150-km segments, relegating any remainder to its own segments. We also want to make sure to remove any survey data that falls outside of the geostrata, to ensure that detection functions are regionally specific. We will use the built-in tables for species codes, ship codes, and group size calibration coefficients. data(species_codes) data(ships) data(group_size_coefficients) survey &lt;- load_survey_settings( out_handling = &#39;remove&#39;, max_row_interval = Inf, segment_method = &quot;equallength&quot;, segment_target_km = 150, segment_max_interval = 24, segment_remainder_handling = c(&quot;segment&quot;), ship_list = ships, species_codes = species_codes, group_size_coefficients = group_size_coefficients, smear_angles = FALSE ) Geostrata We will use the built-in dataset of Central North Pacific geostrata: data(strata_cnp) strata_cnp %&gt;% names [1] &quot;HI_EEZ&quot; &quot;OtherCNP&quot; &quot;MHI&quot; &quot;WHICEAS&quot; [5] &quot;Spotted_OU&quot; &quot;Spotted_FI&quot; &quot;Spotted_BI&quot; &quot;Bottlenose_KaNi&quot; [9] &quot;Bottlenose_OUFI&quot; &quot;Bottlenose_BI&quot; &quot;NWHI&quot; Species cohorts Cohort 1: All species This first cohort will serve as a ‘catch-all’ for species who do not need special handling. It does not hurt to include all species in this catch-all cohort – except perhaps by increasing the file-size of your processed data by a few kilobytes – even if you will be creating a separate, dedicated cohort for one of these species downstream. Having a catch-all cohort like this serves two purposes: (1) it avoids unforeseen complications if you will be modeling detection functions with multi-species pools, as mentioned above; and (2) it will simplify the code you will use to produce summary statistics of effort and sightings totals, since you will not need to pool together statistics from multiple cohorts. To build this catch-all cohort, we will not specify any species so that all species in the data are included, and we will specify that only the generic geostrata should be used, so that species specific insular stock boundaries are ignored. Here we show all possible inputs. Most of these match the built-in defaults. Those that do not (there are just 3) are noted with a commented asterisk. all_species &lt;- load_cohort_settings( id = &quot;all&quot;, # * species = NULL, strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;), # * probable_species = FALSE, sighting_method = 0, cue_range = 0:7, school_size_range = c(0, 10000), school_size_calibrate = TRUE, calibration_floor = 0, use_low_if_na = TRUE, io_sightings = 0, geometric_mean_group = TRUE, truncation_km = 7.5, # * beaufort_range = 0:6, abeam_sightings = TRUE, strata_overlap_handling = c(&quot;smallest&quot;), distance_types = c(&#39;S&#39;,&#39;F&#39;,&#39;N&#39;), distance_modes = c(&#39;P&#39;,&#39;C&#39;), distance_on_off = TRUE ) Cohort 2: Bottlenose dolphin As mentioned above, bottlenose dolphins are going to be analyzed as part of a multi-species pool that includes the rough-toothed dolphin, Risso’s dolphin, and pygmy killer whale. Because those species’ codes will be needed to model the detection function used in bottlenose dolphin density/abundance estimation, those species codes will be included in this cohort’s settings. Also mentioned above: Hawaii has a pelagic population of bottlenose dolphins as well as several distinct insular stocks. In this case study, we are interested in estimating only the abundance of the pelagic population, which means we will need to include geostrata of the insular stock boundaries in order to make sure the effort and sightings within those insular areas are ignored. That means we will specify the generic geostrata (\"WHICEAS\", \"HI_EEZ\", and \"Other_CNP\"), as well as the geostrata for the insular stocks. We can spell out as many any inputs that we wish, but here will only show the inputs that are non-default and/or different from Cohort 1 above. bottlenose &lt;- load_cohort_settings( id = &quot;bottlenose&quot;, species = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;, &#39;Bottlenose_BI&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_KaNi&#39;), truncation_km = 7.5) Cohort 3: Pantropical spotted dolphin Similar to the bottlenose dolphin above, spotted dolphins in Hawaiian waters belong to pelagic stocks as well as insular stocks. We will be estimating density/abundance for the only pelagic stocks here, but we need to include the geostrata for the insular stocks in order to ignore their effort and sightings. spotted &lt;- load_cohort_settings( id = &quot;spotted&quot;, species = &#39;002&#39;, strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;, &#39;Spotted_OU&#39;,&#39;Spotted_FI&#39;,&#39;Spotted_BI&#39;), truncation_km = 7.5) Compile settings Finally, we create our settings object: settings &lt;- load_settings(strata = strata_cnp, survey = survey, cohorts = list(all_species, bottlenose, spotted)) Save this settings object locally, to use in downstream scripts: save(settings, file=&#39;whiceas_settings.RData&#39;) "],["processing.html", " 3 Data processing Behind the scenes Review", " 3 Data processing In our WHICEAS case study example, we are interested in estimating density/abundance for 2017 and 2020 only, but we want to use surveys from previous years to help model species detection functions. We will therefore be using a dataset of NOAA surveys in the Central North Pacific from 1986 to 2020. das_file &lt;- &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39; You can process your survey data using a single function, process_surveys(), which takes two arguments: the filepath(s) to your DAS survey data, and your settings object. For example: cruz &lt;- process_surveys(das_file, settings) That single command will convert your raw DAS data to a “cruz” object, a list of polished datasets that are prepared to be passed to subsequent analyses. Behind the scenes The process_surveys() function is a wrapper for several discrete stages of data formatting/processing. Behind the scenes, each of those stages is carried out using a specific LTabundR function. The remainder of this page is a detailed step-by-step explanation of the data processing that occurs when you call process_surveys(). Bring in cruise data Read in and process your .DAS file using the functions in Sam Woodward’s swfscDAS package. To do so quickly, we built a wrapper function that makes this quick and easy: das &lt;- load_das(das_file, perform_checks = TRUE, print_glimpse = TRUE) Process strata Run the following function to add strata and study-area information to each row of DAS data: das_strata &lt;- process_strata(das, settings) This function loops through each stratum data.frame you have provided it in settings$strata, formats the stratum, and asks whether each DAS row occurs within it. For each stratum, a column named stratum_&lt;StratumName&gt; is added to the das object; each row in this column is TRUE (included) or FALSE. A similar procedure is run if a dataframe is provided in settings$study_area. A column named study_area is added to das containing a boolean (TRUE if the sub-segment or sighting occurs within the study area). Format DAS data into a cruz object The function format_das() takes care of some final formatting and initiates the cruz object data structure. cruz &lt;- format_das(das_strata, verbose=TRUE) This function (1) remove rows with invalid Cruise numbers, times, or locations; (ii) calculate the distance, in km, between each row of data; (iii) adds a ship column to the dataset, with initials for the ship corresponding to each cruise; (iv) creates a new list, cohorts, which copies the cruise data for each cohort specified in your settings; and (v) adds a stratum column to the data in each cohort. That column specifies a single stratum assignment for each row of DAS data in the event of overlapping strata, based upon the cohort setting stratum_overlap_handling. The cruz object The function format_das() returns a list, which we have saved in an object named cruz, with several slots: cruz %&gt;% names [1] &quot;settings&quot; &quot;strata&quot; &quot;cohorts&quot; The slots strata and study_area provide the area, in square km, of each polygon being used: cruz$strata stratum area 1 HI_EEZ 2474595.769 2 OtherCNP 34215265.219 3 MHI 212033.063 4 WHICEAS 402948.734 5 Spotted_OU 5102.666 6 Spotted_FI 10509.869 7 Spotted_BI 39454.720 8 Bottlenose_KaNi 2755.024 9 Bottlenose_OUFI 14417.035 10 Bottlenose_BI 4668.072 11 NWHI 449375.569 The slot cohorts is itself a list with one slot for each cohort. The slots are named using the id cohort setting. cruz$cohorts %&gt;% names [1] &quot;all&quot; &quot;bottlenose&quot; &quot;spotted&quot; Each cohort slot has a copy of the DAS data with a new stratum column, which contains a stratum assignment tailored to its cohort-specific settings. For instance, the all cohort, whose stratum_overlap_handling is set to \"smallest\", assigns the smallest stratum in the event of overlapping or nested strata: cruz$cohorts$all$stratum %&gt;% table(useNA=&#39;ifany&#39;) . HI_EEZ OtherCNP WHICEAS 117715 126225 85669 Since the bottlenose cohort uses a different subset of geostrata, its distribution of stratum assignments will also differ: cruz$cohorts$bottlenose$stratum %&gt;% table(useNA=&#39;ifany&#39;) . Bottlenose_BI Bottlenose_KaNi Bottlenose_OUFI HI_EEZ OtherCNP 3415 1495 6862 117715 126225 WHICEAS 73897 This list, with these five primary slots, will be referred to from hereon as a cruz object. Segmentize the data To allocate survey data into discrete ‘effort segments’, which are used in variance estimation in subsequent steps, run the function segmentize(). This process is controlled by both survey-wide and cohort-specific settings, which are now carried in a slot within the cruz object. The process is outlined in detail in the Appendix on Segmentizing. cruz &lt;- segmentize(cruz, verbose=FALSE) This function does not change the high-level structure of the cruz object … cruz %&gt;% names [1] &quot;settings&quot; &quot;strata&quot; &quot;cohorts&quot; … or the cohort names in the cohorts slot: cruz$cohorts %&gt;% names [1] &quot;all&quot; &quot;bottlenose&quot; &quot;spotted&quot; For each cohorts slot, the list structure is the same: cruz$cohorts$all %&gt;% names [1] &quot;segments&quot; &quot;das&quot; cruz$cohorts$bottlenose %&gt;% names [1] &quot;segments&quot; &quot;das&quot; cruz$cohorts$spotted %&gt;% names [1] &quot;segments&quot; &quot;das&quot; The segments slot contains summary data for each effort segment, including start/mid/end coordinates, average conditions, and segment distance: cruz$cohorts$all$segments %&gt;% glimpse Rows: 1,888 Columns: 37 $ Cruise &lt;dbl&gt; 901, 901, 901, 901, 901, 901, 901, 901, 901, 901, 901, 90… $ ship &lt;chr&gt; &quot;Mc2&quot;, &quot;Mc2&quot;, &quot;Mc2&quot;, &quot;Mc2&quot;, &quot;Mc2&quot;, &quot;Mc2&quot;, &quot;Mc2&quot;, &quot;Mc2&quot;, &quot;… $ stratum &lt;chr&gt; &quot;WHICEAS&quot;, &quot;WHICEAS&quot;, &quot;WHICEAS&quot;, &quot;WHICEAS&quot;, &quot;WHICEAS&quot;, &quot;W… $ seg_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… $ yday &lt;dbl&gt; 37, 37, 41, 44, 49, 51, 37, 37, 41, 43, 46, 47, 48, 37, 3… $ dist &lt;dbl&gt; 3.606733, 150.092607, 149.477608, 150.656675, 16.254889, … $ lat1 &lt;dbl&gt; 21.99817, 21.82833, 21.14817, 19.19833, 20.40850, 21.4130… $ lon1 &lt;dbl&gt; -159.1487, -158.6080, -158.0943, -156.0760, -156.1288, -1… $ DateTime1 &lt;dttm&gt; 2009-02-06 07:33:52, 2009-02-06 11:35:03, 2009-02-10 10:… $ timestamp1 &lt;dbl&gt; 1233905632, 1233920103, 1234262132, 1234548006, 123498100… $ lat2 &lt;dbl&gt; 21.82850, 21.14967, 19.19783, 20.40850, 21.41083, 21.6810… $ lon2 &lt;dbl&gt; -158.6105, -158.0918, -156.0758, -156.1288, -157.2680, -1… $ DateTime2 &lt;dttm&gt; 2009-02-06 11:33:21, 2009-02-10 10:33:32, 2009-02-13 17:… $ timestamp2 &lt;dbl&gt; 1233920001, 1234262012, 1234547957, 1234981000, 123514077… $ mlat &lt;dbl&gt; 21.88533, 19.57767, 21.33983, 19.80333, 21.40417, 21.9613… $ mlon &lt;dbl&gt; -158.7983, -156.0140, -160.2627, -155.0388, -157.2693, -1… $ mDateTime &lt;dttm&gt; 2009-02-06 10:14:24, 2009-02-08 08:39:43, 2009-02-11 11:… $ mtimestamp &lt;dbl&gt; 1233905632, 1233920103, 1234262132, 1234548006, 123498100… $ use &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRU… $ Mode &lt;chr&gt; &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C… $ EffType &lt;chr&gt; NA, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;… $ OnEffort &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE… $ ESWsides &lt;dbl&gt; NA, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,… $ year &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 200… $ month &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … $ day &lt;int&gt; 6, 6, 10, 13, 18, 20, 6, 6, 10, 12, 15, 16, 17, 6, 7, 7, … $ min_line &lt;int&gt; 424489, 424600, 425764, 426976, 428376, 428554, 424728, 4… $ max_line &lt;int&gt; 424599, 425763, 426975, 428375, 428553, 429845, 424824, 4… $ n_rows &lt;int&gt; 23, 586, 611, 611, 82, 476, 74, 31, 51, 211, 62, 22, 95, … $ avgBft &lt;dbl&gt; 4.000000, 3.886344, 2.693855, 4.236977, 5.650836, 5.42778… $ avgSwellHght &lt;dbl&gt; 4.000000, 3.614870, 2.990173, 4.295680, 6.000000, 5.91794… $ avgHorizSun &lt;dbl&gt; 1.544731, 5.621365, 5.251324, 7.670550, 7.650836, 4.41183… $ avgVertSun &lt;dbl&gt; 2.000000, 1.531826, 1.581961, 1.660281, 1.000000, 1.56607… $ avgGlare &lt;dbl&gt; 0.45526876, 0.00000000, 0.24616396, 0.11466734, 0.0000000… $ avgVis &lt;dbl&gt; 7.000000, 6.695591, 5.839215, 5.630527, 6.500000, 5.76905… $ avgCourse &lt;dbl&gt; 133.6261, 185.2734, 149.4506, 204.8376, 284.8100, 147.147… $ avgSpdKt &lt;dbl&gt; 8.701834, 8.264356, 9.226427, 9.395206, 9.681004, 9.17516… # Number of segments cruz$cohorts$all$segments %&gt;% nrow [1] 1888 # Segment length distribution x &lt;- cruz$cohorts$all$segments$dist hist(x, breaks = seq(0,ceiling(max(x, na.rm=TRUE)),by=1), xlab=&#39;Segment lengths (km)&#39;, main=paste0(&#39;Target km: &#39;,settings$survey$segment_target_km)) And the das slot holds the original data.frame of DAS data, modified slightly: the column OnEffort has been modified according to Beaufort range conditions, and the column seg_id indicates which segment the event occurs within cruz$cohorts$all$das %&gt;% names [1] &quot;Event&quot; &quot;DateTime&quot; &quot;Lat&quot; &quot;Lon&quot; [5] &quot;OnEffort&quot; &quot;Cruise&quot; &quot;Mode&quot; &quot;OffsetGMT&quot; [9] &quot;EffType&quot; &quot;ESWsides&quot; &quot;Course&quot; &quot;SpdKt&quot; [13] &quot;Bft&quot; &quot;SwellHght&quot; &quot;WindSpdKt&quot; &quot;RainFog&quot; [17] &quot;HorizSun&quot; &quot;VertSun&quot; &quot;Glare&quot; &quot;Vis&quot; [21] &quot;ObsL&quot; &quot;Rec&quot; &quot;ObsR&quot; &quot;ObsInd&quot; [25] &quot;Data1&quot; &quot;Data2&quot; &quot;Data3&quot; &quot;Data4&quot; [29] &quot;Data5&quot; &quot;Data6&quot; &quot;Data7&quot; &quot;Data8&quot; [33] &quot;Data9&quot; &quot;Data10&quot; &quot;Data11&quot; &quot;Data12&quot; [37] &quot;EffortDot&quot; &quot;EventNum&quot; &quot;file_das&quot; &quot;line_num&quot; [41] &quot;stratum_HI_EEZ&quot; &quot;stratum_OtherCNP&quot; &quot;stratum_WHICEAS&quot; &quot;year&quot; [45] &quot;month&quot; &quot;day&quot; &quot;yday&quot; &quot;km_int&quot; [49] &quot;km_cum&quot; &quot;ship&quot; &quot;stratum&quot; &quot;seg_id&quot; [53] &quot;use&quot; The segmentize() function and its associated settings were designed to give researchers full control over how data are segmented, be it for design-based density analysis (which tend to use long segments of 100 km or more and allow for non-contiguous effort to be included in the same segment) or for habitat modeling (which tend to use short segments of 5 - 10 km and disallow non-contiguous effort to be pooled into the same segment). To demonstrate that versatility, checkout the appendix on segmentizing. Process sightings To process sightings for each cohort of species, use the function process_sightings(). This function has three basic steps: for each cohort, the function (1) prepares a sightings table using the function das_sight() from swfscDAS; (2) filters those sightings to species codes specified for the cohort in your settings input; and (3) evaluates each of those sightings, asking if each should be included in the analysis according to your settings. cruz &lt;- process_sightings(cruz) The function produces a formatted dataset and adds it to a new sightings slot. cruz$cohorts$all %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; cruz$cohorts$bottlenose %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; cruz$cohorts$spotted %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; Note that the sightings table has a column named included (TRUE = yes, use it in the analysis). Any sightings that do not meet the inclusion criteria as specified in your settings will be included = FALSE, but they won’t be removed from the data. Since the sightings in each cohort are processed slightly differently according to the cohort’s specific settings – most importantly the species that will be included – you should expect different numbers of included/excluded sightings in each cohort dataset: cruz$cohorts$all$sightings$included %&gt;% table . FALSE TRUE 810 3122 cruz$cohorts$bottlenose$sightings$included %&gt;% table . FALSE TRUE 114 408 When this function’s verbose argument is TRUE (the default), a message is printed each time a sighting does not meet the inclusion criteria. Sightings data structure The sightings table has many other variables: cruz$cohorts$all$sightings %&gt;% names [1] &quot;Event&quot; &quot;DateTime&quot; &quot;Lat&quot; &quot;Lon&quot; [5] &quot;OnEffort&quot; &quot;Cruise&quot; &quot;Mode&quot; &quot;OffsetGMT&quot; [9] &quot;EffType&quot; &quot;ESWsides&quot; &quot;Course&quot; &quot;SpdKt&quot; [13] &quot;Bft&quot; &quot;SwellHght&quot; &quot;WindSpdKt&quot; &quot;RainFog&quot; [17] &quot;HorizSun&quot; &quot;VertSun&quot; &quot;Glare&quot; &quot;Vis&quot; [21] &quot;ObsL&quot; &quot;Rec&quot; &quot;ObsR&quot; &quot;ObsInd&quot; [25] &quot;EffortDot&quot; &quot;EventNum&quot; &quot;file_das&quot; &quot;line_num&quot; [29] &quot;stratum_HI_EEZ&quot; &quot;stratum_OtherCNP&quot; &quot;stratum_WHICEAS&quot; &quot;year&quot; [33] &quot;month&quot; &quot;day&quot; &quot;yday&quot; &quot;km_int&quot; [37] &quot;km_cum&quot; &quot;ship&quot; &quot;stratum&quot; &quot;seg_id&quot; [41] &quot;use&quot; &quot;SightNo&quot; &quot;Subgroup&quot; &quot;SightNoDaily&quot; [45] &quot;Obs&quot; &quot;ObsStd&quot; &quot;Bearing&quot; &quot;Reticle&quot; [49] &quot;DistNm&quot; &quot;Cue&quot; &quot;Method&quot; &quot;Photos&quot; [53] &quot;Birds&quot; &quot;CalibSchool&quot; &quot;PhotosAerial&quot; &quot;Biopsy&quot; [57] &quot;CourseSchool&quot; &quot;TurtleSp&quot; &quot;TurtleGs&quot; &quot;TurtleJFR&quot; [61] &quot;TurtleAge&quot; &quot;TurtleCapt&quot; &quot;PinnipedSp&quot; &quot;PinnipedGs&quot; [65] &quot;BoatType&quot; &quot;BoatGs&quot; &quot;PerpDistKm&quot; &quot;species&quot; [69] &quot;best&quot; &quot;low&quot; &quot;high&quot; &quot;prob&quot; [73] &quot;mixed&quot; &quot;ss_tot&quot; &quot;lnsstot&quot; &quot;ss_percent&quot; [77] &quot;n_sp&quot; &quot;n_obs&quot; &quot;n_best&quot; &quot;n_low&quot; [81] &quot;n_high&quot; &quot;calibr&quot; &quot;mixed_max&quot; &quot;spp_max&quot; [85] &quot;included&quot; Columns 42 onwards correspond to sightings information. Columns of note: species contains the species code. There is only one species-code per row (i.e, multi-species sightings have been expanded to multiple rows). best, low, and high contain the refined group size estimates, averaged across observers and calibrated according to the cohort’s settings specifications. For multi-species sightings, these numbers represent the number of individuals for the single species represented in the row (i.e., the original group size estimate has been scaled by the percentage attritbuted to this species). The columns following those group size estimates (prob through spp_max) detail how group sizes were estimated: prob indicates whether probable species codes were accepted; mixed indicates whether this species’ sighting is part of a mixed-species sighting; n_sp provides the number of species occurring in this sighitng; n_obs gives the number of observers who contributed group size estimates; n_best through n_high gives the number of valid group size estimates given; and calibr indicates whether or not calibration was attempted for this sighting based on the settings (see next section); mixed_max indicates whether this species was the most abundant in the sighting (if multi-species); spp_max indicates the species code for the most abundant species in the sighting (if multi-species). As explained above, the final column, included, indicates whether this species should be included in the analysis. Here is a glimpse of the data: cruz$cohorts$all$sightings %&gt;% glimpse Rows: 3,932 Columns: 85 $ Event &lt;chr&gt; &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;… $ DateTime &lt;dttm&gt; 1986-11-26 09:00:00, 1986-11-26 14:40:00, 1986-11-26… $ Lat &lt;dbl&gt; 4.983333, 5.616667, 5.866667, 7.050000, 7.466667, 9.4… $ Lon &lt;dbl&gt; -120.9500, -121.6667, -121.9833, -123.5000, -123.9167… $ OnEffort &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ Cruise &lt;dbl&gt; 989, 989, 989, 989, 989, 989, 989, 989, 989, 989, 989… $ Mode &lt;chr&gt; &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;… $ OffsetGMT &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ EffType &lt;chr&gt; &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;… $ ESWsides &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,… $ Course &lt;dbl&gt; 310, 316, 313, 310, 310, 305, 305, 305, 305, 305, 305… $ SpdKt &lt;dbl&gt; 10.2, 9.9, 9.6, 10.2, 10.3, 9.8, 9.8, 9.6, 10.1, 10.1… $ Bft &lt;dbl&gt; 4, 4, 4, 3, 3, 2, 2, 2, 1, 1, 1, 4, 5, 4, 4, 3, 1, 2,… $ SwellHght &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ WindSpdKt &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ RainFog &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… $ HorizSun &lt;dbl&gt; 6, 9, 10, NA, 7, 5, 5, 7, 8, 8, 8, 3, 5, NA, 4, NA, N… $ VertSun &lt;dbl&gt; 2, 1, 3, NA, 1, 3, 3, 1, 1, 1, 1, 2, 1, NA, 1, NA, NA… $ Glare &lt;lgl&gt; FALSE, FALSE, FALSE, NA, FALSE, FALSE, FALSE, FALSE, … $ Vis &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ ObsL &lt;chr&gt; &quot;004&quot;, &quot;022&quot;, &quot;056&quot;, &quot;056&quot;, &quot;004&quot;, &quot;031&quot;, &quot;031&quot;, &quot;004… $ Rec &lt;chr&gt; &quot;056&quot;, &quot;031&quot;, &quot;062&quot;, &quot;062&quot;, &quot;056&quot;, &quot;022&quot;, &quot;022&quot;, &quot;056… $ ObsR &lt;chr&gt; &quot;062&quot;, &quot;057&quot;, &quot;004&quot;, &quot;004&quot;, &quot;062&quot;, &quot;057&quot;, &quot;057&quot;, &quot;062… $ ObsInd &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ EffortDot &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ EventNum &lt;chr&gt; &quot;18&quot;, &quot;43&quot;, &quot;59&quot;, &quot;9&quot;, &quot;32&quot;, &quot;10&quot;, &quot;10&quot;, &quot;22&quot;, &quot;34&quot;, … $ file_das &lt;chr&gt; &quot;CenPac1986-2020_Final_alb.das&quot;, &quot;CenPac1986-2020_Fin… $ line_num &lt;int&gt; 10295, 10321, 10340, 10358, 10384, 10451, 10451, 1046… $ stratum_HI_EEZ &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… $ stratum_OtherCNP &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ stratum_WHICEAS &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… $ year &lt;dbl&gt; 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1986,… $ month &lt;dbl&gt; 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1… $ day &lt;int&gt; 26, 26, 26, 27, 27, 28, 28, 28, 28, 28, 28, 29, 29, 3… $ yday &lt;dbl&gt; 330, 330, 330, 331, 331, 332, 332, 332, 332, 332, 332… $ km_int &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… $ km_cum &lt;dbl&gt; 64.38787, 172.92124, 221.24918, 248.23899, 317.83407,… $ ship &lt;chr&gt; &quot;DSJ&quot;, &quot;DSJ&quot;, &quot;DSJ&quot;, &quot;DSJ&quot;, &quot;DSJ&quot;, &quot;DSJ&quot;, &quot;DSJ&quot;, &quot;DSJ… $ stratum &lt;chr&gt; &quot;OtherCNP&quot;, &quot;OtherCNP&quot;, &quot;OtherCNP&quot;, &quot;OtherCNP&quot;, &quot;Othe… $ seg_id &lt;int&gt; 34, 35, 35, 35, 36, 37, 37, 37, 37, 37, 29, 38, 38, 3… $ use &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ SightNo &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;01&quot;, &quot;02&quot;, &quot;01&quot;, &quot;01&quot;, &quot;02&quot;, &quot;03&quot;,… $ Subgroup &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ SightNoDaily &lt;chr&gt; &quot;19861126_1&quot;, &quot;19861126_2&quot;, &quot;19861126_3&quot;, &quot;19861127_1… $ Obs &lt;chr&gt; &quot;004&quot;, &quot;057&quot;, &quot;004&quot;, &quot;056&quot;, &quot;004&quot;, &quot;031&quot;, &quot;031&quot;, &quot;004… $ ObsStd &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ Bearing &lt;dbl&gt; 335, 354, 336, 18, 332, 333, 333, 6, 312, 312, 38, 80… $ Reticle &lt;dbl&gt; 8.41, 3.88, 0.50, 11.40, 0.80, 0.30, 0.30, 2.98, 0.40… $ DistNm &lt;dbl&gt; 0.4, 0.8, 3.2, 0.3, 2.5, 3.8, 3.8, 1.0, 3.5, 3.5, 7.0… $ Cue &lt;dbl&gt; 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 6, 6, 3, 2, 3, 2,… $ Method &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,… $ Photos &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ Birds &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;N&quot;… $ CalibSchool &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ PhotosAerial &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ Biopsy &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ CourseSchool &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleSp &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleGs &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleJFR &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleAge &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleCapt &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ PinnipedSp &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ PinnipedGs &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ BoatType &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ BoatGs &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ PerpDistKm &lt;dbl&gt; 0.3130756, 0.1548694, 2.4104840, 0.1716898, 2.1736533… $ species &lt;chr&gt; &quot;049&quot;, &quot;015&quot;, &quot;077&quot;, &quot;002&quot;, &quot;002&quot;, &quot;033&quot;, &quot;018&quot;, &quot;037… $ best &lt;dbl&gt; 2.318841, 8.843199, 4.637681, 27.517262, 21.826776, 3… $ low &lt;dbl&gt; 2.000000, 6.253644, 4.637681, 17.139473, 14.144886, 1… $ high &lt;dbl&gt; 2.000000, 10.711798, NA, 35.140620, 27.788449, 45.560… $ prob &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… $ mixed &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE,… $ ss_tot &lt;dbl&gt; 2.318841, 8.843199, 4.637681, 27.517262, 21.826776, 3… $ lnsstot &lt;dbl&gt; 0.8410673, 2.1796487, 1.5342145, 3.3148135, 3.0831375… $ ss_percent &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000… $ n_sp &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,… $ n_obs &lt;int&gt; 1, 3, 1, 3, 3, 3, 3, 5, 5, 5, 1, 1, 3, 4, 2, 2, 2, 5,… $ n_best &lt;int&gt; 1, 2, 0, 3, 3, 1, 1, 4, 5, 5, 0, 1, 3, 3, 1, 2, 2, 4,… $ n_low &lt;int&gt; 1, 3, 1, 3, 3, 3, 3, 5, 5, 5, 1, 1, 3, 4, 2, 2, 2, 5,… $ n_high &lt;int&gt; 1, 2, 0, 3, 3, 1, 1, 4, 5, 5, 0, 1, 3, 3, 1, 2, 2, 4,… $ calibr &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ mixed_max &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, FALS… $ spp_max &lt;chr&gt; &quot;049&quot;, &quot;015&quot;, &quot;077&quot;, &quot;002&quot;, &quot;002&quot;, &quot;033&quot;, &quot;033&quot;, &quot;037… $ included &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… Note that the process_sightings() function draws upon cruz$settings for inclusion criteria, but some of those settings can be overridden with the function’s manual inputs if you want to explore your options (see below). School size estimates In the settings we are using in this tutorial, school size estimates are adjusted using the calibration models from Barlow, Gerrodette, and Perryman (1998) (their analysis is refined slightly and further explained in Gerrodette, Perryman and Barlow, 2002). These calibration corrections are observer-specific. Most observers tend to underestimate school size and their estimates are adjusted up; others tend to overestimate and their estimates are adjusted down. Some observers do not have calibration coefficients, and for them a generic adjustment (upwards, by dividing estimates by 0.8625) is used. In LTabundR, each observer’s estimate is calibrated, then all observer estimates are averaged. To do that averaging, our settings specify that we shall use a geometric weighted mean, instead of an arithmetic mean, that weights school size estimates from multiple observers according to the variance of their calibration coefficients. Here are our current best estimates of school size: cruz$cohorts$all$sightings$best %&gt;% head(20) [1] 2.318841 8.843199 4.637681 27.517262 21.826776 31.713333 [7] 3.786667 3.478261 21.284389 221.965766 2.318841 1.159420 [13] 13.758964 6.242983 16.940000 18.247174 1.159420 38.004297 [19] 35.000000 16.596526 Let’s compare those estimates to unadjusted ones, in which calibration (and therefore weighted geometric mean) is turned off: cruz_demo &lt;- process_sightings(cruz, calibrate = FALSE, verbose = FALSE) cruz_demo$cohorts$all$sightings$best %&gt;% head(20) [1] 2.000000 8.485281 4.000000 21.897596 16.570558 23.226667 [7] 2.773333 3.000000 20.885620 217.807182 2.000000 1.000000 [13] 11.744603 5.517848 12.000000 15.000000 1.000000 38.985490 [19] 35.000000 14.642958 Note that, since calibration is only used for schools above a certain size, the difference between calibration and non-calibrated estimates becomes clearer in larger groups. You can also carry out calibration corrections without using a geometric weighted mean (the arithmetic mean will be used instead): cruz_demo &lt;- process_sightings(cruz, calibrate = TRUE, geometric_mean = FALSE, verbose = FALSE) cruz_demo$cohorts$all$sightings$best %&gt;% head(20) [1] 2.318841 9.217391 4.637681 27.866184 22.455556 31.713333 [7] 3.786667 3.478261 24.139715 251.742745 2.318841 1.159420 [13] 13.744928 6.198068 16.940000 18.095652 1.159420 46.792494 [19] 35.000000 17.131014 Note that when geometric_mean = TRUE but calibration is not carried out, the simple geometric mean is calculated instead of the weighted geometric mean, since the weights are the variance estimates from the calibration routine. Also note that school size calibration is only carried out if settings$group_size_calibration is not NULL. However, even when calibration coefficients are provided, it is possible to specify that calibration should only be carried out for raw estimates above a minimum threshold (see cohort setting calibration_floor, whose default is 0), since observers may be unlikely to mis-estimate the school size of a lone whale or pair. For observers who have calibration coefficients in the settings$group_size_coefficients table, that minimum is specified for each observer individually. For observers not in that table, calibration will only be applied to raw school size estimates above settings$cohorts[[i]]$calibration_floor or above. Subgroup size estimates After sightings data are processed, the process_surveys() function calls the subroutine process_subgroups() to find and calculate subgroup school size estimates for false killer whales, if any occur in the DAS data (Event code “G”). cruz &lt;- process_subgroups(cruz) If subgroups are found, a subgroups slot is added to the analysis list for a cohort. cruz$cohorts$all %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; &quot;subgroups&quot; This subgroups slot holds a list with three dataframes: events (each row is a school size estimate for a single subgroup during a single phase – 1 or 2 – within a single sighting; this is effectively the raw data); subgroups (each row is a single phase for a single subgroup, with all school size estimates averaged together (both arithmetically and geometrically); and sightings (each row is a school size estimate for a single phase for a single sighting, with all subgroup school sizes summed together). cruz$cohorts$all$subgroups %&gt;% names [1] &quot;sightings&quot; &quot;subgroups&quot; &quot;events&quot; See the case study on false killer whales for detailed examples. Review By the end of this process, you have a single data object, cruz, with all the data you need to move forward into the next stages of mapping and analysis. The LTabundR function cruz_structure() provides a synopsis of the data structure: cruz_structure(cruz) &quot;cruz&quot; list structure ======================== $settings $strata --- with 11 polygon coordinate sets $survey --- with 10 input arguments $cohorts --- with 3 cohorts specified, each with 19 input arguments $strata ... containing a summary dataframe of 11 geostrata and their spatial areas ... geostratum names: HI_EEZ, OtherCNP, MHI, WHICEAS, Spotted_OU, Spotted_FI, Spotted_BI, Bottlenose_KaNi, Bottlenose_OUFI, Bottlenose_BI, NWHI $cohorts $all geostrata: WHICEAS, HI_EEZ, OtherCNP $segments --- with 1888 segments (median = 149.3 km) $das --- with 329609 data rows $sightings --- with 3932 detections $subgroups --- with 255 subgroups, 49 sightings, and 389 events $bottlenose geostrata: WHICEAS, HI_EEZ, OtherCNP, Bottlenose_BI, Bottlenose_OUFI, Bottlenose_KaNi $segments --- with 2049 segments (median = 148.8 km) $das --- with 329609 data rows $sightings --- with 522 detections $spotted geostrata: WHICEAS, HI_EEZ, OtherCNP, Spotted_OU, Spotted_FI, Spotted_BI $segments --- with 2053 segments (median = 148.5 km) $das --- with 329609 data rows $sightings --- with 527 detections Each species-specific cohort has its own list under cruz$cohorts, and each of these cohorts has the same list structure: segments is a summary table of segments. das is the raw DAS data, modified with seg_id to associate each row with a segment. sightings is a dataframe of sightings processed according to this cohort’s settings. subgroups (if any subgroup data exist in your survey) is a list with subgroup details. In each of these data.frame’s, there are three critically important columns to keep in mind: seg_id: this column is used to indicate the segment ID that a row of data belongs to. use: this column indicates whether a row of effort should be used in the line-transect analysis Every row of data within a single segment with have the same use value. included: this column occurs in the sightings dataframe only. It indicates whether the sightings should be included in line-transect analysis based on the specified settings. Any sighting with use == FALSE will also have included == FALSE, but it is possible for sightings to have use == TRUE with included == FALSE. For example, if the setting abeam_sightings is set to FALSE, a sighting with a bearing angle beyond the ship’s beam can be excluded from the analysis (included == FALSE) even though the effort segment it occurs within will still be used (use == TRUE). Finally, let’s save this cruz object locally, to use in downstream scripts: save(cruz,file=&#39;whiceas_cruz.RData&#39;) "],["maps.html", " 4 Maps Publishable maps Interactive maps Interactive dashboard", " 4 Maps To build a flexible system for mapping cruise data, we have the following functions: Publishable maps Base maps Begin with a basic map, including EEZ borders: m &lt;- map_base(region=&#39;cnp&#39;) m We also have a base map for the California Current … m &lt;- map_base(region=&#39;ccs&#39;) m And the ETP: m &lt;- map_base(region=&#39;etp&#39;) m Add strata Add your research strata to your map: m &lt;- map_base(region=&#39;cnp&#39;) m1 &lt;- map_strata(m, cruz_1720$settings, region=&#39;cnp&#39;) Add survey tracks m1 &lt;- map_effort(m, cruz_1720) m1 The defaults of map_effort() assume, for simplicity, that you want to see the segments to be included in density estimation for the first cohort specified in your settings. You can adjust this and other defaults using the function arguments. Customizing effort Inputs This map changes survey track thickness and color. m1 &lt;- map_effort(m, cruz_1720, effort_color=&#39;firebrick&#39;, effort_stroke=2.5, effort_linetype=1,) Color-code conditions Your second customization option is to add format variables to the segments slot of the cohort of interest in the cruz object. This gives you full control of line color, thickness, and line-type according to whatever specifications you wish to set, e.g., color-coding by effort type or Beaufort sea state. This is possible because the function map_effort() looks for the variables col (line color), lwd (line thickness or stroke), and lty (line type) in the columns of cruz$segments. If these columns exist, the values therein will be used instead of the function defaults. For example, color-code by Beaufort scale: # Save copy of segments to modify cruzi &lt;- cruz_1720 segments &lt;- cruzi$cohorts$all$segments # Add column `col`: color code by BFT sea state bft_colors &lt;- c(&#39;steelblue4&#39;,&#39;steelblue2&#39;,&#39;cadetblue1&#39;,&#39;grey&#39;) segments$col &lt;- bft_colors[4] segments$col[ segments$avgBft &lt;= 7 ] &lt;- bft_colors[3] # bft 5 + segments$col[ segments$avgBft &lt;= 4 ] &lt;- bft_colors[2] # bft 3 - 4 segments$col[ segments$avgBft &lt;= 2 ] &lt;- bft_colors[1] # bft 0 -2 # Update sub_segments slot in `cruz` object cruzi$cohorts$all$segments &lt;- segments # Update map m_custom2 &lt;- map_effort(m, cruzi) # Add legend using native functions from mapping package `tmap` m_custom2 &lt;- m_custom2 + tmap::tm_add_legend(&#39;line&#39;, col = bft_colors, lwd = 3, labels = c(&#39; 0 - 2&#39;, &#39; 3 - 4&#39;, &#39; 5 +&#39;, &#39; no data&#39;), title=&quot;Beaufort sea state&quot;) + tmap::tm_layout(legend.position=c(&#39;left&#39;,&#39;bottom&#39;)) # Show map m_custom2 Add sightings Use the function map_sightings() to add sightings to your map: m1 &lt;- map_sightings(m, cruz_1720) Customizing sightings To demonstrate some of the customization options, consider this map that shows sightings of false killer whales with custom dot color, shape, and size: m1 &lt;- map_sightings(m, cruz_1720, include_species = &#39;033&#39;, color_base = &#39;purple&#39;, shape_base = 18, size_base = 1) Next is a map of humpback whales and sperm whales, color-coded by species and shape-coded by whether or not the sighting will be included in the analysis: m1 &lt;- map_sightings(m, cruz_1720, include_species = c(&#39;076&#39;,&#39;046&#39;), color_code = TRUE, shape_code = TRUE) Overview Here is an overview of the steps needed to map strata, survey tracks, and sightings all together: m &lt;- map_base(&#39;cnp&#39;) m &lt;- map_strata(m, cruz_1720$settings) m &lt;- map_effort(m, cruz_1720) m &lt;- map_sightings(m, cruz_1720, size_base=.4) m Interactive maps LTabundR also has an interactive map function, which maps survey data using the leaflet package. map_cruz(cruz_1720, cohort=1, eez_show=FALSE, strata_show=FALSE, effort_show=TRUE, effort_resolution=1, sightings_show=TRUE, sightings_color = &#39;firebrick&#39;, verbose=FALSE) Note that you can also click on sightings and tracklines to see their details. Refer to the documentation for this function (?map_cruz) to see all the options available for stylizing these maps. Interactive dashboard Finally, note that LTabundR comes with an interactive data explorer app (a Shiny app) for filtering survey data according to effort scenario and species code, toggling map_cruz() settings, and reviewing summary tables of effort and sightings (including inspection of truncation distances). cruz_explorer(cruz) Screenshots from this app: "],["filter.html", " 5 Filter &amp; combine surveys Filter Combine", " 5 Filter &amp; combine surveys You may soon encounter the need to filter a processed cruz object to only certain years, regions, or cruise numbers. You may also need to combine one processed cruz object with another. Here we will continue with the cruz object we created on the previous page. As a reminder, here is the data structure of that object: cruz_structure(cruz) &quot;cruz&quot; list structure ======================== $settings $strata --- with 11 polygon coordinate sets $survey --- with 10 input arguments $cohorts --- with 3 cohorts specified, each with 19 input arguments $strata ... containing a summary dataframe of 11 geostrata and their spatial areas ... geostratum names: HI_EEZ, OtherCNP, MHI, WHICEAS, Spotted_OU, Spotted_FI, Spotted_BI, Bottlenose_KaNi, Bottlenose_OUFI, Bottlenose_BI, NWHI $cohorts $all geostrata: WHICEAS, HI_EEZ, OtherCNP $segments --- with 1888 segments (median = 149.3 km) $das --- with 329609 data rows $sightings --- with 3932 detections $subgroups --- with 255 subgroups, 49 sightings, and 389 events $bottlenose geostrata: WHICEAS, HI_EEZ, OtherCNP, Bottlenose_BI, Bottlenose_OUFI, Bottlenose_KaNi $segments --- with 2049 segments (median = 148.8 km) $das --- with 329609 data rows $sightings --- with 522 detections $spotted geostrata: WHICEAS, HI_EEZ, OtherCNP, Spotted_OU, Spotted_FI, Spotted_BI $segments --- with 2053 segments (median = 148.5 km) $das --- with 329609 data rows $sightings --- with 527 detections Filter LTabundR lets you filter a cruz object using the function filter_cruz(). For example, in our WHICEAS case study, we processed surveys from 1986 - 2020, which we needed to do to model our detection functions, but our interest for mapping is specifically valid effort in 2017 and 2020 only, and only within the \"WHICEAS\" geostratum. cruz_1720 &lt;- filter_cruz(cruz, analysis_only = TRUE, years = c(2017, 2020), regions = &#39;WHICEAS&#39;) We will use this filtered cruz object for mapping &amp; sightings summaries downstream. save(cruz_1720,file=&#39;whiceas_cruz_1720.RData&#39;) Note that filter_cruz() has many other filter options. See ?filter_cruz() for details. Combine Say you have two processed cruz objects: one containing survey effort from the WHICEAS geostratum area only, and one containing survey effort from the pelagic Hawaiian EEZ geostratum (\"HI_EEZ\") that does not include WHICEAS effort. Let’s make those fake datasets right now, using filter_cruz(): WHICEAS-only data: cruz_whiceas &lt;- filter_cruz(cruz, regions = &#39;WHICEAS&#39;, verbose = FALSE) Pelagic Hawaiian EEZ - only data: cruz_hi_eez &lt;- filter_cruz(cruz, regions = &#39;HI_EEZ&#39;, not_regions = &#39;WHICEAS&#39;, verbose = FALSE) Say you want to combine these datasets together in order to reconstruct the equivalent of our original cruz object. You can do this with the LTabundR function, cruz_combine(). # Make a list of cruz objects cruzes &lt;- list(cruz_whiceas, cruz_hi_eez) # Now combine cruz_demo &lt;- cruz_combine(cruzes) Re-constituted data structure: cruz_structure(cruz_demo) &quot;cruz&quot; list structure ======================== $settings $strata --- with 11 polygon coordinate sets $survey --- with 10 input arguments $cohorts --- with 3 cohorts specified, each with 19 input arguments $strata ... containing a summary dataframe of 11 geostrata and their spatial areas ... geostratum names: HI_EEZ, OtherCNP, MHI, WHICEAS, Spotted_OU, Spotted_FI, Spotted_BI, Bottlenose_KaNi, Bottlenose_OUFI, Bottlenose_BI, NWHI $cohorts $all geostrata: WHICEAS, HI_EEZ, OtherCNP $segments --- with 1013 segments (median = 149.1 km) $das --- with 203387 data rows $sightings --- with 1793 detections $subgroups --- with 146 subgroups, 21 sightings, and 212 events $bottlenose geostrata: WHICEAS, HI_EEZ, OtherCNP, Bottlenose_BI, Bottlenose_OUFI, Bottlenose_KaNi $segments --- with 1174 segments (median = 128.3 km) $das --- with 203387 data rows $sightings --- with 277 detections $spotted geostrata: WHICEAS, HI_EEZ, OtherCNP, Spotted_OU, Spotted_FI, Spotted_BI $segments --- with 1178 segments (median = 113.3 km) $das --- with 203387 data rows $sightings --- with 115 detections "],["summarize.html", " 6 Summarize survey Summarize effort Summarize by Beaufort Summarize sightings Summarize certain species cruz_explorer()", " 6 Summarize survey Here we will summarize the 2017 &amp; 2020 survey data we prepared on the previous page. load(&#39;whiceas_cruz_1720.RData&#39;) Summarize effort The summarize_effort() functions builds tables with total kilometers and days surveyed. effort &lt;- summarize_effort(cruz_1720, cohort=1) This function summarizes effort in several default tables: effort %&gt;% names() [1] &quot;total&quot; &quot;total_by_cruise&quot; &quot;total_by_year&quot; &quot;total_by_effort&quot; [5] &quot;total_by_stratum&quot; Total surveyed The slot $total provides the grand total distance and unique dates surveyed: library(DT) effort$total %&gt;% DT::datatable(options=list(initComplete = htmlwidgets::JS( &quot;function(settings, json) {$(this.api().table().container()).css({&#39;font-size&#39;: &#39;9pt&#39;});}&quot;) )) Total surveyed by effort The slot $total_by_effort provides the total distance and days surveyed, grouped by segments that will be included in the analysis and those that won’t: Total surveyed by stratum The slot $total_by_stratum provides the total distance and days surveyed within each stratum, again grouped by segments that will be included in the analysis and those that won’t: Summarize by Beaufort bft &lt;- summarize_bft(cruz_1720, cohort=1) This function summarizes effort by Beaufort in four default tables: bft %&gt;% names() [1] &quot;overall&quot; &quot;by_year&quot; &quot;by_stratum&quot; &quot;details&quot; Simple overall breakdown The slot $overall provides the total effort – and proportion of effort – occurring in each Beaufort state: Breakdown by year The slot $by_year provides the above for each year separately: Breakdown by stratum The slot $by_stratum provides the above for each geostratum separately: Detailed breakdown The slot $details provides the above for each cruise-year-study area-geostratum combination within the data: Summarize sightings The summarize_sightings() function builds tables summarizing the sightings within each cohort-analysis. (Eventually, we may want to include an option to merge all sightings from all cohort-analyses into a single table.) sightings &lt;- summarize_sightings(cruz_1720, cohort=1) This function summarizes sightings in four default tables: sightings %&gt;% names() [1] &quot;simple_totals&quot; &quot;analysis_totals&quot; [3] &quot;stratum_simple_totals&quot; &quot;stratum_analysis_totals&quot; Simple species totals The slot $simple_totals includes all sightings, even if they will not be inluded in analysis: Analysis totals The slot $analysis_totals only includes sightings that meet all inclusion criteria for the analysis: Simple totals for each stratum The slot $stratum_simple_totals splits the first table (simple species totals) so that sightings are tallied for each geo-stratum separately: Analysis totals for each stratum The slot $stratum_analysis_totals splits the second table (analysis totals for each species) so that sightings are tallied for each geo-stratum separately: Summarize certain species To deep-dive into details for a ceratin species (or group of species), use the function summarize_species(). species &lt;- summarize_species(spp=&#39;046&#39;, cruz_1720) Error in summarize_species(spp = &quot;046&quot;, cruz_1720): object &#39;cohorti&#39; not found This functions a list with a variety of summaries: species %&gt;% names Error in species %&gt;% names: object &#39;species&#39; not found The slots $n_total and $n_analysis provide the total number of sightings and the number eligible for inclusion in the analysis: species$n_total Error in eval(expr, envir, enclos): object &#39;species&#39; not found species$n_analysis Error in eval(expr, envir, enclos): object &#39;species&#39; not found School size details This table only includes the sightings eligible for analysis: Error in crosstalk::is.SharedData(data): object &#39;species&#39; not found Annual summaries (all sightings) Error in crosstalk::is.SharedData(data): object &#39;species&#39; not found Annual summaries (analysis only) Error in crosstalk::is.SharedData(data): object &#39;species&#39; not found Regional summaries (all sightings) Error in crosstalk::is.SharedData(data): object &#39;species&#39; not found Regional summaries (analysis only) Error in crosstalk::is.SharedData(data): object &#39;species&#39; not found Detection distances This table can be used to determine the best truncation distance to use, based on the percent truncation you wish and the number of sightings available at each option. Error in crosstalk::is.SharedData(data): object &#39;species&#39; not found All sightings data Finally, this last slot holds a dataframe of all sightings data for the specified species: Error in crosstalk::is.SharedData(data): object &#39;species&#39; not found cruz_explorer() Note that all of these summary tables can be viewed interactively using the function cruz_explorer(), which allows you to efficiently subset the data according to various filters. cruz_explorer(cruz_1720) "],["g0.html", " 7 Estimating g(0) Relative g(0) Weighted average g(0)", " 7 Estimating g(0) Detection function models assume g(0) is 1.0. In distance sampling, a “detection function” is fit to the your sighting distances to reflect the fact that animals farther out are more difficult to detect. That detection function is a model of how the probability of detection declines with increasing distance from your survey trackline. The equations for detection function models are all constructed to assume that the probability of detecting an animal on your trackline (distance = 0 km) is 1.0 – you never miss an animal on your trackline. This trackline detection probability is referred to as g(0). In reality, though, it almost never is – and it greatly impacts results. When searching for marine mammals at sea, even some of those occurring directly on your survey trackline will be missed. Real g(0) is actually less than 1.0. This technicality makes a big difference: if g(0) is actually 0.5, the assumption that g(0) is 1.0 will underestimate animal abundance by 50%. Some animals, such as pygmy and dwarf sperm whales (Genus Kogia), are very cryptic and easily missed, and thus likely have a true g(0) below 0.1. This means that estimates assuming their g(0) is still 1.0 will underestimate true abundance by 90%! Moreover, all species – whether you are a pygmy sperm whale or a blue whale – become easier to miss when sighting conditions deteriorate (e.g., Beaufort sea states 4 - 6). Therefore, g(0) must be estimated then used to scale the detection function. So g(0) matters, and the assumption of most detection functions that g(0) = 1.0 is nearly always wrong. Luckily, there is a way to handle this that avoids constructing new detection function equations or estimating even more parameters during the detection function fitting process: we use an estimate of g(0) to scale the detection function. Say the detection function predicts that the probability of detection is 1.0, 0.8, and 0.6 at distances 0 km, 1 km, and 2km, respectively. If g(0) is actually 0.5, then we can scale the detection function so that those respective predictions are now 0.5, 0.4, and 0.3. Estimating g(0) for a survey generally involves four steps: First, you estimate g(0) in perfect conditions (i.e., Beaufort sea state 0). Second, you scale that estimate downward to approximate g(0) when conditions are less than ideal (i.e., a separate g(0) estimate for each Beaufort sea state from 1 to 6). This is known as the relative trackline probability, or relative g(0), or most simply: Rg(0). Third, you determine the weighted average value of g(0) for your particular survey, based on the proportional distribution of effort in each sea state. You pass this single value to your line-transect analysis functions. Fourth, you need to determine the CV of your weighted estimate of g(0). This isn’t straightforward, because you first need to simulate a new distribution for the weighted g(0) estimate, from which you then calculate the CV. The first of these steps is typically the biggest lift analytically, and very few studies provide absolute estimates of g(0) for their species of interest. Doing so involves Bayesian simulations and special field methods (see this example from Jay Barlow, NOAA-NFMFS Southwest Fisheries Science Center). Instead, most studies assume that the absolute g(0) is in fact 1.0 – even though it’s not – and proceed directly to the second step – relative trackline probability, Rg(0). This is more common because it can be estimated directly from the survey data, thanks to an approached developed in Barlow (2015), “Inferring trackline detection probabilities, g(0), for cetaceans from apparent densities in different survey conditions” (Marine Mammal Science). Relative g(0) Archived Rg(0) estimates As of 2022, most northeast Pacific NOAA-NMFS studies still use the Rg(0) estimates from Barlow (2015), which are based on NOAA-NMFS cruises from 1986 to 2010. LTabundR includes Barlow’s results in a built-in dataset. data(barlow_2015) Here is the top of this dataset, in which each row is a Rg(0) estimate for a single species - Beaufort sea state scenario. barlow_2015 %&gt;% head(12) title scientific spp truncation 1 Delphinus spp Delphinus 005-016-017 5.5 2 Delphinus spp Delphinus 005-016-017 5.5 3 Delphinus spp Delphinus 005-016-017 5.5 4 Delphinus spp Delphinus 005-016-017 5.5 5 Delphinus spp Delphinus 005-016-017 5.5 6 Delphinus spp Delphinus 005-016-017 5.5 7 Delphinus spp Delphinus 005-016-017 5.5 8 Stenella attenuata ssp Stenella attenuata ssp 002-006-089,-090 5.5 9 Stenella attenuata ssp Stenella attenuata ssp 002-006-089,-090 5.5 10 Stenella attenuata ssp Stenella attenuata ssp 002-006-089,-090 5.5 11 Stenella attenuata ssp Stenella attenuata ssp 002-006-089,-090 5.5 12 Stenella attenuata ssp Stenella attenuata ssp 002-006-089,-090 5.5 pooling regions bft Rg0 Rg0_CV 1 none none 0 1.000 0.00 2 none none 1 1.000 0.00 3 none none 2 0.940 0.25 4 none none 3 0.722 0.25 5 none none 4 0.485 0.14 6 none none 5 0.394 0.20 7 none none 6 0.404 0.50 8 none none 0 1.000 0.00 9 none none 1 0.728 0.03 10 none none 2 0.531 0.06 11 none none 3 0.386 0.09 12 none none 4 0.282 0.12 This dataset includes Rg(0) estimates for the following species: barlow_2015$title %&gt;% unique [1] &quot;Delphinus spp&quot; &quot;Stenella attenuata ssp&quot; [3] &quot;Stenella longirostris ssp&quot; &quot;Striped dolphin&quot; [5] &quot;Rough-toothed dolphin&quot; &quot;Bottlenose dolphin&quot; [7] &quot;Risso&#39;s dolphin&quot; &quot;Short-finned pilot whale&quot; [9] &quot;Killer whale&quot; &quot;Sperm whale&quot; [11] &quot;Kogia spp&quot; &quot;Cuvier&#39;s beaked whale&quot; [13] &quot;Mesoplodon spp&quot; &quot;Dall&#39;s porpoise&quot; [15] &quot;Minke whale&quot; &quot;Sei/Bryde&#39;s&quot; [17] &quot;Fin whale&quot; &quot;Blue whale&quot; [19] &quot;Humpback whale&quot; &quot;Unidentified dolphin&quot; [21] &quot;Unidentified cetacean&quot; &quot;Pacific white-sided dolphin&quot; [23] &quot;Pygmy killer whale&quot; If your study species – or one with similar detectability – can be found on this list, then you can take this data.frame of Rg(0) values and move on to the next step. Note that if you do not have a large survey dataset, this may be the only option available to you. Estimating new Rg(0) values requires a large number of sightings (i.e., hundreds) across many Beaufort states. New Rg(0) estimates LTabundR includes a function, g0_model(), which you can use to apply the Barlow (2015) modeling methods to generate new estimates of Rg(0) based on your own survey data. To do this, you first need a cruz object in which effort has been split into short segments (5 - 10 km). If you want to work with NOAA-NMFS WinCruz data from the Pacific, you can use a built-in dataset of 1986 - 2020 surveys that is processed specifically for use in Rg(0) estimation: data(&quot;noaa_10km_1986_2020&quot;) To use this dataset in R(0) estimation, we first filter it to systematic effort within sea states 0 - 6: cruzi &lt;- filter_cruz(noaa_10km_1986_2020, analysis_only = TRUE, eff_types = &#39;S&#39;, bft_range = 0:6, on_off = TRUE) You can then estimate Rg(0) for each Beaufort sea state using the function g0_model(). For example, the code for striped dolphin (Stenella coeruleoalba) is as follows: rg0 &lt;- g0_model(spp = &#39;013&#39;, truncation_distance = 5.5, cruz = cruzi, pool_bft = NULL, jackknife_fraction = .1) The jackknife_fraction input indicates that standard error and CV will be estimated using an iterative jackknife procedure in which 10% of the data is removed in each iteration. Find more details on this process using the function documentation, ?g0_model(). The input pool_bft provides a way to specify that low Beaufort sea states, which are typically rare in open-ocean surveys, should be pooled. This step may be needed in order to achieve a monotonic decline in the g(0) ~ Bft relationship for some species, but the default is NULL, i.e., no pooling. If pool_bft is the character string \"01\", Beaufort states 1 will be pooled into state 0. If pool_bft is the character string \"012\", Beaufort states 1 and 2 will be pooled into state 0. We recommend beginning with NULL then modifying this if needed, based on the output. The chief result of this function is a $summary table: rg0$summary bft Rg0 ESW n Rg0_SE Rg0_CV ESW_SE 1 0 1.0000000 3.852646 10 0.00000000 0.00000000 0.21940156 2 1 0.8721458 3.628054 10 0.07090241 0.08129651 0.17790469 3 2 0.6955418 3.388363 10 0.09678853 0.13915558 0.12854327 4 3 0.4490084 3.138752 10 0.06556831 0.14602914 0.08001243 5 4 0.2760549 2.887344 10 0.03741221 0.13552456 0.05958114 6 5 0.2185606 2.643238 10 0.03275263 0.14985606 0.08844467 7 6 0.2085412 2.414272 10 0.04340699 0.20814588 0.13017458 The model predicts that Rg(0) declines rapidly with deteriorating sea state: plot(Rg0 ~ bft, data = rg0$summary, ylim=c(0,1), type=&#39;o&#39;, pch=16) abline(h=seq(0,1,by=.1), col=&#39;grey85&#39;, lty=3) In addition to the summary above, this function returns various details, including the details of the Generalized Additive Model (GAM) (for both the estimate and the jackknifed datasets), and the raw sightings and segments used in the model. [1] &quot;Rg0&quot; &quot;gam&quot; &quot;jackknife&quot; &quot;summary&quot; &quot;sightings&quot; &quot;segments&quot; To produce these estimates efficiently for many species, you can use the wrapper function g0_table(), as follows. First you build a list of parameters for each species/species group: species &lt;- list( list(spp = c(&#39;005&#39;, &#39;016&#39;, &#39;017&#39;), title = &#39;Delphinus spp&#39;, truncation = 5.5), list(spp = &#39;021&#39;, title = &quot;Risso&#39;s dolphin&quot;, truncation = 5.5, pool_bft = &#39;12&#39;), list(spp = &#39;046&#39;, title = &#39;Sperm whale&#39;, truncation = 5.5), list(spp = c(&#39;047&#39;, &#39;048&#39;, &#39;080&#39;), title = &#39;Kogia spp&#39;, truncation = 4.0), list(spp = &#39;061&#39;, title = &quot;Cuvier&#39;s beaked whale&quot;, truncation = 4.0), list(spp = &#39;074&#39;, title = &#39;Fin whale&#39;, truncation = 5.5, regions = &#39;CCS&#39;)) Note that we used shorter truncation distances for cryptic species. Note also that we had to pool Beaufort sea states 0-2 for Risso’s dolphins in order to maintain a monotonic decline in the Rg(0) ~ Beaufort curve. Note also that we limited the geostrata used to model the fin whale Rg(0) curve to the California Current System (‘CCS’, after Barlow 2015), so that zero-inflated segments did not confound the model. We then pass this species list to g0_table(). In this example, we are only estimating the Rg(0) relationship, without conducting jackknife estimation: rg0s &lt;- g0_table(cruzi, species, eff_types = &#39;S&#39;, jackknife_fraction = NULL) Now plot the result using a dedicated LTabundR function: g0_plot(rg0s, panes=1) Weighted average g(0) Since g(0) clearly depends upon survey conditions, and since each survey is carried out in a specific sequence of conditions, a unique, weighted g(0) value must be estimated for each species in each geostratum and year of interest. This will be done automatically by the line-transect analysis functions coming up (see LTabundR::lta()), meaning you only need a table of Rg(0) values in order to proceed to the next step. But you can also calculate weighted g(0) values separately as an isolated analysis, which we show below. Let’s say we want to estimate the average g(0) for striped dolphins during the WHICEAS survey years of 2017 and 2020. From above, we have an estimate of the Rg(0) and its CV for each Beaufort state: rg0$summary %&gt;% select(bft, Rg0, Rg0_CV) bft Rg0 Rg0_CV 1 0 1.0000000 0.00000000 2 1 0.8721458 0.08129651 3 2 0.6955418 0.13915558 4 3 0.4490084 0.14602914 5 4 0.2760549 0.13552456 6 5 0.2185606 0.14985606 7 6 0.2085412 0.20814588 We also have a processed cruz object with 2017 data: load(&#39;whiceas_cruz_1720.RData&#39;) cruz_17 &lt;- filter_cruz(cruz_1720, years = 2017, verbose=FALSE) To view the distribution of effort in this WHICEAS 2017 across sea states, we can use the function summarize_bft(): summarize_bft(cruz_17)$overall # A tibble: 6 × 3 bftr km prop &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 130. 0.0200 2 2 583. 0.0897 3 3 901. 0.139 4 4 2140. 0.329 5 5 1873. 0.288 6 6 875. 0.135 We then use the function g0_weighted_var() to compute the weighted Rg(0) for our survey as well as its CV. This function carries out an automated optimization routine to simulate a new distribution for the weighted g(0), which is then used to estimate the weighted CV. weighted_g0_2017 &lt;- g0_weighted(Rg0 = rg0$summary$Rg0, Rg0_cv = rg0$summary$Rg0_CV, cruz = cruz_17) The result: weighted_g0_2017$g0[1:2] wt.mean wt.cv 1 0.326 0.143 Now let’s do the same for WHICEAS 2020 and compare the weighted g(0) estimate: # Filter to 2020 cruz_20 &lt;- filter_cruz(cruz_1720, years = 2020, verbose=FALSE) # Summarize Bft effort summarize_bft(cruz_20)$overall # A tibble: 6 × 3 bftr km prop &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 88.2 0.0165 2 2 262. 0.0490 3 3 434. 0.0813 4 4 1482. 0.278 5 5 2004. 0.376 6 6 1067. 0.200 Note that conditions were a bit worse in 2020 compared to 2017. We therefore expect the weighted g(0) estimate to be lower: weighted_g0_2020 &lt;- g0_weighted(Rg0 = rg0$summary$Rg0, Rg0_cv = rg0$summary$Rg0_CV, cruz = cruz_20) The result: weighted_g0_2020$g0[1:2] wt.mean wt.cv 1 0.286 0.149 Confirmed: weighted g(0) for 2020 is slightly lower than in 2017, due to generally worse survey conditions. These year-specific estimates should prevent those different conditions from impacting their respective abundance estimates. "],["lta.html", " 8 Line-transect analysis Key inputs Variance estimation Other inputs Output Unusual estimate scenarios Behind the scenes", " 8 Line-transect analysis We use line-transect analysis to produce estimates of animal density and/or abundance based upon surveys. The main LTabundR function for conducting line-transect analysis is lta(), which calls for four primary arguments in addition to your cruz object: lta(cruz, Rg0, fit_filters, df_settings, estimates) Below we explain each of these inputs, discuss other optional inputs, and explore the results produced by lta(). Key inputs cruz This is the cruz object you have generated with process_surveys(). Before running lta(), ensure that this cruz object is filtered only to the years, regions, and sighting conditions you would like to use for detection function fitting. Filter your cruz object with full flexibility using LTabundR::filter_cruz(). Note that filtering for detection function fitting is typically less stringent than filtering for downstream steps for abundance estimation, since as many sightings are included as possible to combat low sample sizes, as long as sightings were observed using standard methods in an unbiased search pattern, and as long as you do not expect detectability to vary across years and regions. Here we will work with a version of the 1986-2020 Central North Pacific survey data we processed a few pages back. This version is included as a built-in dataset within LTabundR: data(&quot;cnp_150km_1986_2020&quot;) cruz &lt;- cnp_150km_1986_2020 As it is provided, this dataset does not need any filtering. We will use these data to estimate the abundance of striped dolphins (Stenella coeruleoalba), Fraser’s dolphins (Lagenodelphis hosei), and Melon-headed whales (Preponocephala electra) within the WHICEAS study area in 2017 and 2020. We will group these three species into a ‘species pool’ in order to gain a sufficient sample size for fitting a detection function. We will then use “Species” as a covariate within the detection function model, along with other variables including Beaufort Sea State, ship name, and log-transformed school size. Rg0 The result of LTabundR::g0_model(), which is a data.frame with Relative trackline detection probabilities, Rg(0), for each species in each Beaufort sea state. See LTabundR dataset data(\"g0_results\"), used below, as an example. This is an optional input. If not provided, g(0) will be assumed to 1.0, and its CV will be assumed to be 0. Alternatively, you can manually specify values for g(0) and its CV in the estimates argument below. Here we will use a data.frame of Rg(0) estimates based on the same survey years, 1986 - 2020, which has been provided as a built-in dataset: data(&quot;g0_results&quot;) Rg0 &lt;- g0_results fit_filters The fit_filters input specifies how to filter the data before fitting the detection function. It accepts a named list, which in our example will look like this: fit_filters = list(spp = c(&#39;013&#39;, &#39;026&#39;, &#39;031&#39;), pool = &#39;Multi-species pool 1&#39;, cohort = &#39;all&#39;, truncation_distance = 5, other_species = &#39;remove&#39;) spp: A character vector of species codes. Using multiple species codes may be useful when you have low sample sizes for a cohort of similar species. cohort: The cohort containing these species, provided as a name or a number indicating which slot in cruz$cohorts should be referenced. truncation_distance: The truncation distance to apply during model fitting. The remaining inputs are optional (i.e., they all have defaults): pool: A character string, providing a title for this species pool. If not specified, the species codes used will be concatenated to produce a title automatically. other_species: A character vector with four recognized values: If \"apply\" (the default if not specified), the species code will be changed to \"Other\" for sightings in which the species was in a mixed-species school but was not the species with the largest percentage of the total school size. In those cases, the species was not as relevant to the detection of the school as the other species were, which may bias the detection function. This creates a factor level for the detection function to use (when \"species\" is a covariate) to distinguish between cue-relevant species that are within the specified pool and those that are not. The second option for other_species is \"ignore\", which does not reassign species codes to \"Other\", and ignores whether the species of interest held the plurality for a mixed species detection. The third option is \"remove\": any species re-assigned to \"Other\" will be removed before the detection function is fit; this can be useful if only a small number of species are re-assigned to \"Other\", which would then obviate species as a viable covariate (since the sample size of all species levels would be unlikely to exceed df_settings$covariates_n_per_level – see below). The fourth and final option is coerce, which forces all species codes to \"Other\" for the purposes of detection function fitting and abundance estimation. This is effectively the same as removing ‘species’ from the list of covariates, but this option can be a convenience if you want to quickly toggle the use of species as a covariate for a specific species pool, and/or produce abundance estimates for unidentified taxa (e.g., an ‘Unidentified dolphins’ species pool that includes multiple species codes). df_settings The df_settings input specifies how to fit a detection function to the filtered data. It accepts a named list, which in our example will look like this: df_settings = list(covariates = c(&#39;bft&#39;,&#39;lnsstot&#39;,&#39;cruise&#39;,&#39;year&#39;,&#39;ship&#39;,&#39;species&#39;), covariates_factor = c(FALSE, FALSE, TRUE, TRUE, TRUE, TRUE), covariates_levels = 2, covariates_n_per_level = 10, detection_function_base = &#39;hn&#39;, base_model = &#39;~1&#39;, delta_aic = 2) (Note that all of these inputs have defaults, and are therefore optional.) covariates Covariates you wish to include as candidates in detection function models, provided as a character vector. The covariates must match columns existing within cruz$cohorts$&lt;cohort_name&gt;$sightings. Note that the function will ignore case, coercing all covariates to lowercase. Default: no covariates. covariates_factor A Boolean vector, which must be the same length as covariates, indicating whether each covariate should be treated as a factor instead of a numeric. Default: NULL. covariates_levels The minimum number of levels a factor covariate must have in order to be included as an eligible covariate. Default: 2. covariates_n_per_level The minimum number of observations within each level of a factor covariate. If this condition is not met, the covariate is excluded from the candidates. Default: 10. detection_function_base The base key for the detection function, provided as a character vector. Accepted values are \"hn\" (half-normal key, the default, which exhibits greater stability when fitting to cetacean survey data; Gerrogette and Forcada 2005), \"hr\" (hazard-rate), or c(\"hn\", \"hr), which will loop through both keys and attempt model fitting. base_model The initial model formula, upon which to build using candidate covariates. If not provided by the user, the default is \"~ 1\". delta_aic The AIC difference between the model yielding the lowest AIC and other candidate models, used to define the best-fitting models. Typically, AIC differences of less than 2 (the default) indicate effectively equal model performance. If this value is not zero, then model averaging will be done: if multiple models are within delta_aic of the model with the lowest AIC, all “best” models will be used in subsequent steps and their results will be averaged. See Details below. estimates The estimates input specifies which estimates of density and abundance to produce based on the fitted detection function. This input accepts a list of sub-lists, which in our example will look something like this: estimates &lt;- list(list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2017, regions = &#39;WHICEAS&#39;)) This example shows only a single sub-list, specifying how to generate a density/abundance estimate for striped dolphins (species code 013) within the “WHICEAS” geostratum for 2017. spp: A character vector of species codes. title: A title for this abundance estimate, given as a character vector, ’ e.g., \"Striped dolphin - pelagic\". If left blank, the species code(s) will be concatenated to use as a title. years: A numeric vector of years, used to filter data to include only effort/sightings from these years. regions: A character vector of geostratum names, used to filter the data. Any segment or sighting occurring within any (but not necessarily all) of the provided regions will be returned. This holds true for nested regions: for example, in analyses from the Central North Pacific, in which the Hawaii EEZ geostratum (\"HI-EEZ\") is nested within the larger geostratum representing the entire CNP study area (\"OtherCNP\"), an input of regions = \"OtherCNP\" will return segments/sightings both inside the Hawaii EEZ and outside of it. You also have the option of manually specifying other filters &amp; arguments. Each of these sub-lists accepts the following named slots: cruises: An optional numeric vector of cruise numbers, used to filter data to include effort/sighting from only certain cruises. Ignored if NULL. regions_remove: A character vector of geostratum names, similar to above. Any segment or sighting occurring within any of these not_regions will not be returned. Using the example above, if regions = \"OtherCNP\" and not_regions = \"HI-EEZ\", only segments occuring within OtherCNP and outside of HI-EEZ will be returned. This can be particularly useful for abundance estimates for pelagic stock that exclude nested insular stocks. g0: If left as the default NULL, the lta() function will automatically estimate the weighted trackline detection probability (g0) according to the distribution of Beaufort sea states contained within the survey years/regions for which density/abundance is being estimated (this is done using the LTabundR function g0_weighted()). This will only be done if the Rg0 input above is not NULL; if it is and you do not provide g(0) values here, g0 will be coerced to equal 1. To coerce g(0) to a certain value of your own choosing, you can provide a numeric vector of length 1 or 2. If length 1, this value represents g(0) for all schools regardless of size. If length 2, these values represent g(0) for small and large school sizes, as defined by g0_threshold below. g0_cv: Similar to g0 above: if left NULL, the CV of the g(0) estimate will be automatically estimated based on weighted survey conditions. Alternatively, you can manually specify a CV here, using a numeric vector of length 1 or 2. If you do not specify a value and Rg0 input is NULL, g0_cv will be coerced to equal 0. g0_threshold: The school size threshold between small and large groups. alt_g0_spp: An alternate species code to use to draw Relative g(0) values from the Rg0 input. This is useful in the event that Rg(0) was not estimated for the species whose density/abundance you are estimating, but there is a similarly detectable species whose Rg(0) parameters have been estimated. combine_g0: A Boolean, with default FALSE. If TRUE, weighted g0 estimates will be produced separately for each species code provided (specifically, for each unique row in the Rg0 table that is found after filtering by the species codes you provide in this estimate), THEN average those estimates together. This can be useful when you do not have a Rg(0) estimates for a certain species, but you can approximate g0 by averaging together estimates from multiple species (e.g., averaging together weighted g(0) from across rorqual species in order to get a weighted g(0) estimate for ‘Unidentified rorquals’). region_title: An optional character vector indicating the title you would like to give to the region pertaining to this estimate. This can be useful if you have a complicated assemblage of regions you are combining and/or removing. If not supplied, the function will automatically generate a region_title based on regions and regions_remove. forced_effort: If this is a single numeric value instead of NULL (NULL is the default), this value will be used as the survey effort, in km, in a brute-force method; this same value will be used for every year and region. This is only helpful if you are looking for a relatively easy way to compare results from your own analysis to another (e.g., comparing LTabundR results to reports from NOAA reports prior to 2021, in which effort was calculated slightly differently). area: If this is a single numeric value instead of NULL (NULL is the default), this value will be used as the area of the region in which abundance is being estimated, in square km, in a brute-force approach. If left NULL, the function will calculate the final area of the survey area resulting from the regions and regions_remove filters above. remove_land: A Boolean, with default TRUE, indicating whether or not land area should be removed from the survey area before calculating its area for abundance estimation. This term is only referenced if area is not specified manually. Here is the full estimates list for all the species-year-geostratum combinations for which we want to estimate density/abundance: estimates &lt;- list( list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2017, regions = &#39;WHICEAS&#39;), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2020, regions = &#39;WHICEAS&#39;), list(spp = &#39;026&#39;, title = &#39;Frasers dolphin&#39;, years = 2017, regions = &#39;WHICEAS&#39;), list(spp = &#39;026&#39;, title = &#39;Frasers dolphin&#39;, years = 2020, regions = &#39;WHICEAS&#39;), list(spp = &#39;031&#39;, title = &#39;Melon-headed whale&#39;, years = 2017, regions = &#39;WHICEAS&#39;), list(spp = &#39;031&#39;, title = &#39;Melon-headed whale&#39;, years = 2020, regions = &#39;WHICEAS&#39;)) Each of these sub-lists specifies the details for a single estimate of density/abundance, making it possible to produce multiple estimates from the same detection function model. Generally, there needs to be a sub-list for each species-region-year combination of interest. You can imagine that building up these sub-lists can get tedious. It can also introduce the possibility of error or inconsistencies across estimates of multiple species. To address that issue, LTabundR includes the function lta_estimates(), which makes your code for preparing an estimates object much more efficient. That function is demonstrated in the Case Studies chapters. Quickly review results: results$estimate title species Region Area year segments km 1 Striped dolphin 013 (WHICEAS) 402948.7 2017 38 3040.009 2 Striped dolphin 013 (WHICEAS) 402948.7 2020 40 4585.386 3 Frasers dolphin 026 (WHICEAS) 402948.7 2020 40 4585.386 4 Melon-headed whale 031 (WHICEAS) 402948.7 2017 38 3040.009 5 Melon-headed whale 031 (WHICEAS) 402948.7 2020 40 4585.386 Area_covered ESW_mean n g0_est ER_clusters D_clusters N_clusters size_mean 1 9610.88 3.161465 3 0.315 0.0009868393 0.0005020214 202.28888 31.95079 2 16236.27 3.540874 3 0.280 0.0006542525 0.0003299532 132.95423 53.95865 3 17084.73 3.725909 2 0.446 0.0004361683 0.0001312386 52.88243 132.51143 4 10669.12 3.509569 2 0.511 0.0006578929 0.0001835580 73.96447 185.98103 5 16949.65 3.696451 3 0.483 0.0006542525 0.0001832491 73.84001 226.13289 size_sd ER D N 1 0.5780968 0.03153029 0.01606793 6474.550 2 2.7402089 0.03530258 0.01780121 7172.977 3 182.1747087 0.05779729 0.01733567 6985.385 4 96.4369707 0.12235559 0.03379710 13618.500 5 175.5620746 0.14794801 0.04158955 16758.457 More details on the lta() output are provided below. Variance estimation By default, the lta() function produces a single estimate of the detection function and a single estimate of density/abundance estimate for each sub-list within estimates(). However, you can obtain the coefficient of variation (CV) of those estimates by activating the function’s bootstrap variance estimation feature. To do this, add bootstraps as an input specifying a large number of iterations (1,000 iterations is standard, but we suggest first testing your code with 5 - 10 bootstraps before committing; the function typically requires ~1 hour per 100 bootstraps.). For the purposes of example only, we will just use 10 bootstrap iterations here: results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, bootstraps = 10) This command will first produce official estimates of the detection function and density/abundance, then it will repeat the analysis for the number of iterations you have specified. In each iteration, survey segments are re-sampled according to standard bootstrap variance estimation methods (see more details below, in “Behind the Scenes”). Other inputs There are a few other optional input arguments that lend further control over the lta() procedure. lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, ss_correction = 1, bootstraps = 10 toplot = TRUE, verbose = TRUE,) use_g0: A Boolean, with default TRUE, indicating whether or not to use custom g(0) value(s). If FALSE, the assumed g(0) value will be 1. This is an easy way to toggle on-and-off automated g(0) estimation and/or ignore manually supplied g(0) values. ss_correction: Should a correction be applied to school sizes? School sizes will be scaled by this number. The default, 1, means no changes will occur. toplot: A Boolean, with default TRUE, indicating whether detection function plots (Distance::plot.ds()) should be displayed as the candidate models are tested. verbose: A Boolean, with default TRUE, indicating whether or not updates should be printed to the Console. Output During processing While lta() is running, it will print things to the Console (if verbose is TRUE), plot diagnostic plots of how the study area is being calculated (if toplot is TRUE), and plot detection function model candidates (if toplot is TRUE). To demonstrate this, we will run the estimate for striped dolphins in 2017 only, without variance bootstrapping. To expedite processing, we will manually supply g(0) values from Bradford et al. (2021) (this saves about 3 minutes per estimate): new_estimates &lt;- list( list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2017, regions = &#39;WHICEAS&#39;, g0 = 0.35, g0_cv = 0.19)) # Run it: demo &lt;- lta(cruz, Rg0, fit_filters, df_settings, new_estimates) Error in create.model.frame(xmat, as.formula(ddfobj$scale$formula), meta.data, : NA covariate values in the data, check your data. Error in create.model.frame(xmat, as.formula(ddfobj$scale$formula), meta.data, : NA covariate values in the data, check your data. Error in create.model.frame(xmat, as.formula(ddfobj$scale$formula), meta.data, : NA covariate values in the data, check your data. Error in View(estimate_results, title = &quot;Abundance&quot;) : invalid &#39;x&#39; argument Error in create.model.frame(xmat, as.formula(ddfobj$scale$formula), meta.data, : NA covariate values in the data, check your data. Error in create.model.frame(xmat, as.formula(ddfobj$scale$formula), meta.data, : NA covariate values in the data, check your data. Error in create.model.frame(xmat, as.formula(ddfobj$scale$formula), meta.data, : NA covariate values in the data, check your data. Error in View(estimate_results, title = &quot;Abundance&quot;) : invalid &#39;x&#39; argument Error in create.model.frame(xmat, as.formula(ddfobj$scale$formula), meta.data, : NA covariate values in the data, check your data. Error in create.model.frame(xmat, as.formula(ddfobj$scale$formula), meta.data, : NA covariate values in the data, check your data. Error in create.model.frame(xmat, as.formula(ddfobj$scale$formula), meta.data, : NA covariate values in the data, check your data. Error in View(estimate_results, title = &quot;Abundance&quot;) : invalid &#39;x&#39; argument Error in create.model.frame(xmat, as.formula(ddfobj$scale$formula), meta.data, : NA covariate values in the data, check your data. Error in create.model.frame(xmat, as.formula(ddfobj$scale$formula), meta.data, : NA covariate values in the data, check your data. Error in create.model.frame(xmat, as.formula(ddfobj$scale$formula), meta.data, : NA covariate values in the data, check your data. Error in View(estimate_results, title = &quot;Abundance&quot;) : invalid &#39;x&#39; argument Error in lta(cruz, Rg0, fit_filters, df_settings, new_estimates): Three failed attempts to complete the analysis for this iteration! Additionally, windows will appear showing details for the detection function models and details of the density/abundance estimate. Outputs The lta() function returns a list of objects. To demonstrate this output, we will pull back in the dataset representing the result of the analysis above, for all three species in both years (with 5 bootstrap iterations): This list of results has five slots: names(results) [1] &quot;pool&quot; &quot;inputs&quot; &quot;estimate&quot; &quot;df&quot; &quot;bootstrap&quot; pool: The species pool pertaining to these estimates. inputs: A list of the inputs used to produce these estimates. estimate: A table of density/abundance estimates for each species/region/year combination specified in the estimates input. results$estimate title species Region Area year segments km 1 Striped dolphin 013 (WHICEAS) 402948.7 2017 38 3040.009 2 Striped dolphin 013 (WHICEAS) 402948.7 2020 40 4585.386 3 Frasers dolphin 026 (WHICEAS) 402948.7 2020 40 4585.386 4 Melon-headed whale 031 (WHICEAS) 402948.7 2017 38 3040.009 5 Melon-headed whale 031 (WHICEAS) 402948.7 2020 40 4585.386 Area_covered ESW_mean n g0_est ER_clusters D_clusters N_clusters size_mean 1 9610.88 3.161465 3 0.316 0.0009868393 0.0005004327 201.64872 31.95079 2 16236.27 3.540874 3 0.280 0.0006542525 0.0003299532 132.95423 53.95865 3 17084.73 3.725909 2 0.448 0.0004361683 0.0001306527 52.64635 132.51143 4 10669.12 3.509569 2 0.509 0.0006578929 0.0001842793 74.25510 185.98103 5 16949.65 3.696451 3 0.483 0.0006542525 0.0001832491 73.84001 226.13289 size_sd ER D N 1 0.5780968 0.03153029 0.01601708 6454.061 2 2.7402089 0.03530258 0.01780121 7172.977 3 182.1747087 0.05779729 0.01725827 6954.200 4 96.4369707 0.12235559 0.03392990 13672.011 5 175.5620746 0.14794801 0.04158955 16758.457 df: A named list with details for the detection function. results$df %&gt;% names [1] &quot;best_models&quot; &quot;all_models&quot; &quot;best_objects&quot; &quot;sightings&quot; &quot;sample_size&quot; [6] &quot;curve&quot; results$df$best_models Model Key_function Formula Pmean AIC 1 5 hn ~1 + ship + lnsstot 0.5481587 1058.131 2 7 hn ~1 + ship + lnsstot + species 0.5422751 1058.232 $\\\\Delta$AIC Covariates tested pool 1 0.000 bft, lnsstot, ship, species Multi-species pool 1 2 0.101 bft, lnsstot, ship, species Multi-species pool 1 bootstrap: If bootstrap variance estimation was carried out, the output would also include bootstrap, a named list with results from the bootstrap process, only returned if the bootstraps input is greater than 1. results$bootstrap %&gt;% names [1] &quot;summary&quot; &quot;details&quot; &quot;df&quot; results$bootstrap$summary %&gt;% head # A tibble: 5 × 18 # Groups: title, Region [3] title Region year species iterations ESW_mean g0_mean g0_cv km ER &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Frasers d… (WHIC… 2020 026 10 3.59 0.451 0.864 4512. 0.105 2 Melon-hea… (WHIC… 2017 031 10 3.36 0.393 0.816 3133. 0.153 3 Melon-hea… (WHIC… 2020 031 10 3.54 0.544 0.678 4512. 0.179 4 Striped d… (WHIC… 2017 013 10 3.34 0.313 0.150 3133. 0.0488 5 Striped d… (WHIC… 2020 013 10 3.58 0.277 0.128 4512. 0.0493 # … with 8 more variables: D &lt;dbl&gt;, size &lt;dbl&gt;, Nmean &lt;dbl&gt;, Nmedian &lt;dbl&gt;, # Nsd &lt;dbl&gt;, CV &lt;dbl&gt;, L95 &lt;dbl&gt;, U95 &lt;dbl&gt; Unusual estimate scenarios Most line-transect estimates in most areas are relatively straightforward: you want an estimate for a single species in a single year, and your geostrata do not overlap, nor do they need to be stratified or combined in funny ways. But there will also be unusual and slightly more complicated scenarios. We outline some of those below and demonstrate how they can be handled within the LTabundR framework. Species combinations &amp; g(0) Multi-species schools Multi-species schools can confound detection function model fitting, since your species of interest may not be the predominant species in the group, which means that the other species present may be having a greater influence over the detection function. You can decide how to account for this using the other_species slot in your fit_filters list. See the details on this discussed above. Species pools When you don’t have enough sightings of individual species to model a detection function effectively, it can be useful to pool sightings from multiple species who have similar detection characteristics. This is a common tactic in the Central North Pacific. When you do this, you typically need to make the following changes to a “normal” lta() call: In your df_settings list, consider adding species as a covariate, and ensure that you specify that it should be treated as a factor. This may improve detection function model fit. In your fit_filters list, specify multiple species codes and name your species pool accordingly (e.g., “Multi-species pool 1”). In your fit_filters list, specify how to handle “Other” species (see above). In your estimates list, add a sub-list for each species-region-year for which you want a density/abundance estimate. We took all of these steps in the example above with striped dolphins, Fraser’s dolphins, and melon-headed whales. Use that code as a guide. Pooling similar species Species that can be confused with one another may need to be pooled together for abundance estimation. For example, in the northeast Pacific, sei whales, Bryde’s whales, and fin whales can co-occur but they are difficult to distinguish in the field. They are also relatively uncommon, and may need to be pooled with other species in order to obtain a sound detection function model. To account for this, we want to estimate the density/abundance in the WHICEAS study area of all detections of sei, Bryde’s, fin whales together. To handle this, we will follow all the steps taken for a multi-species pool, as discussed above. Additionally, in our estimates sub-list(s), we will specify (1) that multiple species should be included in the estimate, and (2) that the weighted g(0) estimates for each of the individual species should be averaged together: estimates &lt;- list( list(spp = c(&#39;072&#39;, &#39;073&#39;, &#39;099&#39;, &#39;074&#39;), title = &quot;Sei/Bryde&#39;s/Fin whale&quot;, years = 2017, regions = &#39;WHICEAS&#39;, combine_g0 = TRUE)) Rare unidentified taxa A similar problem occur when you have species codes for unidentified taxa that have been identified down to a family- or genus-level. For example, “Unidenfitied Mesoplodon” is a species code (the code is \"050\") for any beaked whale that is definitely in the genus Mesoplodon. There are plenty of these sightings, which means g(0) and its CV can be estimated just fine without referring to other species codes. Other unidentified taxa, however, are less common. For example, in Hawaiian studies, density/abundance is estimated for “Unidentified rorquals”. In the field there is a species code for this group, \"070\", but it is rarely used – there are enough sightings to model the detection function, but not nearly enough sightings to estimate g(0) or its CV. In this case, we need to combine g(0) from more common species codes in order to estimate the unidentified rorqual’s g(0). To do this, we fit a detection function using species code \"070\" … fit_filters &lt;- list(spp = c(&#39;070&#39;), pool = &#39;Unidentified rorqual&#39;, cohort = &#39;all&#39;, truncation_distance = 5.5) … then, in our estimates list, we specify some alternate g(0) species designations. estimates &lt;- list( list(spp = &#39;070&#39;, title = &#39;Unidentified rorqual&#39;, years = 2017, regions = &#39;WHICEAS&#39;, alt_g0_spp = c(&#39;071&#39;,&#39;099&#39;,&#39;074&#39;,&#39;075&#39;), combine_g0 = TRUE)) Geostratum combinations In your estimates sub-lists, the regions and regions_remove slots give you control of the geographic scope of (1) the weighted g(0) and CV used in density estimation, (2) the effort and sightings used to estimate density, (3) and the area used to calculate abundance. A note on cohort geostrata Recall that, when processing your survey data to create a cruz object, you provide a list of geostrata as an argument in your process_surveys() call. You also have the option to specify a subset of those geostrata for each species cohort (see load_cohort_settings()), which is an option that you should almost always use. Selecting a subset of geostrata is important because that subset is used to “segmentize” your survey data – i.e., break effort into discrete sections that can be bootstrap-sampling during the lta() variance estimation routine – and the segmentizing procedure always breaks segments when a survey passes from one geostratum to another. This matters because the lta() bootstrapping routine will re-sample survey segments in a way that preserves the proportion of segments occurring in each geostratum, to ensure that all geostrata are represented in the same proportion as the original estimate. When segments are unncessarily broken into small segments by irrelevant geostrata that have been included in the analysis, the bootstrap estimate of the CV is likely to be too large. For example: in the Central North Pacific, there are about 11 geostrata commonly used. These include the Hawaiian EEZ geostratum, the Main Hawaiian Islands geostratum, and the larger CNP geostratum that represents the maximum range of the study area. These three geostrata are typically all you need for most density estimates for most species. However, a few species – e.g., bottlenose dolphin, pantropical spotted dolphin, and false killer whale – have special geostrata that represent insular stock boundaries and/or pelagic stock boundaries. If those insular geostrata are used in density estimates for which they do not apply, they will confound the bootstrap estimate of density/abundance CV. Punchline: be sure to specify only the relevant geostrata in each cohort’s settings. Combining disparate geostrata For example, in Hawaii bottlenose dolphins belong to a pelagic stock as well as several insular stocks. If you wished to estimate the abundance of all insular stocks together, you simply provide their respective geostratum names in the regions slot of your estimates sub-list: estimates &lt;- list(list(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;, years = 2017, regions = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;))) Removing insular geostrata Conversely, you may wish to estimate density/abundance for pelagic bottlenose dolphins only, ignoring the insular stocks. You can substract geostrata using the regions_remove slot: estimates &lt;- list(list(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;, years = 2017, regions = &#39;HI_EEZ&#39;, regions_remove = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;))) Combine partially overlapping geostrata Say you want to estimate the density/abundance for a set of geostrata that partially overlap. An example of this is that the Northwest Hawaiian Islands geostratum overlaps slightly with the Main Hawaiian Islands geostratum. This is not an issue; when study area is calculated within lta() (actually, that function calls another function, strata_area(), to do this. That function is demonstrated below), overlap among strata is accounted for. estimates &lt;- list(list(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;, years = 2017, regions = c(&#39;MHI&#39;,&#39;NWHI&#39;))) Regionally stratified analysis Field surveys are sometimes stratified such that trackline design and/or density can differ substantially across regions. Also, analysts may sometimes wish to estimate density/abundance for individual regions separately, regardless of design stratification. In lta(), you can accommodate a stratified analysis by providing an estimates sub-list for each geostratum. For example, in 2002 the Hawaiian EEZ was surveyed with different effort intensity in the Main Hawaiian Islands region compared to pelagic waters. For that reason, density/abundance estimates ought to be stratified by region: estimates &lt;- list( list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2002, regions = &#39;MHI&#39;), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2002, regions = &#39;HI_EEZ&#39;, regions_remove = &#39;MHI&#39;)) Here we have one 2002 estimate for the Main Hawaiian Islands, and a second for the pelagic Hawiian EEZ, achieved by subtracting the \"MHI\" stratum from the \"HI_EEZ\" stratum. Once lta() processing is complete, you can summarize and plot the results for each study area separately. The next step is to combine the stratified estimates to generate a grand estimate for the entire EEZ. This is achieved using the LTabundR functions lta_enlist() and lta_destratify(). We discuss this further in a later chapter. Subgroup-based analyses After 2010, Pacific Islands Fisheries Science Center (PIFSC) began a sub-group protocol referred to as the “PC Protocol” after the scientific name for false killer whales, Pseudorca crassidens, which was the species for which the protocol was designed. False killer whales are rare and occur in dispersed subgroups, which complicates conventional distance sampling approaches to line-transect analysis. To handle this, a separate, subgroup-based analytical approach was developed in 2014 - 2017. This approach could theoretically be used for other species that occur in subgroups. We cover this on a separate page. Other scenarios Study periods that span years, such as a December - January survey. This is not yet handled well within the LTabundR framework. More will go here. Behind the scenes Area estimation Unless you manually specify the study area in your estimates list, lta() will calculate your study area for you based on the geostrata you provide. It does so by calling the LTabundR function strata_area(), which you can use on your own to explore geostratum combination options. This function was designed using the sf package to handle complex polygon combinations, and it uses Natural Earth datasets to remove land within your study area (this is a feature you can turn off, if you want). Here are some examples of how strata_area() handles complex scenarios. Say you want to estimate abundance in the ‘WHCEAS’ study area, but you want to make sure the study area estimate is accurately removing land: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;WHICEAS&#39;), verbose = FALSE) Say you want to estimate abundance in the pelagic Hawaiian EEZ, ignoring effort and sightings within the Main Hawaiian Islands stratum and accurately removing small islands in northwest Hawaii: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;HI_EEZ&#39;), strata_remove = c(&#39;MHI&#39;), verbose = FALSE) Say you want to estimate abundance of pelagic bottlenose dolphins within the WHICEAS study area, ignoring the insular stocks: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;WHICEAS&#39;), strata_remove = c(&#39;Bottlenose_KaNi&#39;,&#39;Bottlenose_OUFI&#39;,&#39;Bottlenose_BI&#39;), verbose = FALSE) Say you want to estimate abundance for only the insular bottlenose stocks: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;Bottlenose_KaNi&#39;,&#39;Bottlenose_OUFI&#39;,&#39;Bottlenose_BI&#39;), verbose = FALSE) Say you want to estimate abundance for false killer whales within the Northwest Hawaiian Islands and Main Hawaiian Islands study areas combined, but those geostrata partially overlap: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;MHI&#39;,&#39;NWHI&#39;), verbose = FALSE) Say you want to estimate abundance for the Hawaiian EEZ outside of those partially overlapping geostrata: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = &#39;HI_EEZ&#39;, strata_remove = c(&#39;MHI&#39;,&#39;NWHI&#39;), verbose = FALSE) g(0) estimation If you want lta() to calculate a weighted g(0) estimate (and associated CV) that is specific to the conditions associated with your estimates sub-list parameters, all you need to do is provide the Rg0 input. When you do this, the lta() function will find the Rg0 values associated with the species code(s) in your estimates sub-list, then calculate weighted g(0) and its CV using the LTabundR function, g0_weighted(), which we discussed and demonstrated on the previous page. If lta() can’t find your species code in the Rg0 table you provide, it will give up and assume that g(0) is 1.0 and that g0_cv is 0.0. If your estimates sub-list has a alt_g0_spp slot, lta() will use that species code instead to filter the Rg0 table. If your estimates sub-list has a combine_g0 slot that is TRUE, lta() will filter the Rg0 table using all species codes you provide. If that filtration results in multiple Rg0 species being found, weighted g(0) will be calculated for each of those species separately, then those g(0) estimates will be combined using a geometric mean (using the LTabundR function g0_combine()). If combine_g0 is FALSE, only the first species code provided in your estimates sub-list will be used to filter Rg0. If you want to supply a weighted g(0) estimate and its CV yourself, you can add the g0 and g0_cv slots to your estlimates sublist, as explained above. If you want to coerce g(0) to be assumed to be 1.0 (with CV = 0.0), you can either (1) not supply the Rg0 input, or (2) manually specify the g0 and g0_cv slots in your estimates sub-list accordingly. Covariates in detection function estimation Before detection functions are modelled, any covariates supplied by the user and specified as a factor are first tested for eligibility. Only factors with at least two levels (or whatever you specified with df_settings$covariates_levels) and 10 observations in each level (or whatever you specified with df_settings$covariates_n_per_level) are eligible for inclusion. Fitting a detection function The detection function is estimated using functions in the package mrds, primarily the main function mrds::ddf(), which uses a Horvitz-Thompson-like estimator to predict the probability of detection for each sighting. If multiple base key functions (e.g., half-normal or hazard-rate) are provided, and/or if covariates are specified, model fitting is done in a forward stepwise procedure: In the first round, the base model (no covariates, i.e., \"~1\") is fit first. In the second round, each covariate is added one at a time; at the end of the round, the covariate, if any, that produces the lowest AIC below the AIC from the previous round is added to the formula. This process is repeated in subsequent rounds, adding a new covariate term in each round, until the AIC no longer improves. If a second base key is provided, the process is repeated for that second key. All models within delta_aic of the model with the lowest AIC qualify as best-fitting models. The best-fitting model(s) is(are) then used to estimate the Effective Strip half-Width (ESW) based on the covariates associated with each sighting. If multiple best-fitting models occur, we will find the average ESW for each sighting across all models, using a weighted mean approach in which we weight according to model AIC. To turn off this model averaging step, set delta_aic to 0 to avoid passing multiple models to the abundance estimation stage. This stage of the lta() command is executed within a backend function, LTabundR::fit_df(), which has its own documentation for your reference. Estimating density &amp; abundance Estimates are produced for various combinations of species, regions, and years, according to the arguments specified in your estimates list(s). Before these estimates are produced, we filter the data used to fit the detection function to strictly systematic (design-based) effort (i.e., EffType = \"S\"), in which standard protocols are in use (i.e., OnEffort = TRUE) and the Beaufort sea state is less than 7 (though these controls can be modified using the lta() inputs abund_eff_types and abund_bft_range (see above). This stage of the lta() command is executed within a back-end function, LTabundR::abundance(), which has its own documentation for your reference. Bootstrap variance estimation If the bootstraps input value is greater than 1, bootstrap variance estimation will be attempted. In each bootstrap iteration, survey segments are re-sampled with replacement before fitting the detection function and estimating density/abundance. Re-sampling is done in a routine that preserves the proportion of segments from each geostratum. Note that the entire process is repeated in each bootstrap: step-wise fitting of the detection function, averaging of the best-fitting models, and density/abundance estimation for all species/region/year combinations specified in your estimates input. At the end of the bootstrap process, results are summarized for each species/region/year combination. 95% confidence intervals are calculated using the BCA method (package coxed, function bca()). g(0) values during bootstrapping When conducting the non-parametric bootstrap routine to estimate the CV of density and abundance, uncertainty is incorporated into the g(0) value in each iteration using a parametric bootstrapping subroutine: First, a logit-transformed distribution is modeled based upon the mean and CV of g(0) provided by the user in the estimates input (see documentation for LTabundR::g0_optimize() for details on this step). This modeled distribution is used to randomly draw a g(0) value for each iteration of the density/abundance bootstrap routine. In this way, the uncertainty in g(0) is propagated into uncertainty in density/abundance. "],["destratify.html", " 9 Stratified analysis", " 9 Stratified analysis Field surveys are sometimes stratified such that trackline design and/or density can differ substantially across regions. Also, analysts may sometimes wish to estimate density/abundance for individual regions separately, regardless of design stratification. On the previous page, we demonstrated how to accommodate a stratified analysis by providing an estimates sub-list for each geostratum. For example, in 2002 the Hawaiian EEZ was surveyed with different effort intensity in the Main Hawaiian Islands region compared to pelagic waters. Here is the code that generates density/abundance estimates of striped dolphins in 2002 (stratified) and 2010 (unstratified), with only 10 bootstrap iterations: # Survey data data(&quot;cnp_150km_1986_2020&quot;) cruz &lt;- cnp_150km_1986_2020 # Rg0 table data(&quot;g0_results&quot;) Rg0 &lt;- g0_results # Detection function filters fit_filters &lt;- list(spp = c(&#39;013&#39;, &#39;026&#39;, &#39;031&#39;), pool = &#39;Multi-species pool 1&#39;, cohort = &#39;all&#39;, truncation_distance = 5, other_species = &#39;remove&#39;) # Detection function settings df_settings &lt;- list(covariates = c(&#39;bft&#39;,&#39;lnsstot&#39;,&#39;cruise&#39;,&#39;year&#39;,&#39;ship&#39;,&#39;species&#39;), covariates_factor = c(FALSE, FALSE, TRUE, TRUE, TRUE, TRUE), covariates_levels = 2, covariates_n_per_level = 10, detection_function_base = &#39;hn&#39;, base_model = &#39;~1&#39;, delta_aic = 2) # Estimates estimates &lt;- list( list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2002, regions = &#39;MHI&#39;), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2002, regions = &#39;HI_EEZ&#39;, regions_remove = &#39;MHI&#39;), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2010, regions = &#39;HI_EEZ&#39;)) # Run it results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, bootstraps = 10) # Save it locally saveRDS(results, file=&#39;lta/multispecies_pool_1.RData&#39;) Let’s read these results back in using the LTabundR function lta_enlist(), which stores LTA results in a flexible list structure. ltas &lt;- lta_enlist(&#39;lta/&#39;) As these results stand, 2002 estimates are stratified into 2 separate regions: (ltas %&gt;% lta_report(verbose = FALSE))$table4 2002 (HI_EEZ) - (MHI) 2002 1 Species or category Density Abundance CV 95% CI Density 2 Striped dolphin 14.2 32,135 0.47 16,707-64,184 12.06 (MHI) 2010 (HI_EEZ) 1 Abundance CV 95% CI Density Abundance CV 95% CI 2 2,557 0.89 0-7,491 23.86 59,051 0.42 45,459-131,350 Now let’s process these LTA results through an LTabundR function, lta_destratify(), which will combine the separate regional estimates from 2002 into a single estimate for the year. ltas_2a &lt;- lta_destratify(ltas, years = 2002, combine_method = &#39;arithmetic&#39;, new_region = &#39;(HI_EEZ)&#39;) The new_region argument specifies how to refer to the combined region. In this case we want the 2002 study area to be named the same as the unstratified 2010 study area, hence \"(HI_EEZ)\". The combine_method argument is explained below. Now let’s re-check the summary table: (ltas_2a %&gt;% lta_report(verbose=FALSE))$table4 2002 (HI_EEZ) 2010 (HI_EEZ) 1 Species or category Density Abundance CV 95% CI Density Abundance 2 Striped dolphin 14.02 34,691 0.82 8,462-142,221 23.86 59,051 1 CV 95% CI 2 0.42 45,459-131,350 There is now only one set of columns for 2002, and the values therein are combinations of the stratified regions. The “de-stratification” routine within lta_destratify() sums abundance estimates across regions to get combined abundance. To estimate density in the combined regions, the function uses weighted averaging in which the area of a geostratum serves as its weight. To estimate CV and confidence interval of the combined result, the function uses one of two combine_methods (this is an argument in the function). The default method is \"arithmetic\", which uses classic formulae to estimate the combined variance and the corresponding confidence interval. This is done in a way that allows multiple geostrata to be combined, not just two. The second option, \"bootstrap\", uses an iterative method that draws bootstrap samples, with replacement, from the bootstrap estimate of density within each stratified region. ltas_2b &lt;- lta_destratify(ltas, years = 2002, combine_method = &#39;bootstrap&#39;, new_region = &#39;(HI_EEZ)&#39;) (ltas_2b %&gt;% lta_report(verbose=FALSE))$table4 2002 (HI_EEZ) 2010 (HI_EEZ) 1 Species or category Density Abundance CV 95% CI Density Abundance 2 Striped dolphin 14.02 34,691 0.4 17,882-75,136 23.86 59,051 1 CV 95% CI 2 0.42 45,459-131,350 Note that use of lta_destratify() only makes sense if the stratified regions have zero overlap. "],["trend-analysis.html", " 10 Trend analysis", " 10 Trend analysis When a species’ density appears to change dramatically from one survey year to the next, it could be due to several factors: the species’ abundance may have changed; its range may have shifted; or the timing of its migratory movements may have shifted. This apparent change could also be due solely to random chance: you can sample the exact same population in two different surveys, and you are liable to produce different abundance estimates due simply to random variation in how often you encounter your target species. In other words, random variation in the encounter rate may lead you to estimate a change in abundance, when in fact there is no change. For this reason, whenever you suspect that abundance has changed between years – i.e., whenever the confidence intervals for two years do not overlap – it is good practice to carry out follow-up tests. One such test was developed in (Bradford et al. 2020 and Bradford et al. (2021). That test has been provided in LTabundR with the function er_simulator(), which refers to a simulation-based test of random variation in the encounter rate (ER). This function uses randomization simulations to test for the probability that year-to-year changes observed in a species’ encounter rate are due to random sampling variation (and not actual change in the encounter rate). More specifcially, this function uses bootstrap sampling of survey segments to see if random variation in sampling could possibly produce an apparent but immaterial change in encounter rate across years. You will find full analytical details in the Appendix to Bradford et al. (2020) for analytical details, but briefly: in each bootstrap iteration, survey segments are resampled in a way that preserves the proportion of effort occurring within each geostratum in the data. The resampled data are used to calculate the overall ER across all survey years, since the null hypothesis is that the ER does not change across years. This overall ER is used to predict the number of sightings in each year, based on the distance covered by the resampled segments in each year. This process is repeated (typically hundreds to thousands of times) to produce a distribution of predicted sighting counts in each year. This distribution reflects the range of ERs that could be possible due simply to random variation and not to underlying changes in abundance. These distributions are compared to the actual number of sightings observed in their respective year. The fraction of simulated sightings counts that are more extreme than the observed count reflects the probability that the observed count is due to random sample variation alone. For example, Bradford et al. (2021) found non-overlapping confidence intervals in their estimates of Bryde’s whale abundance in 2002, 2010, and 2017. To test for the significance of these trends, they carried out the “ER simulator” routine described above. In LTabundR, we would carry out the same analysis as follows: Take your processed data: data(cnp_150km_1986_2020) cruz &lt;- cnp_150km_1986_2020 Filter it to systematic effort in the years of interest: cruzi &lt;- filter_cruz(cruz, analysis_only = TRUE, years = c(2002, 2010, 2017), regions = &#39;HI_EEZ&#39;, eff_types = &#39;S&#39;, bft_range = 0:6) Conduct the ER simulation, passing the species code for the Bryde’s whale (\"072\"). For the purposes of this example, we will only use 100 iterations. er_results &lt;- er_simulator(spp = &#39;072&#39;, cruz = cruzi, iterations = 100) This routine provides a list with two slots: er_results %&gt;% names [1] &quot;summary&quot; &quot;details&quot; The summary slot returns the p-value for each year, i.e., the chances that the observed number of sightings was due purely to random variation in the encounter rate. er_results$summary years observed p 1 2002 171 1.0 2 2010 310 0.0 3 2017 241 0.8 In this example, the encounter rates observed in 2002 and 2010 are very likely due to some process other than random variation in the encounter rate, such as range shifts, seasonal movement timing shifts, and/or changes in abundance. However, the observed encounter rate in 2017 could easily be explained by random variation in the ER. The details slot returns the simulation predictions for each year, which can be used to replicate the histograms that are printed when the function is run. A dataframe with a row for each year. Columns provide the number of observations of the species of interest during systematic effort, and the p-value of the test. The p-value represents the fraction of simulated encounter rates that exceed the observed encounter rate. "],["subgroup-based-analysis.html", " 11 Subgroup-based analysis Inputs Outputs Behind the scenes", " 11 Subgroup-based analysis False killer whales (Pseudorca crassidens) are rare and occur in dispersed subgroups, which complicates conventional distance sampling approaches to line-transect analysis. To better estimate their abundance, in 2010 the Pacific Islands Fisheries Science Center (PIFSC) initiated a sub-group protocol referred to as the “PC Protocol”, a reference to the species’ scientific name. To conduct line-transect analysis with this sub-group-based protocol, a method was developed in 2014 - 2017 To handle this, a separate, subgroup-based analytical approach was developed in 2014 - 2017, then updated in 2020 (Bradford et al. 2020). An additional complication is that false killer whales in Hawaiian waters belong to two discrete populations – the Northwest Hawaiian Islands (NWHI) population and a pelagic population – whose ranges partially overlap, which means that population assignment cannot always be based simply on the geographic location of sightings. When geographic inference of population is not possible, biopsy-sampled genetics, photo-identification, and acoustics are used to assign each sighting to a population post-hoc. To accommodate these special circumstances with an appropriate balance of flexibility and efficiency, LTabundR includes a function named lta_subgroup(), whose use will look something like this: lta_subgroup(df_sits, truncation_distance, ss, cruz10, g0_spp, g0_truncation, g0_pool_bft, g0_jackknife_fraction = 0.1, density_segments, density_das, density_sightings, abundance_area, iterations = 1000, output_dir = &#39;../test_code/subgroup/&#39;, toplot = TRUE, verbose = TRUE, density_bootstraps = 10000) We will step through each of these inputs below, using a case study in which we estimate false killer whale abundance in the Hawaiian EEZ for 2017. Inputs df_sits This is a data.frame of sightings you want to use to fit the detection function model. For false killer whales in Bradford et al. (2020), this is a combination of systematic sightings prior to 2010 and Phase 1 sightings from 2010 onwards (using the PC protocol). No filtering will be applied to these sightings within this function, so make sure you provide the data pre-filtered. Bradford et al. (2020) used a single detection function for all populations of false killer whale. LTabundR has a built-in dataset for processed Central North Pacific surveys, 1986-2020, using 150-km segments. We will use that here: data(&quot;cnp_150km_1986_2020&quot;) cruz &lt;- cnp_150km_1986_2020 The code used to generate this dataset can be seen by pulling up the help documentation: ?noaa_10km_1986_2020. As mentioned above, for 1986 - 2010, all detections are assumed to be ‘Phase 1’ sightings, and therefore usable in detection function estimation. Here we draw those sightings from the above cruz object, filtering as needed (the species code for false killer whales is \"033\"), and to simplify we will select only a few key columns. sits1 &lt;- cruz$cohorts$all$sightings %&gt;% filter(OnEffort == TRUE, year &lt; 2011, Lat &gt;= 5, Lat &lt;= 40, Lon &gt;= -185, Lon &lt;= -120, species == &#39;033&#39;, mixed == FALSE) %&gt;% select(DateTime, Lat, Lon, Cruise, PerpDistKm) sits1 %&gt;% nrow [1] 33 sits1 %&gt;% head DateTime Lat Lon Cruise PerpDistKm 1 1986-11-13 09:43:00 10.466667 -139.2833 990 1.17493742 2 1987-08-19 15:30:00 12.050000 -133.3000 1080 2.24543067 3 1987-12-01 09:23:00 8.266667 -122.5500 1080 0.42589077 4 1989-08-22 06:45:00 11.800000 -141.7333 1268 0.40838431 5 1989-08-22 16:39:00 12.716667 -143.1000 1268 0.68815211 6 1989-09-10 17:18:00 7.350000 -129.5333 1268 0.07751339 For 2011 onwards, we will use Phase 1 subgroup detections from the PC protocol, making sure that the column holding detection distances is named PerpDistKm: sits2 &lt;- cruz$cohorts$all$subgroups$subgroups %&gt;% filter(OnEffort == TRUE, lubridate::year(DateTime) &gt;= 2011, Lat &gt;= 5, Lat &lt;= 40, Lon &gt;= -185, Lon &lt;= -120, Species == &#39;033&#39;, Phase == 1) %&gt;% select(DateTime, Lat, Lon, Cruise, PerpDistKm = PerpDist) sits2 %&gt;% nrow [1] 53 sits1 %&gt;% head DateTime Lat Lon Cruise PerpDistKm 1 1986-11-13 09:43:00 10.466667 -139.2833 990 1.17493742 2 1987-08-19 15:30:00 12.050000 -133.3000 1080 2.24543067 3 1987-12-01 09:23:00 8.266667 -122.5500 1080 0.42589077 4 1989-08-22 06:45:00 11.800000 -141.7333 1268 0.40838431 5 1989-08-22 16:39:00 12.716667 -143.1000 1268 0.68815211 6 1989-09-10 17:18:00 7.350000 -129.5333 1268 0.07751339 To create df_sits for detection function fitting, we combine these datasets together: df_sits &lt;- rbind(sits1, sits2) df_sits %&gt;% nrow [1] 86 truncation_distance The truncation distance, in km, will be applied during detection function model fitting. Typically the farthest 5 - 10% of sightings are truncated, but this needs to be balanced by sample size considerations. Get candidate distances: truncation_options &lt;- quantile(df_sits$PerpDistKm, c(0.90,0.91,0.92,0.93,0.94,.95)) truncation_options 90% 91% 92% 93% 94% 95% 4.249637 4.350266 4.403387 4.486396 4.881672 5.217843 Plot these options: hist(df_sits$PerpDistKm, main=&#39;Detection distances&#39;) abline(v=truncation_options, col=&#39;red&#39;, lty=3) Get sample size remaining for each candidate distance: data.frame(km = truncation_options, n = sapply(truncation_options, function(x){length(which(df_sits$PerpDistKm &lt;= x))})) km n 90% 4.249637 77 91% 4.350266 78 92% 4.403387 79 93% 4.486396 80 94% 4.881672 80 95% 5.217843 81 Based on these results, we will choose a truncation distance of 4.5 km. truncation_distance &lt;- 4.5 ss This is a numeric vector of subgroup school sizes. The function will find this vector’s geometric mean and bootstrapped CV. In Bradford et al. (2020), school size data come from all Phase 1 and Phase 2 estimates of subgroup sizes from 2010 onwards. In the processed cruz object, each of those estimates is the geometric mean of repeat estimates from separate observers. ss &lt;- cruz$cohort$all$subgroups$subgroups %&gt;% filter(lubridate::year(DateTime) &gt;= 2011, Lat &gt;= 5, Lat &lt;= 40, Lon &gt;= -185, Lon &lt;= -120, Species == &#39;033&#39;) %&gt;% pull(GSBest_geom) ss %&gt;% length [1] 183 ss [1] 5.00 7.00 1.00 1.00 1.00 1.33 2.00 2.75 2.67 1.00 6.00 1.00 [13] 2.00 18.50 1.00 4.00 5.00 2.00 1.00 4.00 4.50 2.50 2.00 2.00 [25] 1.00 1.00 5.00 1.00 4.00 1.00 1.00 1.00 1.00 1.00 1.00 2.00 [37] 2.00 1.00 4.00 9.83 2.50 4.75 8.25 3.00 2.00 3.00 3.67 3.50 [49] 4.50 2.00 1.00 1.00 2.00 1.00 1.00 1.00 1.00 1.00 2.00 1.00 [61] 1.00 1.00 2.00 3.00 2.00 1.50 2.67 1.33 2.00 6.00 4.33 2.00 [73] 1.00 3.00 5.00 5.80 4.00 1.00 2.00 2.00 4.00 4.50 2.00 2.00 [85] 2.00 4.50 2.00 1.00 2.00 1.00 1.00 2.00 2.00 1.50 4.50 1.00 [97] 1.00 3.33 3.00 2.00 2.00 7.50 5.00 4.00 3.67 2.00 3.00 2.00 [109] 8.50 8.00 4.00 2.00 1.00 1.00 4.00 4.00 2.00 1.00 2.00 6.00 [121] 2.00 2.00 2.50 1.00 1.00 1.00 1.00 1.00 2.00 2.00 1.00 1.00 [133] 1.00 1.00 1.00 1.00 1.00 2.00 1.00 2.00 2.00 2.00 1.00 4.00 [145] 7.50 1.50 2.00 1.00 1.00 1.00 1.00 2.00 1.00 2.00 3.00 3.00 [157] 2.00 3.00 1.50 1.00 1.00 3.00 2.00 4.00 1.00 1.00 1.00 2.00 [169] 2.00 4.00 2.00 2.00 1.00 1.00 3.00 1.00 3.00 2.50 2.00 2.00 [181] 1.00 1.00 1.00 cruz10 This is a processed cruz object with short segment lengths, ideally 10 km or less (hence the 10 in the input name). This cruz object will be used to estimate Rg(0), i.e., the relative trackline detection probability (see its chapter). LTabundR comes with a built-in dataset we can use for this purpose: data(&quot;noaa_10km_1986_2020&quot;) cruz10 &lt;- noaa_10km_1986_2020 The code used to generate this dataset can be seen by pulling up the help documentation: ?noaa_10km_1986_2020. g0_spp This is a character vector of species code(s) to use to estimate Rg(0). In most cases this will be a single species, e.g., \"033\" for false killer whales. g0_spp &lt;- &#39;033&#39; g0_truncation The truncation distance to use when estimating Rg(0). In Bradford et al. (2020) this is 5.5 km. g0_truncation &lt;- 5.5 g0_pool_bft A way to specify that low Beaufort sea states, which are typically rare in open-ocean surveys, should be pooled. This step may be needed in order to achieve a monotonic decline in the g(0) ~ Bft relationship, but the default is NULL, i.e., no pooling. If g0_pool_bft is the character string \"01\", Beaufort states 1 will be pooled into state 0. If g0_pool_bft is the character string \"012\", Beaufort states 1 and 2 will be pooled into state 0. g0_pool_bft = NULL g0_jackknife_fraction The proportion of data to leave out within each jackknife permutation. The default is 0.1 (i.e., 10% of the data, yielding 10 jackknife loops), after Barlow (2015). g0_jackknife_fraction = 0.1 density_segments The survey segments to be used in density/abundance estimation. For example, Bradford et al. (2020) used 150-km segments to estimate false killer whale density in the Hawaiian EEZ in 2017. For this we can use the 1986-2020 dataset we loaded above. Note that no filtering will be applied to these segments by the lta_subgroup() function, so w need to filter them ourselves first: we want only systematic segments for the Hawaiian EEZ in 2017 (specially, just cruises 1705 and 1706). cruzi &lt;- filter_cruz(cruz = cruz, analysis_only = TRUE, years = 2017, cruises = c(1705, 1706), regions = &#39;HI_EEZ&#39;, bft_range = 0:6, eff_types = &#39;S&#39;, on_off = TRUE) From this filtered cruz object, we will isolate the segments data: density_segments &lt;- cruzi$cohorts$all$segments Since we do not want to stratify our analysis by smaller geostrata, such as the Main Hawaiian Islands, we will go ahead and coerce all stratum assignments to the Hawaiian EEZ geostratum: density_segments$stratum &lt;- &#39;HI_EEZ&#39; density_das This is the complete survey data corresponding to the above segments. These data will be used to determine the proportion of survey effort occurring in each Beaufort sea state during estimation of Relative g(0). density_das &lt;- cruzi$cohorts$all$das density_sightings These are the encounters to use in density/abundance estimation. In Bradford et al. (20120), these were the Phase 1 detections of false killer whale subgroups within the population-region-year of interest, e.g., Northwest Hawaiian Island population sightings within the Hawaiian EEZ in 2017. No filtering is applied to these sightings within lta_subgroups(), so make sure only the sightings you wish to use are included and nothing more. In this example, since we do not have population information on hand, we will not filter detections to a specific population. Instead, we will estimate abundance of all false killer whales within the Hawaiian EEZ: density_sightings &lt;- cruz$cohorts$all$subgroups$subgroups %&gt;% filter(EffType == &#39;S&#39;, OnEffort == TRUE, lubridate::year(DateTime) == 2017, PerpDist &lt;= truncation_distance, Species == &#39;033&#39;, Phase == 1) density_sightings %&gt;% nrow [1] 32 density_sightings %&gt;% head Cruise Date DateTime Lat Lon OnEffort Bft EffType 1 1705 2017-09-21 2017-09-21 14:17:27 20.04283 -161.9177 TRUE 5 S 2 1705 2017-09-21 2017-09-21 14:39:40 20.06017 -161.9840 TRUE 5 S 3 1705 2017-09-29 2017-09-29 13:22:08 23.55750 -175.8527 TRUE 1 S 4 1705 2017-09-29 2017-09-29 13:26:05 23.56050 -175.8643 TRUE 1 S 5 1705 2017-09-29 2017-09-29 13:26:43 23.56083 -175.8662 TRUE 1 S 6 1705 2017-09-29 2017-09-29 13:32:33 23.56517 -175.8832 TRUE 1 S SightNo Species SubGrp Angle RadDist seg_id PerpDist GSBest GSH GSL 1 064 033 A 14 5.0374102 1602 1.2186598 3.00 4 2.00 2 064 033 B 24 3.1483814 1602 1.2805621 3.67 6 2.33 3 073 033 A 49 3.3706200 1610 2.5438392 2.00 3 1.00 4 073 033 B 68 0.6852359 1610 0.6353397 1.00 2 1.00 5 073 033 C 40 0.3703978 1610 0.2380871 1.00 1 1.00 6 073 033 E 89 2.9817023 1610 2.9812482 1.00 2 1.00 GSBest_geom GSH_geom GSL_geom use stratum_HI_EEZ stratum_OtherCNP 1 3.00 4 2.00 TRUE TRUE TRUE 2 3.67 6 2.33 TRUE TRUE TRUE 3 2.00 3 1.00 TRUE TRUE TRUE 4 1.00 2 1.00 TRUE TRUE TRUE 5 1.00 1 1.00 TRUE TRUE TRUE 6 1.00 2 1.00 TRUE TRUE TRUE stratum_MHI stratum Phase 1 FALSE HI_EEZ 1 2 FALSE HI_EEZ 1 3 FALSE HI_EEZ 1 4 FALSE HI_EEZ 1 5 FALSE HI_EEZ 1 6 FALSE HI_EEZ 1 As above, let’s make sure the geostratum assignments for these sightings are simple: density_segments$stratum &lt;- &#39;HI_EEZ&#39; abundance_area This is the area, in square km, of the region of interest. The density estimate will be scaled by this area. We have two options for finding this area. The first is to draw the area from our cohort$strata slot: cruz$strata$area[cruz$strata$stratum == &#39;HI_EEZ&#39;] [1] 2474596 The second is to calculate it ourselves using the LTabundR function strata_area(). This second option will be useful if your study area is a complicated combination/substraction of multiple geostrata. data(strata_cnp) abundance_area &lt;- strata_area(strata_all = strata_cnp, strata_keep = &#39;HI_EEZ&#39;)$km2 abundance_area [1] 2474596 Remaining inputs iterations: Number of iterations to use in the various CV bootstrapping procedures occurring throughout this function, specifically: Effective Strip Half-Width CV estimation, school size CV estimation, weighted g(0) CV estimation, and encounter rate estimation. output_dir: The path in which results RData files should be stored. If left \"\", the current working directory will be used. toplot: A Boolean, with default FALSE, indicating whether to plot various aspects of the analysis. verbose: A Boolean, with default TRUE, indicating whether to print status updates to the Console. density_bootstraps: Number of bootstrap iterations to use for the CV estimate of density and abundance specifically. This input allows this final step to use a different (typically larger) iteration size than the iterations input above. save(results_subgroup, file=&#39;results_subgroup.RData&#39;) load(&#39;results_subgroup.RData&#39;) Outputs The function returns a list with many slots, including estimates of density and abundance – along with estimates of intermediate parameters – with a CV derived from a bootstrapping routine. To demonstrate this output, we will use results from a call with only 10 bootstrap iterations. results_subgroup %&gt;% names [1] &quot;D&quot; &quot;D_CV&quot; &quot;D_L95&quot; [4] &quot;D_U95&quot; &quot;N&quot; &quot;N_CV&quot; [7] &quot;N_L95&quot; &quot;N_U95&quot; &quot;ER&quot; [10] &quot;ss&quot; &quot;n&quot; &quot;L&quot; [13] &quot;n_segments&quot; &quot;g0&quot; &quot;g0_cv&quot; [16] &quot;g0_details&quot; &quot;df&quot; &quot;bootstraps&quot; [19] &quot;iterations&quot; &quot;density_bootstraps&quot; Most of these slots hold best-estimates of parameters or sample size details: results_subgroup[c(1:15, 19:20)] $D 1 0.003101353 $D_CV 1 0.8798567 $D_L95 [1] 0.00155031 $D_U95 [1] 0.0159308 $N [1] 7675 $N_CV [1] 0.8798095 $N_L95 [1] 3836 $N_U95 [1] 39422.27 $ER [1] 0.001852608 $ss [1] 2.466557 $n [1] 32 $L [1] 17272.95 $n_segments [1] 137 $g0 [1] 0.305 $g0_cv [1] 0.447 $iterations [1] 10 $density_bootstraps [1] 10000 The g0_details slot includes the results from the g0_model() and g0_weighted() functions called internally by lta_subgroup(). See those functions’ documentation pages for details. results_subgroup$g0_details %&gt;% names [1] &quot;Rg0&quot; &quot;gam&quot; &quot;jackknife&quot; &quot;summary&quot; &quot;sightings&quot; &quot;segments&quot; The df slot includes details of the detection function fit. See the documentation for df_plot() for details. results_subgroup$df %&gt;% names [1] &quot;best_models&quot; &quot;all_models&quot; &quot;best_objects&quot; &quot;sightings&quot; The bootstraps slot has the bootstrapped values for various parameters, in case they are useful for troubleshooting, subsequent analyses, and/or plotting: results_subgroup$bootstraps %&gt;% names [1] &quot;esw&quot; &quot;ss&quot; &quot;g0&quot; &quot;er&quot; &quot;D&quot; &quot;N&quot; Some examples: Behind the scenes This function performs the following operations: Fits a detection function to df_sits without covariates, using the LTabundR function df_fit(), in order to estimate the effective strip half-width (ESW). Conducts bootstrap re-sampling of the detection function fitting routine in order to estimate the CV of ESW. Estimates the geometric mean of subgroup school size based on the ss input. Creates a bootstrap-resampled distribution of subgroup school sizes, with which CV is estimated. Models the Relative g(0) in different survey conditions using the LTabundR function g0_model(). This function also estimates the CV of the Rg(0) estimate in each Beaufort sea state using jackknife resampling. Estimates the encounter rate (subgroup detections / trackline surveyed). Creates a bootstrap-resampled distribution of encounter rate estimates. Calculates a weighted g(0) estimate according to the proportion of effort occurring in each Beaufort sea state, then uses an automated parameter optimization routine (see details in LTabundR function g0_weighted()) to estimate the CV of the weighted g(0) estimate. Creates a bootstrap-resampled distribution of the weighted g(0) estimate. Estimates density using the best estimates of effective strip half-width, school size, g(0), and the encounter rate. Estimates abundance by scaling the density estimate by the provided abundance_area. Creates a bootstrap-resampled distribution of the density estimate by iteratively drawing values from the resampled distributions of the constituent parameters of the density equation. Creates a bootstrap-resampled distribtion of the abundance estimate by scaling the density distribution by abundance_area. Note that this approach could theoretically be used for other species that occur in subgroups. "],["tables.html", " 12 Tables Summary tables Appendix tables", " 12 Tables To demonstrate how LTA results can be summarized, tabularized, and plotted, we will use a built-in LTabundR dataset, which has denisty/abundance estimates for the Hawaiian EEZ in 2010 and 2017 for striped dolphins, Fraser’s dolphins, and melon-headed whales, ran with only 20 iterations: data(lta_result) We created these LTA results using the following built-in dataset: data(cnp_150km_1986_2020) Summary tables To summarize lta() results using the standard table format provided in recent NOAA stock assessment reports, use the function lta_report(). tables &lt;- lta_report(lta_result, cruz = cnp_150km_1986_2020, verbose = TRUE) Providing the cruz object is not required, but if it is not provided, one of the five summary tables ($table1a below) will not be returned. tables %&gt;% names [1] &quot;table1a&quot; &quot;table1b&quot; &quot;table2&quot; &quot;table3&quot; &quot;table4&quot; &quot;tableA1&quot; &quot;tableA2&quot; Table 1 in reports: Sample sizes Table 1a If cruz was provided, $table1a 1a will include total sighting counts for all species in the years from lta_result, broken down by region. The Ntot column holds all sightings, regardless of effort status or Beaufort sea state. Nsys holds counts of systematic-only sightings (i.e., EffType = “S” and Bft &lt;= 6), which may still include sightings that are beyond the species-specific truncation distance and were therefore excluded from density/abundance estimation. These counts are provided separately from the $table1b slot below, since those counts are based on the lta_result object, and will not include sightings for species that did not have a specific LTA estimate specified when it was made. We also include this separately so as to give the user full flexibility in how they summarize sighting counts by region/population/stock. tables$table1a %&gt;% DT::datatable(options=list(initComplete = htmlwidgets::JS( &quot;function(settings, json) {$(this.api().table().container()).css({&#39;font-size&#39;: &#39;9pt&#39;});}&quot;) )) Table 1b This table holds sighting counts used in estimates of density/abundance. The rows match the rows for Tables 3 and 4. In this table, columns are still prepared for total sightings (Ntot) and systematic sightings (Nsys), but they are left blank, since it is not clear how sightings from multiple regions in $table1a would be concatenated for this table. The user can fill in those gaps accordingly. tables$table1b %&gt;% DT::datatable(options=list(initComplete = htmlwidgets::JS( &quot;function(settings, json) {$(this.api().table().container()).css({&#39;font-size&#39;: &#39;9pt&#39;});}&quot;) )) Table 2 in reports: Detection functions Table 3: Parameter estimates Table 4: Density/abundance Appendix tables Table A1: Study areas Table A2: Effort totals (parsed by Beaufort sea state) tables$tableA2 $`2010` # A tibble: 1 × 10 Species Stratum Effort B0 B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 all HI_EEZ 19882 0.00101 0.0208 0.0539 0.130 0.469 0.283 0.0413 $`2017` # A tibble: 1 × 10 Species Stratum Effort B0 B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 all HI_EEZ 18843 0.000700 0.00883 0.0384 0.119 0.313 0.341 0.179 "],["plots.html", " 13 Plots Abundance plots Detection function plots", " 13 Plots Abundance plots To plot the abundance estimate with error bars representing the 95% confidence interval, use the function lta_plot(): lta_plot(lta_result, nrows=1) Detection function plots To plot the best-fit detection model for a species pool, use the df_plot() function. df_plot(lta_result) This function provides various stylization options, including the option to show multiple best-fitting models atop a single histogram of detections: df_plot(lta_result, model_colors=RColorBrewer::brewer.pal(n = 4, name = &quot;Dark2&quot;), model_pch = 16, pt_show=2, pt_alpha=.3, bootstrap_show = FALSE, legend_show=TRUE, legend_x=2.8) "],["whiceas.html", " 14 WHICEAS Data processing Data exploration Rg0 Density &amp; abundance Results", " 14 WHICEAS Here we demonstrate code that reproduces the Bradford et al. (2022) WHICEAS report (In press) within the new LTabundR framework. This study estimates cetacean abundance for Hawaiian WHICEAS study area for 2017 and 2020. Here we use survey data from 1986 to 2020 to estimate Relative g(0) and detection functions. Currently, coefficients of variation (CV) of density and abundance are estimated using only 100 bootstrap iterations (the publication uses 1,000) to reduce processing time. library(dplyr) library(LTabundR) Data processing Settings Survey-wide settings data(species_codes) data(ships) data(group_size_coefficients) survey &lt;- load_survey_settings( out_handling = &#39;remove&#39;, max_row_interval = Inf, segment_method = &quot;equallength&quot;, segment_target_km = 150, segment_max_interval = 24, segment_remainder_handling = c(&quot;segment&quot;), ship_list = ships, species_codes = species_codes, group_size_coefficients = group_size_coefficients, smear_angles = FALSE ) Geostrata data(strata_cnp) Cohort-specific settings Cohort 1: all species all_species &lt;- load_cohort_settings( id = &quot;all&quot;, # * species = NULL, strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;), # * probable_species = FALSE, sighting_method = 0, cue_range = 0:7, school_size_range = c(0, 10000), school_size_calibrate = TRUE, calibration_floor = 0, use_low_if_na = TRUE, io_sightings = 0, geometric_mean_group = TRUE, truncation_km = 7.5, # * beaufort_range = 0:6, abeam_sightings = TRUE, strata_overlap_handling = c(&quot;smallest&quot;), distance_types = c(&#39;S&#39;,&#39;F&#39;,&#39;N&#39;), distance_modes = c(&#39;P&#39;,&#39;C&#39;), distance_on_off = TRUE ) Cohort 2: bottlenose dolphins bottlenose &lt;- load_cohort_settings( id = &quot;bottlenose&quot;, species = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;, &#39;Bottlenose_BI&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_KaNi&#39;), truncation_km = 7.5) Cohort 3: pantropical spotted dolphins spotted &lt;- load_cohort_settings( id = &quot;spotted&quot;, species = &#39;002&#39;, strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;, &#39;Spotted_OU&#39;,&#39;Spotted_FI&#39;,&#39;Spotted_BI&#39;), truncation_km = 7.5) Process settings &lt;- load_settings(strata = strata_cnp, survey = survey, cohorts = list(all_species, bottlenose, spotted)) cruz &lt;- process_surveys(das_file = &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39;, settings = settings) save(cruz, file=&#39;whiceas/whiceas_cruz.RData&#39;) load(&#39;whiceas/whiceas_cruz.RData&#39;) Data exploration Processed data structure cruz_structure(cruz) Map of sightings For the entire cruz object: map_cruz(cruz, sightings_color = &#39;firebrick&#39;) For just the surveys of interest: # Filter cruz1720 &lt;- filter_cruz(cruz, years = c(2017, 2020), regions = &#39;WHICEAS&#39;, eff_types = &#39;S&#39;, bft_range = 0:6) # Map, including survey tracks map_cruz(cruz1720, sightings_color = &#39;firebrick&#39;, effort_show = TRUE, effort_resolution = 3, effort_weight = 4, effort_opacity = .4) Interactive dashboard Use this to determine truncation distances. Note them in your lta() code below: cruz_explorer(cruz) Rg0 We can use the built-in dataset, data(g0_results), which has Beaufort-specific Relative g(0) estimates for most species based on 1986-2020 surveys. data(&quot;g0_results&quot;) Rg0 &lt;- g0_results # Plot the results: g0_plot(Rg0, panes = 3) Density &amp; abundance First we can define common values that will be constant across all estimates we produce: bootstraps &lt;- 100 years &lt;- 1986:2020 fit_regions &lt;- NULL fit_not_regions &lt;- NULL toplot = TRUE verbose = TRUE results_path &lt;- &#39;whiceas/lta/&#39; df_settings &lt;- list(covariates = c(&#39;bft&#39;,&#39;lnsstot&#39;,&#39;cruise&#39;,&#39;year&#39;,&#39;ship&#39;,&#39;species&#39;), covariates_factor = c(FALSE, FALSE, TRUE, TRUE, TRUE, TRUE), covariates_levels = 2, covariates_n_per_level = 10, simplify_cue = TRUE, simplify_bino = TRUE, detection_function_base = &#39;hn&#39;, base_model = &#39;~1&#39;, delta_aic = 2) For most species, we want to estimate density/abundance for the same set of year-region scenarios. To reduce code redundancy, as well as the risk of typing errors (and our work!), we can use the LTabundR function lta_estimates() to economize how we prepare our estimates input. For most species, these are the year-region scenarios for which we want estimates: scenarios &lt;- list(list(years = 2017, regions = &#39;WHICEAS&#39;), list(years = 2020, regions = &#39;WHICEAS&#39;)) The lta_estimates() function will generate a custom function that makes it easy to create a set of estimates sub-lists for each species of interest: estimator &lt;- lta_estimates(scenarios) That result, estimator, is actually a function. Here’s an example of how this function will work, using the first species pool as an example: estimates &lt;- c(estimator(spp = &#39;013&#39;, title = &quot;Striped dolphin&quot;), estimator(spp = &#39;026&#39;, title = &quot;Fraser&#39;s dolphin&quot;, alt_g0_spp = &#39;013&#39;), estimator(spp = &#39;031&#39;, title = &quot;Melon-headed whale&quot;, alt_g0_spp = &#39;013&#39;)) estimates [[1]] [[1]]$years [1] 2017 [[1]]$regions [1] &quot;WHICEAS&quot; [[1]]$spp [1] &quot;013&quot; [[1]]$title [1] &quot;Striped dolphin&quot; [[2]] [[2]]$years [1] 2020 [[2]]$regions [1] &quot;WHICEAS&quot; [[2]]$spp [1] &quot;013&quot; [[2]]$title [1] &quot;Striped dolphin&quot; [[3]] [[3]]$years [1] 2017 [[3]]$regions [1] &quot;WHICEAS&quot; [[3]]$spp [1] &quot;026&quot; [[3]]$title [1] &quot;Fraser&#39;s dolphin&quot; [[3]]$alt_g0_spp [1] &quot;013&quot; [[4]] [[4]]$years [1] 2020 [[4]]$regions [1] &quot;WHICEAS&quot; [[4]]$spp [1] &quot;026&quot; [[4]]$title [1] &quot;Fraser&#39;s dolphin&quot; [[4]]$alt_g0_spp [1] &quot;013&quot; [[5]] [[5]]$years [1] 2017 [[5]]$regions [1] &quot;WHICEAS&quot; [[5]]$spp [1] &quot;031&quot; [[5]]$title [1] &quot;Melon-headed whale&quot; [[5]]$alt_g0_spp [1] &quot;013&quot; [[6]] [[6]]$years [1] 2020 [[6]]$regions [1] &quot;WHICEAS&quot; [[6]]$spp [1] &quot;031&quot; [[6]]$title [1] &quot;Melon-headed whale&quot; [[6]]$alt_g0_spp [1] &quot;013&quot; The output of estimator() is a list of sub-lists specifying a set of density/abundance estimates you want to produce based on the detection function for a single species pool. Here is the full code for producing those estimates for all species from Bradford et al. (2021): Multi-species pool 1 # Striped dolphin (013), Fraser&#39;s dolphin (026), Melon-headed whale (031) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;013&#39;, &#39;026&#39;, &#39;031&#39;), pool = &#39;Multi-species pool 1&#39;, cohort = &#39;all&#39;, truncation_distance = 5, other_species = &#39;remove&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;013&#39;, title = &quot;Striped dolphin&quot;), estimator(spp = &#39;026&#39;, title = &quot;Fraser&#39;s dolphin&quot;, alt_g0_spp = &#39;013&#39;), estimator(spp = &#39;031&#39;, title = &quot;Melon-headed whale&quot;, alt_g0_spp = &#39;013&#39;)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 2 # Rough-toothed dolphin (15), Common bottlenose dolphin (18), Risso&#39;s (21), # Pygmy killer whale (32) # Notes # Bottlenose abundance is estimated in a separate cohort, but included here for DF fitting if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), pool = &#39;Multi-species pool 2&#39;, cohort = &#39;all&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;015&#39;, title = &quot;Rough-toothed dolphin&quot;), estimator(spp = &#39;021&#39;, title = &quot;Risso&#39;s dolphin&quot;), estimator(spp = &#39;032&#39;, title = &quot;Pygmy killer whale&quot;)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 3 # Short-finned pilot whale (036), Longman&#39;s beaked whale (065) # No Rg(0) available for Longman&#39;s -- will use SF pilot whale instead to estimate its weighted g0 if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;036&#39;, &#39;065&#39;), pool = &#39;Multi-species pool 3&#39;, cohort = &#39;all&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;036&#39;, title = &quot;Short-finned pilot whale&quot;), estimator(spp = &#39;065&#39;, title = &quot;Longman&#39;s beaked whale&quot;, alt_g0_spp = &#39;036&#39;)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 4 # Killer whale (37), sperm whale (46) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;037&#39;, &#39;046&#39;), pool = &#39;Multi-species pool 4&#39;, cohort = &#39;all&#39;, truncation_distance = 5.5, other_species = &#39;remove&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;037&#39;, title = &quot;Killer whale&quot;), estimator(spp = &#39;046&#39;, title = &quot;Sperm whale&quot;)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 5 # Pygmy sperm whale (47), dwarf sperm whale (48), UNID Kogia (80), # Blainville&#39;s beaked whale (59), Cuvier&#39;s beaked whale (61), # UNID Mesoplodon (51), UNID beaked whale (49), Minke whale (71) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;047&#39;, &#39;048&#39;, &#39;080&#39;, &#39;059&#39;, &#39;061&#39;, &#39;051&#39;, &#39;049&#39;, &#39;071&#39;), pool = &#39;Multi-species pool 5&#39;, cohort = &#39;all&#39;, truncation_distance = 4.5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;047&#39;, title = &quot;Pygmy sperm whale&quot;), estimator(spp = &#39;048&#39;, title = &quot;Dwarf sperm whale&quot;), estimator(spp = &#39;080&#39;, title = &quot;Unidentified Kogia&quot;), estimator(spp = &#39;059&#39;, title = &quot;Blainville&#39;s beaked whale&quot;), estimator(spp = &#39;061&#39;, title = &quot;Curvier&#39;s beaked whale&quot;), estimator(spp = &#39;051&#39;, title = &quot;Unidentified Mesoplodon&quot;), estimator(spp = &#39;049&#39;, title = &quot;Unidentified beaked whale&quot;), estimator(spp = &#39;071&#39;, title = &quot;Minke whale&quot;)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 6 # Bryde&#39;s whale (72), Sei whale (73), Fin whale (74), Blue whale (75), # Sei/Bryde&#39;s (99), Fin/Sei/Bryde&#39;s (72, 73, 74, 99) # Bryde&#39;s, Sei&#39;s, and Sei/Bryde&#39;s all use same Rg0 (title = &quot;Sei/Bryde&#39;s&quot;) # Sei/Bryde&#39;s/Fin use an average of Fin and Sei/Bryde&#39;s. if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;072&#39;, &#39;073&#39;, &#39;074&#39;,&#39;075&#39;,&#39;099&#39;), pool = &#39;Multi-species pool 6&#39;, cohort = &#39;all&#39;, truncation_distance = 5.0, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;072&#39;, title = &quot;Bryde&#39;s whale&quot;), estimator(spp = &#39;073&#39;, title = &quot;Sei whale&quot;), estimator(spp = &#39;074&#39;, title = &quot;Fin whale&quot;), estimator(spp = &#39;075&#39;, title = &quot;Blue whale&quot;), estimator(spp = &#39;099&#39;, title = &quot;Sei/Bryde&#39;s whale&quot;), estimator(spp = c(&#39;072&#39;, &#39;073&#39;, &#39;099&#39;, &#39;074&#39;), title = &quot;Sei/Bryde&#39;s/Fin whale&quot;, combine_g0 = TRUE)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Humpback whale if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;076&#39;), pool = &#39;Humpback whale&#39;, cohort = &#39;all&#39;, truncation_distance = 5.5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;-c(estimator(spp = &#39;076&#39;, title = &quot;Humpback whale&quot;)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Unidentified rorquals # UNID rorquals (70) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;070&#39;), pool = &#39;Unidentified rorqual&#39;, cohort = &#39;all&#39;, truncation_distance = 5.5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;070&#39;, title = &quot;Unidentified rorqual&quot;, alt_g0_spp = c(&#39;071&#39;,&#39;099&#39;,&#39;074&#39;,&#39;075&#39;), combine_g0 = TRUE)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Unidentified dolphins # UNID dolphin (177, 277, 377, 77) if(TRUE){ # toggle spp &lt;- c(&#39;177&#39;,&#39;277&#39;,&#39;377&#39;,&#39;077&#39;) pool_title &lt;- &#39;Unidentified dolphin&#39; # Detection function specifications fit_filters &lt;- list(spp = c(&#39;177&#39;,&#39;277&#39;,&#39;377&#39;,&#39;077&#39;), pool = pool_title, cohort = &#39;all&#39;, truncation_distance = 5.5, other_species = &#39;coerce&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- estimator(spp = spp, title = pool_title) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Unidentified cetaceans # UNID cetacean (78, 79, 98, 96) if(TRUE){ # toggle spp &lt;- c(&#39;078&#39;,&#39;079&#39;,&#39;098&#39;,&#39;096&#39;) pool_title &lt;- &#39;Unidentified cetacean&#39; # Detection function specifications fit_filters &lt;- list(spp = spp, pool = pool_title, cohort = &#39;all&#39;, truncation_distance = 5.5, other_species = &#39;coerce&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- estimator(spp = spp, title = pool_title, g0=1.0, g0_cv = 0.0) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Bottlenose dolphin # Bottlenose dolphin (018) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), pool = &#39;Bottlenose dolphin&#39;, cohort = &#39;bottlenose&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan scenarios &lt;- list( list(years = 2017, regions = &#39;WHICEAS&#39;, regions_remove = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;), region_title = &#39;(WHICEAS)&#39;), list(years = 2020, regions = &#39;WHICEAS&#39;, regions_remove = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;), region_title = &#39;(WHICEAS)&#39;)) estimator &lt;- lta_estimates(scenarios) estimates &lt;- estimator(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Pantropical spotted dolphin # Pantropical spotted dolphin (002) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;002&#39;), pool = &#39;Pantropical spotted dolphin&#39;, cohort = &#39;spotted&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan scenarios &lt;- list( list(years = 2017, regions = &#39;WHICEAS&#39;, regions_remove = c(&#39;Spotted_OU&#39;, &#39;Spotted_FI&#39;, &#39;Spotted_BI&#39;), region_title = &#39;(WHICEAS)&#39;), list(years = 2020, regions = &#39;WHICEAS&#39;, regions_remove = c(&#39;Spotted_OU&#39;, &#39;Spotted_FI&#39;, &#39;Spotted_BI&#39;), region_title = &#39;(WHICEAS)&#39;)) estimator &lt;- lta_estimates(scenarios) estimates &lt;- estimator(spp = &#39;002&#39;, title = &#39;Pantropical spotted dolphin&#39;) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Results # Load results ltas &lt;- lta_enlist(&#39;whiceas/lta/&#39;) Tables Generate report: reporti &lt;- lta_report(ltas, cruz) Table 1a. Total and systematic sample sizes. reporti$table1a Table 1b. Sample sizes of sightings used in density esitmation. reporti$table1b Table 2. Detection functions for cetacean species and taxonomic categories. reporti$table2 Table 3. Estimates of line-transect parameters for cetacean species and taxonomic categories. reporti$table3 Table 4. Estimates of density (individuals per 1,000 km2) and abundance for cetacean species and taxonomic categories sighted while on systematic survey effort. reporti$table4 Table A1. Study areas. reporti$tableA1 Table A2. Effort totals and by Beaufort sea-state, in each survey year. reporti$tableA2 $`2017` # A tibble: 3 × 9 Species Stratum Effort B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 spotted WHICEAS 4968 0.0191 0.0703 0.137 0.311 0.311 0.151 2 bottlenose WHICEAS 5834 0.0188 0.0821 0.138 0.319 0.301 0.141 3 all WHICEAS 6502 0.0200 0.0897 0.139 0.329 0.288 0.135 $`2020` # A tibble: 3 × 9 Species Stratum Effort B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 spotted WHICEAS 4389 0.0137 0.0401 0.0717 0.263 0.380 0.231 2 bottlenose WHICEAS 4938 0.0176 0.0461 0.0770 0.271 0.386 0.203 3 all WHICEAS 5337 0.0165 0.0490 0.0813 0.278 0.376 0.200 Plots lta_plot(species = NULL, lta_result = ltas, years = c(2017, 2020)) "],["case_cnp.html", " 15 Central North Pacific Data processing Data exploration Rg0 Density &amp; abundance Stratified results De-stratified results Plots", " 15 Central North Pacific Here we demonstrate code that reproduces the Bradford et al. (2021) study within the new LTabundR framework. This study estimates cetacean abundance for the Hawaiian EEZ in 2002, 2010, and 2017. Here we use survey data from 1986 to 2020 to estimate Relative g(0) and detection functions. Note that the 2002 surveys were stratified; below we demonstrate code for handling stratified studies. Currently, coefficients of variation (CV) of density and abundance are estimated using only 100 bootstrap iterations (the publication uses 1,000) to reduce processing time. library(dplyr) library(LTabundR) Data processing Settings Survey-wide settings data(species_codes) data(ships) data(group_size_coefficients) survey &lt;- load_survey_settings( out_handling = &#39;remove&#39;, max_row_interval = Inf, segment_method = &quot;equallength&quot;, segment_target_km = 150, segment_max_interval = 24, segment_remainder_handling = c(&quot;segment&quot;), ship_list = ships, species_codes = species_codes, group_size_coefficients = group_size_coefficients, smear_angles = FALSE ) Geostrata data(strata_cnp) Cohort-specific settings Cohort 1: all species all_species &lt;- load_cohort_settings( id = &quot;all&quot;, # * species = NULL, strata = c(&#39;MHI&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;), # * probable_species = FALSE, sighting_method = 0, cue_range = 0:7, school_size_range = c(0, 10000), school_size_calibrate = TRUE, calibration_floor = 0, use_low_if_na = TRUE, io_sightings = 0, geometric_mean_group = TRUE, truncation_km = 7.5, # * beaufort_range = 0:6, abeam_sightings = TRUE, strata_overlap_handling = c(&quot;smallest&quot;), distance_types = c(&#39;S&#39;,&#39;F&#39;,&#39;N&#39;), distance_modes = c(&#39;P&#39;,&#39;C&#39;), distance_on_off = TRUE ) Cohort 2: bottlenose dolphins bottlenose &lt;- load_cohort_settings( id = &quot;bottlenose&quot;, species = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), strata = c(&#39;MHI&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;, &#39;Bottlenose_BI&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_KaNi&#39;), truncation_km = 7.5) Cohort 3: pantropical spotted dolphins spotted &lt;- load_cohort_settings( id = &quot;spotted&quot;, species = &#39;002&#39;, strata = c(&#39;MHI&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;, &#39;Spotted_OU&#39;,&#39;Spotted_FI&#39;,&#39;Spotted_BI&#39;), truncation_km = 7.5) Process settings &lt;- load_settings(strata = strata_cnp, survey = survey, cohorts = list(all_species, bottlenose, spotted)) cruz &lt;- process_surveys(das_file = &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39;, settings = settings, save_local = TRUE) load(&#39;data/surveys/CenPac1986-2020_Final_alb.RData&#39;) Data exploration Processed data structure cruz_structure(cruz) Map of sightings map_cruz(cruz, sightings_color = &#39;firebrick&#39;) Interactive dashboard Use this to determine truncation distances. Note them in your lta() code below: cruz_explorer(cruz) Rg0 # Bring in 10km-segment data from all NOAA/NMFS surveys, 1986 - 2020 data(&quot;noaa_10km_1986_2020&quot;) # Filter to analysis only cruzi &lt;- filter_cruz(noaa_10km_1986_2020, analysis_only = TRUE, eff_types = &#39;S&#39;, bft_range = 0:6, on_off = TRUE) # Prepare settings list for each species species &lt;- list( list(spp = c(&#39;005&#39;, &#39;016&#39;, &#39;017&#39;), title = &#39;Delphinus spp&#39;, truncation = 5.5), list(spp = c(&#39;002&#39;,&#39;006&#39;,&#39;089&#39;,&#39;090&#39;), title = &#39;Stenella attenuata spp&#39;, truncation = 5.5), list(spp = c(&#39;003&#39;,&#39;010&#39;,&#39;011&#39;,&#39;088&#39;,&#39;100&#39;,&#39;101&#39;,&#39;102&#39;,&#39;103&#39;,&#39;107&#39;), title = &#39;Stenella longirostris spp&#39;, truncation = 5.5), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, truncation = 5.5), list(spp = &#39;015&#39;, title = &#39;Rough-toothed dolphin&#39;, truncation = 5.5), list(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;, truncation = 5.5, pool_bft = &#39;12&#39;), list(spp = &#39;021&#39;, title = &quot;Risso&#39;s dolphin&quot;, truncation = 5.5, pool_bft = &#39;12&#39;), list(spp = &#39;026&#39;, title = &quot;Fraser&#39;s dolphin&quot;, truncation = 5.5), list(spp = &#39;031&#39;, title = &#39;Melon-headed whale&#39;, truncation = 5.5), list(spp = &#39;032&#39;, title = &#39;Pygmy killer whale&#39;, truncation = 5.5), list(spp = &#39;036&#39;, title = &#39;Short-finned pilot whale&#39;, truncation = 5.5, pool_bft = &#39;12&#39;), list(spp = &#39;037&#39;, title = &#39;Killer whale&#39;, truncation = 5.5, pool_bft = &#39;12&#39;), list(spp = &#39;046&#39;, title = &#39;Sperm whale&#39;, truncation = 5.5), list(spp = c(&#39;047&#39;, &#39;048&#39;, &#39;080&#39;), title = &#39;Kogia spp&#39;, truncation = 4.0), list(spp = &#39;061&#39;, title = &quot;Cuvier&#39;s beaked whale&quot;, truncation = 4.0), list(spp = &#39;049&#39;, title = &#39;Unid. beaked whale&#39;, truncation = 5.5), list(spp = c(&#39;001&#39;,&#39;051&#39;,&#39;052&#39;,&#39;053&#39;,&#39;054&#39;,&#39;055&#39;, &#39;056&#39;,&#39;057&#39;,&#39;058&#39;,&#39;059&#39;,&#39;060&#39;,&#39;081&#39;, &#39;082&#39;,&#39;083&#39;,&#39;106&#39;,&#39;109&#39;), title = &#39;Mesoplodon spp&#39;, truncation = 4.0), list(spp = &#39;044&#39;, title = &quot;Dall&#39;s porpoise&quot;, truncation = 5.5, regions = &#39;CCS&#39;), list(spp = &#39;071&#39;, title = &#39;Minke whale&#39;, truncation = 4.0), list(spp = c(&#39;072&#39;,&#39;073&#39;,&#39;099&#39;), title = &quot;Sei/Bryde&#39;s&quot;, truncation = 5.5), list(spp = &#39;074&#39;, title = &#39;Fin whale&#39;, truncation = 5.5, regions = &#39;CCS&#39;), list(spp = &#39;075&#39;, title = &#39;Blue whale&#39;, truncation = 5.5, regions = &#39;CCS&#39;), list(spp = &#39;076&#39;, title = &#39;Humpback whale&#39;, truncation = 5.5, regions = &#39;CCS&#39;), list(spp = c(&#39;177&#39;,&#39;277&#39;,&#39;377&#39;,&#39;077&#39;), title = &#39;Unid. dolphin&#39;, truncation = 5.5), list(spp = c(&#39;078&#39;,&#39;079&#39;,&#39;098&#39;,&#39;096&#39;), title = &#39;Unid. cetacean&#39;, truncation = 5.5) ) # Estimate Rgo for your set of species: Rg0 &lt;- g0_table(cruzi, species, eff_types = &#39;S&#39;, jackknife_fraction = 0.1) save(Rg0, file=&#39;cnp/Rg0.RData&#39;) load(&#39;cnp/Rg0.RData&#39;) # Plot the results: g0_plot(Rg0, panes = 3) Density &amp; abundance First we can define common values that will be constant across all estimates we produce: bootstraps &lt;- 100 years &lt;- 1986:2020 fit_regions &lt;- NULL fit_not_regions &lt;- NULL toplot = TRUE verbose = TRUE results_path &lt;- &#39;cnp/lta/&#39; df_settings &lt;- list(covariates = c(&#39;bft&#39;,&#39;lnsstot&#39;,&#39;cruise&#39;,&#39;year&#39;,&#39;ship&#39;,&#39;species&#39;), covariates_factor = c(FALSE, FALSE, TRUE, TRUE, TRUE, TRUE), covariates_levels = 2, covariates_n_per_level = 10, simplify_cue = TRUE, simplify_bino = TRUE, detection_function_base = &#39;hn&#39;, base_model = &#39;~1&#39;, delta_aic = 2) These are the year-region scenarios for which we want estimates: scenarios &lt;- list( list(years = 2002, regions = &#39;MHI&#39;), list(years = 2002, regions = &#39;HI_EEZ&#39;, regions_remove = &#39;MHI&#39;, region_title = &#39;Pelagic HI_EEZ&#39;), list(years = 2010, regions = &#39;HI_EEZ&#39;), list(years = 2017, regions = &#39;HI_EEZ&#39;)) The lta_estimates() function will generate a custom function that makes it easy to create a set of estimates sub-lists for each species of interest: estimator &lt;- lta_estimates(scenarios) Multi-species pool 1 # Striped dolphin (013), Fraser&#39;s dolphin (026), Melon-headed whale (031) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;013&#39;, &#39;026&#39;, &#39;031&#39;), pool = &#39;Multi-species pool 1&#39;, cohort = &#39;all&#39;, truncation_distance = 5, other_species = &#39;remove&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;013&#39;, title = &quot;Striped dolphin&quot;), estimator(spp = &#39;026&#39;, title = &quot;Fraser&#39;s dolphin&quot;, alt_g0_spp = &#39;013&#39;), estimator(spp = &#39;031&#39;, title = &quot;Melon-headed whale&quot;, alt_g0_spp = &#39;013&#39;)) estimates # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 2 # Rough-toothed dolphin (15), Common bottlenose dolphin (18), Risso&#39;s (21), # Pygmy killer whale (32) # Notes # Bottlenose abundance is estimated in a separate cohort, but included here for DF fitting if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), pool = &#39;Multi-species pool 2&#39;, cohort = &#39;all&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;015&#39;, title = &quot;Rough-toothed dolphin&quot;), estimator(spp = &#39;021&#39;, title = &quot;Risso&#39;s dolphin&quot;), estimator(spp = &#39;032&#39;, title = &quot;Pygmy killer whale&quot;)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 3 # Short-finned pilot whale (036), Longman&#39;s beaked whale (065) # No Rg(0) available for Longman&#39;s -- will use SF pilot whale instead to estimate its weighted g0 if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;036&#39;, &#39;065&#39;), pool = &#39;Multi-species pool 3&#39;, cohort = &#39;all&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;036&#39;, title = &quot;Short-finned pilot whale&quot;), estimator(spp = &#39;065&#39;, title = &quot;Longman&#39;s beaked whale&quot;, alt_g0_spp = &#39;036&#39;)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 4 # Killer whale (37), sperm whale (46) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;037&#39;, &#39;046&#39;), pool = &#39;Multi-species pool 4&#39;, cohort = &#39;all&#39;, truncation_distance = 5.5, other_species = &#39;remove&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;037&#39;, title = &quot;Killer whale&quot;), estimator(spp = &#39;046&#39;, title = &quot;Sperm whale&quot;)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 5 # Pygmy sperm whale (47), dwarf sperm whale (48), UNID Kogia (80), # Blainville&#39;s beaked whale (59), Cuvier&#39;s beaked whale (61), # UNID Mesoplodon (51), UNID beaked whale (49), Minke whale (71) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;047&#39;, &#39;048&#39;, &#39;080&#39;, &#39;059&#39;, &#39;061&#39;, &#39;051&#39;, &#39;049&#39;, &#39;071&#39;), pool = &#39;Multi-species pool 5&#39;, cohort = &#39;all&#39;, truncation_distance = 4.5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;047&#39;, title = &quot;Pygmy sperm whale&quot;), estimator(spp = &#39;048&#39;, title = &quot;Dwarf sperm whale&quot;), estimator(spp = &#39;080&#39;, title = &quot;Unidentified Kogia&quot;), estimator(spp = &#39;059&#39;, title = &quot;Blainville&#39;s beaked whale&quot;), estimator(spp = &#39;061&#39;, title = &quot;Curvier&#39;s beaked whale&quot;), estimator(spp = &#39;051&#39;, title = &quot;Unidentified Mesoplodon&quot;), estimator(spp = &#39;049&#39;, title = &quot;Unidentified beaked whale&quot;), estimator(spp = &#39;071&#39;, title = &quot;Minke whale&quot;)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 6 # Bryde&#39;s whale (72), Sei whale (73), Fin whale (74), Blue whale (75), # Sei/Bryde&#39;s (99), Fin/Sei/Bryde&#39;s (72, 73, 74, 99) # Bryde&#39;s, Sei&#39;s, and Sei/Bryde&#39;s all use same Rg0 (title = &quot;Sei/Bryde&#39;s&quot;) # Sei/Bryde&#39;s/Fin use an average of Fin and Sei/Bryde&#39;s. if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;072&#39;, &#39;073&#39;, &#39;074&#39;,&#39;075&#39;,&#39;099&#39;), pool = &#39;Multi-species pool 6&#39;, cohort = &#39;all&#39;, truncation_distance = 5.0, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;072&#39;, title = &quot;Bryde&#39;s whale&quot;), estimator(spp = &#39;073&#39;, title = &quot;Sei whale&quot;), estimator(spp = &#39;074&#39;, title = &quot;Fin whale&quot;), estimator(spp = &#39;075&#39;, title = &quot;Blue whale&quot;), estimator(spp = &#39;099&#39;, title = &quot;Sei/Bryde&#39;s whale&quot;), estimator(spp = c(&#39;072&#39;, &#39;073&#39;, &#39;099&#39;, &#39;074&#39;), title = &quot;Sei/Bryde&#39;s/Fin whale&quot;, combine_g0 = TRUE)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Unidentified rorquals # UNID rorquals (70) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;070&#39;), pool = &#39;Unidentified rorqual&#39;, cohort = &#39;all&#39;, truncation_distance = 5.5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;070&#39;, title = &quot;Unidentified rorqual&quot;, alt_g0_spp = c(&#39;071&#39;,&#39;099&#39;,&#39;074&#39;,&#39;075&#39;), combine_g0 = TRUE)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Unidentified dolphins # UNID dolphin (177, 277, 377, 77) if(TRUE){ # toggle spp &lt;- c(&#39;177&#39;,&#39;277&#39;,&#39;377&#39;,&#39;077&#39;) pool_title &lt;- &#39;Unidentified dolphin&#39; # Detection function specifications fit_filters &lt;- list(spp = c(&#39;177&#39;,&#39;277&#39;,&#39;377&#39;,&#39;077&#39;), pool = pool_title, cohort = &#39;all&#39;, truncation_distance = 5.5, other_species = &#39;coerce&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- estimator(spp = spp, title = pool_title) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Unidentified cetaceans # UNID cetacean (78, 79, 98, 96) if(TRUE){ # toggle spp &lt;- c(&#39;078&#39;,&#39;079&#39;,&#39;098&#39;,&#39;096&#39;) pool_title &lt;- &#39;Unidentified cetacean&#39; # Detection function specifications fit_filters &lt;- list(spp = spp, pool = pool_title, cohort = &#39;all&#39;, truncation_distance = 5.5, other_species = &#39;coerce&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- estimator(spp = spp, title = pool_title, g0=1.0, g0_cv = 0.0) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Bottlenose dolphin # Bottlenose dolphin (018) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), pool = &#39;Bottlenose dolphin&#39;, cohort = &#39;bottlenose&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan scenarios &lt;- list( list(years = 2002, regions = &#39;MHI&#39;, regions_remove = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;), region_title = &#39;(MHI)&#39;), list(years = 2002, regions = &#39;HI_EEZ&#39;, regions_remove = &#39;MHI&#39;, region_title = &#39;Pelagic HI_EEZ&#39;), list(years = 2010, regions = &#39;HI_EEZ&#39;, regions_remove = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;), region_title = &#39;(HI_EEZ)&#39;), list(years = 2017, regions = &#39;HI_EEZ&#39;, regions_remove = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;), region_title = &#39;(HI_EEZ)&#39;)) estimator &lt;- lta_estimates(scenarios) estimates &lt;- estimator(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Pantropical spotted dolphin # Pantropical spotted dolphin (002) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;002&#39;), pool = &#39;Pantropical spotted dolphin&#39;, cohort = &#39;spotted&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan scenarios &lt;- list( list(years = 2002, regions = &#39;MHI&#39;, regions_remove = c(&#39;Spotted_OU&#39;, &#39;Spotted_FI&#39;, &#39;Spotted_BI&#39;), region_title = &#39;(MHI)&#39;), list(years = 2002, regions = &#39;HI_EEZ&#39;, regions_remove = &#39;MHI&#39;, region_title = &#39;Pelagic HI_EEZ&#39;), list(years = 2010, regions = &#39;HI_EEZ&#39;, regions_remove = c(&#39;Spotted_OU&#39;, &#39;Spotted_FI&#39;, &#39;Spotted_BI&#39;), region_title = &#39;(HI_EEZ)&#39;), list(years = 2017, regions = &#39;HI_EEZ&#39;, regions_remove = c(&#39;Spotted_OU&#39;, &#39;Spotted_FI&#39;, &#39;Spotted_BI&#39;), region_title = &#39;(HI_EEZ)&#39;)) estimator &lt;- lta_estimates(scenarios) estimates &lt;- estimator(spp = &#39;002&#39;, title = &#39;Pantropical spotted dolphin&#39;) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Stratified results # Load results ltas &lt;- lta_enlist(&#39;cnp/lta/&#39;) # Generate report reporti &lt;- lta_report(ltas, cruz) Table 1a. Total and systematic sample sizes. reporti$table1a Table 1b. Sample sizes of sightings used in density esitmation. reporti$table1b Table 2. Detection functions for cetacean species and taxonomic categories. reporti$table2 Table 3. Estimates of line-transect parameters for cetacean species and taxonomic categories – stratified in 2002. reporti$table3 Table 4. Estimates of density (individuals per 1,000 km2) and abundance for cetacean species and taxonomic categories sighted while on systematic survey effort – stratified in 2002. reporti$table4 Table A1. Study areas. reporti$tableA1 Table A2. Effort totals and by Beaufort sea-state, in each survey year. reporti$tableA2 $`2002` # A tibble: 6 × 10 Species Stratum Effort B0 B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 spotted MHI 2744 0 0 0.137 0.0852 0.400 0.301 0.0776 2 bottlenose MHI 3640 0 0.00387 0.118 0.0754 0.402 0.337 0.0646 3 all MHI 4153 0 0.00339 0.122 0.0771 0.384 0.353 0.0602 4 bottlenose HI_EEZ 15393 0.00828 0.0196 0.0487 0.0930 0.487 0.314 0.0289 5 all HI_EEZ 15394 0.00828 0.0196 0.0487 0.0930 0.487 0.314 0.0289 6 spotted HI_EEZ 15396 0.00828 0.0196 0.0487 0.0930 0.487 0.314 0.0289 $`2010` # A tibble: 4 × 10 Species Stratum Effort B0 B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 spotted MHI 2161 0 0 0.0756 0.159 0.370 0.373 0.0228 2 bottlenose MHI 2415 0 0 0.0762 0.139 0.391 0.373 0.0208 3 all MHI 2795 0 0 0.0720 0.137 0.379 0.369 0.0433 4 all HI_EEZ 19882 0.00101 0.0208 0.0539 0.130 0.469 0.283 0.0413 $`2017` # A tibble: 4 × 10 Species Stratum Effort B0 B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 spotted MHI 2519 0 0.0237 0.0748 0.186 0.315 0.220 0.180 2 bottlenose MHI 3386 0 0.0221 0.0941 0.176 0.327 0.226 0.155 3 all MHI 4052 0 0.0234 0.104 0.170 0.342 0.217 0.143 4 all HI_EEZ 18843 0.000700 0.00883 0.0384 0.119 0.313 0.341 0.179 De-stratified results # Destratify ltas2 &lt;- lta_destratify(ltas, 2002, combine_method = &#39;bootstrap&#39;, new_region = &#39;(HI_EEZ)&#39;, verbose = FALSE) # Generate report reporti &lt;- lta_report(ltas2, cruz) Table 3. Estimates of line-transect parameters for cetacean species and taxonomic categories. reporti$table3 Table 4. Estimates of density (individuals per 1,000 km2) and abundance for cetacean species and taxonomic categories sighted while on systematic survey effort. reporti$table4 Table A1. Study areas. reporti$tableA1 Table A2. Effort totals and by Beaufort sea-state, in each survey year. reporti$tableA2 $`2002` # A tibble: 3 × 10 Species Stratum Effort B0 B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 bottlenose HI_EEZ 15393 0.00828 0.0196 0.0487 0.0930 0.487 0.314 0.0289 2 all HI_EEZ 15394 0.00828 0.0196 0.0487 0.0930 0.487 0.314 0.0289 3 spotted HI_EEZ 15396 0.00828 0.0196 0.0487 0.0930 0.487 0.314 0.0289 $`2010` # A tibble: 1 × 10 Species Stratum Effort B0 B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 all HI_EEZ 19882 0.00101 0.0208 0.0539 0.130 0.469 0.283 0.0413 $`2017` # A tibble: 1 × 10 Species Stratum Effort B0 B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 all HI_EEZ 18843 0.000700 0.00883 0.0384 0.119 0.313 0.341 0.179 Plots lta_plot(species = NULL, lta_result = ltas2, years = c(2002, 2010, 2017)) "],["california-current-system.html", " 16 California Current System", " 16 California Current System Under construction! "],["stratagallery.html", " 17 Strata gallery Central North Pacific California Current ETP", " 17 Strata gallery This packages comes with several built-in datasets of geographic strata that are commonly used in NOAA/NMFS surveys. The functions strata_explore() and strata_select() were developed to help you explore those built-in options. Central North Pacific strata_explore(&#39;cnp&#39;) To acquire the filepath to one of these strata, pass the index (or indices) printed in the map titles above to the function strata_select(): strata &lt;- strata_select(selections = c(2,3), region = &#39;cnp&#39;) This function returns a named list that can be passed directly to the strata argument in load_settings(). strata %&gt;% names [1] &quot;OtherCNP&quot; &quot;MHI&quot; strata $OtherCNP Lon Lat 1 -131 40.00 2 -126 32.05 3 -120 25.00 4 -120 -5.00 5 -185 -5.00 6 -185 40.00 7 -131 40.00 $MHI Lon Lat 1 -156.00 22.00 2 -154.40 20.60 3 -153.50 19.20 4 -154.35 18.55 5 -155.20 17.75 6 -157.00 18.25 7 -157.50 19.20 8 -161.00 21.84 9 -160.00 23.00 10 -157.00 22.50 11 -156.00 22.00 California Current strata_explore(&#39;ccs&#39;) ETP You can do the same for the Eastern Tropical Pacific (ETP); there are about 70 polygons built-in to LTabundR for this region. We will just show a few of them here, using the start_at argument. strata_explore(&#39;etp&#39;, start_at = 64) "],["segmentizing.html", " 18 Segmentizing Approach Defaults Day vs Equal Length Contiguous vs. non-contiguous effort Segment remainder handling Typical settings", " 18 Segmentizing The package’s segmentize() function and its associated settings were designed to give researchers full control over how data are segmented, be it for design-based density analysis (which tend to use long segments of 100 km or more and allow for non-contiguous effort to be included in the same segment) or for habitat modeling (which tend to use short segments of 5 - 10 km and disallow non-contiguous effort to be pooled into the same segment). Approach Segments are built and stored separately for each cohort of species, since each cohort has specific settings for segmentizing. Within each cohort, the survey is first grouped into blocs of data that all share the same “effort scenario”, i.e., all rows share the same Cruise number, study area status (in or out), geographic stratum, and year. Since a survey may leave a stratum then return to it many days hence, it is normal for these blocs to contain non-contiguous data with large spatial gaps. These gaps will be addressed a few steps down. The blocs are split a final time according to whether the effort scenario meets inclusion criteria for the analysis. These inclusion criteria are controlled by the cohort-specific settings such as distance_types, distance_modes, and distance_on_off. Rows of data that meet the inclusion criteria are relegated to their own data bloc, and given a new column, use, with the value TRUE. Data that do not meet the criteria are relegated to their own bloc as well (column use is FALSE). This means that, at the end of this process, we will have segments that will be used in the density/detection function analysis, and segments that will not. (The excluded segments are not deleted or transformed in any other way; they can still be used in summarizing detections, etc.) Next, the segmentize() function loops through each of these blocs of effort and parses its data into segments according to the segment_method. If segmentizing by \"day\", this is straightforward: all data occurring on a unique date are assigned to its own segment. Segmentizing by \"equallength\" is a bit more complicated in terms of coding: segments are built up one line of data at a time; if the segment_target_km is reached or the segment_max_interval is exceeded, a new segment begins. At the end of this process, you have lists of data sorted into their segments, each with a unique seg_id, as well as a summary dataframe that provides the distance (km); time and coordinates for the beginning, middle, and end of the segment; and the weighted averages of sighting conditions and weather data contained in the segment. Setting up this demo The demonstration of segmentize() on in Processing chapter relies on the settings list that is attached as a slot in the cruz object. But you can override those settings with direct function inputs in segmentize(), which gives us a chance to explore segmentization options. First we load the demo data and carry out initial processing: # Load built-in settings example data(example_settings) settings &lt;- example_settings # Set path to DAS file das_file &lt;- &#39;data/surveys/HICEASwinter2020.das&#39; # First steps of formatting das &lt;- load_das(das_file, perform_checks = FALSE, print_glimpse = FALSE) cruz &lt;- process_strata(das, settings, verbose=FALSE) cruz &lt;- format_das(cruz) Error in format_das(cruz): object &#39;bads&#39; not found And this is the histogram function we will be using to display the results of each run of segmentize(): segment_histogram &lt;- function(cruz, cohort=1, by_day=FALSE){ (settings &lt;- cruz$settings) (segs &lt;- cruz$cohorts[[cohort]]$segments) (segmax &lt;- max(segs$dist,na.rm=TRUE)*1.1) if(by_day){ main_use &lt;- paste0(&#39;Segments (use) | by day&#39;) main_exclude &lt;- paste0(&#39;Segments (exclude) | by day&#39;) }else{ (main_use &lt;- paste0(&#39;Segments (use) | target: &#39;,settings$survey$segment_target_km,&#39; km&#39;)) (main_exclude &lt;- paste0(&#39;Segments (exclude) | target: &#39;,settings$survey$segment_target_km,&#39; km&#39;)) } par(mfrow=c(1,2)) par(mar=c(4.2,4.2,2.5,.5)) hist(segs$dist[segs$use], breaks = seq(0,segmax,by=segmax/40), xlab=&#39;Segment lengths (km)&#39;, main=main_use, cex.main = .8, cex.axis = .8, cex.lab = .8) hist(segs$dist[!segs$use], breaks = seq(0,segmax,by=segmax/40), xlab=&#39;Segment lengths (km)&#39;, main=main_exclude, cex.main = .8, cex.axis = .8, cex.lab = .8) par(mfrow=c(1,1)) } Defaults Here is the segmentize() function parameterized with the “factory default” settings from load_settings(). cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 30, segment_max_interval = 48, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), distance_types = c(&#39;S&#39;,&#39;F&#39;,&#39;N&#39;), distance_modes = c(&#39;P&#39;,&#39;C&#39;), distance_on_off = c(TRUE), verbose=FALSE) Error in UseMethod(&quot;mutate&quot;): no applicable method for &#39;mutate&#39; applied to an object of class &quot;NULL&quot; # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow NULL # Plot segment_histogram(cruz_demo) Day vs Equal Length By day cruz_demo &lt;- segmentize(cruz, segment_method = &#39;day&#39;, segment_target_km = 30, segment_max_interval = 48, verbose=FALSE) Error in UseMethod(&quot;mutate&quot;): no applicable method for &#39;mutate&#39; applied to an object of class &quot;NULL&quot; # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow NULL # Plot segment_histogram(cruz_demo, by_day = TRUE) By target length of 150 km cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 150, segment_max_interval = 48, verbose=FALSE) Error in UseMethod(&quot;mutate&quot;): no applicable method for &#39;mutate&#39; applied to an object of class &quot;NULL&quot; # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow NULL # Plot segment_histogram(cruz_demo, by_day = TRUE) Contiguous vs. non-contiguous effort The example above allows for non-contiguous effort; a segment is allowed to contain effort separated by gaps as large as 24 hours (settings$max_interval). To coerce segments to represent only contiguous effort, make that setting very small: cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 30, segment_max_interval = .1, verbose=FALSE) Error in UseMethod(&quot;mutate&quot;): no applicable method for &#39;mutate&#39; applied to an object of class &quot;NULL&quot; # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow NULL # Plot segment_histogram(cruz_demo, by_day = TRUE) You can see that many contiguous periods of effort were much shorter than the target length of 30 km. This is why allowing for non-contiguous effort can be advantageous for target segment lengths larger than 5 - 10 km. Segment remainder handling The default setting for segment_remainder_handling, c('append','segment'), means that remainders less than half the target length will be randomly appended to another segment, while remainders more than half will be treated as their own segment (and will be placed randomly along the trackline). If you don’t want that level of complexity, you can simply assign a single setting: 'append' will append the remainder in all cases, regardless of remainder length relative to the target length. The same idea goes for 'segment'. The other possible setting is 'disperse', which disperses the remainder evenly across all segments. To demonstrate, let’s use a target length of 100 km. cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 100, segment_max_interval = 48, verbose=FALSE) Error in UseMethod(&quot;mutate&quot;): no applicable method for &#39;mutate&#39; applied to an object of class &quot;NULL&quot; # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow NULL # Plot segment_histogram(cruz_demo, by_day = TRUE) Note that most segments are longer than the target length, due to the impact of dispersing the remainder. If you wanted, you could combat this by making the target length slightly smaller: cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 90, segment_max_interval = 48, verbose=FALSE) Error in UseMethod(&quot;mutate&quot;): no applicable method for &#39;mutate&#39; applied to an object of class &quot;NULL&quot; # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow NULL # Plot segment_histogram(cruz_demo, by_day = TRUE) But in general, the disperse option may be more appropriate for shorter segment lengths. Typical settings Design-based line transect analysis To replicate methods used in density estimation analyses, use large segment lengths (100 km or more) or simply segmentize by day. (See the examples above.) Remember that long segment lengths won’t work well unless you allow for non-contiguous effort. Habitat modeling To replicate the methods used in typical habitat modeling studies, use smaller segment lengths of contiguous effort. cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 5, segment_max_interval = .1, verbose=FALSE) Error in UseMethod(&quot;mutate&quot;): no applicable method for &#39;mutate&#39; applied to an object of class &quot;NULL&quot; # Number of segments cruz_demo$cohorts$default$segments %&gt;% nrow NULL # Plot segment_histogram(cruz_demo, by_day = TRUE) "],["abund9_compare.html", " 19 LTabundR vs. ABUND9 Differences in total effort Differences in on-effort distance Differences in total sightings Differences in on-effort sightings Differences in school size estimation", " 19 LTabundR vs. ABUND9 We have tried to develop LTabundR with the flexibility either to replicate ABUND results or to produce customizable results that could potentially vary from ABUND quite significantly (e.g., formatted for habitat modeling). However, even when we use LTabundR settings intended to replicate ABUND results, there are likely to be some small differences. These are detailed below: Differences in total effort After loading the data, LTabundR removes rows with invalid Cruise numbers, invalid times, and invalid coordinates. As far as we can tell, ABUND does not remove such missing data. This is a relatively minor point; in processing the 1986-2020 data (623,640 rows), 287 rows are missing Cruise info; 1,430 are missing valid times; and 556 are missing valid coordinates, for a total of 2,273 rows removed out of more than 625,000 (0.3% of rows). Many of these rows with missing data have the same coordinates as complete rows nearby (since WinCruz can sometimes produce multiple lines at the same time when setting up metadata for the research day). In ABUND, custom functions are used to calculate whether DAS coordinates occur within geostrata are difficult to validate, and it is possible that they differ from the functions used in R for the same purpose. LTabundR uses functions within the well-established sf package to do these same calculations. Both ABUND and LTabundR calculate the distance surveyed based on the sum of distances between adjacent rows in the DAS file. They do this differently (see below), based on the way they loop through the data, which may yield minor differences in segment track lengths. ABUND loops through the data one row at a time, calculating distance traveled at the same time as allocating effort to segments and processing sightings. It calculates the distance between each new row and the beginning of a segment of effort. That beginning location (object BEGTIME in the Fortran code) is reset with various triggers (including a new date), and the distance traveled is calculated using a subroutine (DISTRAV). For surveys occurring after 1991, the distance between a new coordinate and the BEGTIME coordinate is calculated using a subroutine named GRCIRC (great-circle distance). Prior to 1991, the ship speed and the time since BEGTIME is used to estimate distance traveled. After 1991, the function calculates distance based on coordinates. For all years, the distance calculation only happens if the time gap in time is at least 1.2 minutes (line 405 in ABUND9.FOR), otherwise the distance is returned as 0 km. This function also seems to allow for large gaps between subsequent rows within a single day of effort. The subroutine prints a warning message when the gap is greater than 30 km, but does not modify its estimate of distance traveled. This allows for the possibility that, in rare cases, estimates of distance surveyed will be spuriously large. LTabundR processes data using a modular approach rather than a single large loop. Prior to the segmentizing stage, it calculates the distance between rows of data. Its approach is to calculate the distance between each row and its subsequent row (it does so using the swfscDAS function distance_greatcircle(), which is a nearly-exact recode of the ABUND subroutine GRCIRC for R. There are two important differences that LTabundR applies: (1) In anticipation of WinCruz surveys that operate on much smaller scales with more frequent position updates, we calculate distances for time gaps as small as 30 seconds, not 1.2 minutes. This may generate minor differences in the total length of tracks; (2) If the distance between rows is greater than 30 km, then it is assumed that effort has stopped and the distance is changed to 0 km (that distance can be modified by the user; see the LTabundR function load_survey_settings(). This approach should avoid the misinterpretation of large gaps in effort as large periods of effort. Differences in on-effort distance LTabundR works with DAS data that are loaded and formatted using swfscDAS:das_read() and das_process(). It is possible that these functions categorize events as On- or Off-Effort slightly differently than ABUND, or apply other differences that would be difficult for us to know or track. While ABUND uses a minimum length threshold to create segments, such that full-length segments are never less than that threshold and small remainder segments always occur at the end of a continuous period of effort, LTabundR uses an approach more similar to the effort-chopping functions in swfscDAS: it looks at continuous blocs of effort, determines how many full-length segments can be defined in each bloc, then randomly places the remainder within that bloc according to a set of user-defined settings (see load_survey_settings(). This process produces full-length segments whose distribution of exact lengths is centered about the target length, rather than always being greater than the target length. To control the particularities of segmentizing, LTabundR uses settings such as segment_max_interval, which controls how discontinuous effort is allowed to be pooled into the same segment. These rules may produce slight differences in segment lengths. Note that, since ABUND is a loop-based routine while LTabundR is modular, segments identified by the two program will never be exactly identical, and a 1:1 comparison of segments produced by the two programs is not possible. Differences in total sightings In ABUND9, only sightings that occur while OnEffort == TRUE are returned; in contrast, LTabundR does not remove any sightings (it just flags them differently, using the included column variable). But we can easily filter LTabundR sightings to emulate ABUND9 output. Differences in on-effort sightings LTabundR includes an additional criterion for inclusion in analysis: the sighting must occur at or forward of the beam (this can be deactivated in load_survey_settings(). Since geostratum handling is different in the two programs, it is possible that sightings occurring near stratum margins may be included/excluded differently. Differences in school size estimation If an observer is not included in the Group Size Calibration Coefficients .DAT file, ABUND applies a default coefficient (0.8625) to scale group size estimates; however, it applies this calibration to group sizes of all sizes, including solo animals or small groups of 2-3. In LTabundR, users can choose to restrict calibrations for unknown observers to group size estimates of any size (see load_cohort_settings()) Note that ABUND9 calibrates school sizes slightly differently than ABUND7. The ABUND9 release notes mention a bug in previous versions that incorrectly calibrated school size. LTabundR corresponds perfectly with ABUND9 school size calibrations, but not with ABUND8 or earlier. "],["spp_codes.html", " 20 NOAA/NMFS species codes Table species_translator()", " 20 NOAA/NMFS species codes Table LTabundR provides the standard table of NOAA/NMFS species codes as a built-in dataset: This version of the species codes table was provided by Amanda Bradford (Pacific Islands Fisheries Science Center) in 2021. For an easier way to find a species of interest, look into the species_translator() function on the Miscellaneous functions page. species_translator() To streamline the management of species codes, scientific names, common names, etc., in the functions throughout this package, we have developed a “translator” function that returns the various identifiers for a species according to a variety of search terms. You can search by species code: # source(&#39;R/species_translator.R`) species_translator(id = &#39;035&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common_name1 &quot;Long-finned pilot whale&quot; common_name2 &quot;Atlantic pilot whale&quot; common_name3 &quot;blackfish&quot; common_name4 &quot;pothead&quot; By the short code name: species_translator(id = &#39;LONG_PILOT&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common_name1 &quot;Long-finned pilot whale&quot; common_name2 &quot;Atlantic pilot whale&quot; common_name3 &quot;blackfish&quot; common_name4 &quot;pothead&quot; By the scientific name: species_translator(id = &#39;Globicephala melas&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common_name1 &quot;Long-finned pilot whale&quot; common_name2 &quot;Atlantic pilot whale&quot; common_name3 &quot;blackfish&quot; common_name4 &quot;pothead&quot; Or by one of the species’ common names: species_translator(id = &#39;Long-finned pilot whale&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common_name1 &quot;Long-finned pilot whale&quot; common_name2 &quot;Atlantic pilot whale&quot; common_name3 &quot;blackfish&quot; common_name4 &quot;pothead&quot; The function will return any species for which there is a partial match: species_translator(id = &#39;megap&#39;) %&gt;% t() 76 code &quot;076&quot; short_name &quot;HUMPBACK_W&quot; scientific_name &quot;Megaptera novaeangliae&quot; common_name1 &quot;Humpback whale&quot; common_name2 &quot;&quot; common_name3 &quot;&quot; common_name4 &quot;&quot; 70 code &quot;070&quot; short_name &quot;UNID_RORQL&quot; scientific_name &quot;Balaenopterid sp&quot; common_name1 &quot;Unidentified rorqual (Balaenoptera; Megaptera)&quot; common_name2 &quot;&quot; common_name3 &quot;&quot; common_name4 &quot;&quot; species_translator(id = &#39;killer&#39;) code short_name scientific_name common_name1 32 032 PYGMY_KLLR Feresa attenuata Pygmy killer whale 33 033 FALSE_KLLR Pseudorca crassidens False killer whale 37 037 KILLER_WHA Orcinus orca Killer whale 110 110 Orcinus orca Transient killer whale 111 111 Orcinus orca Resident killer whale 112 112 Orcinus orca Offshore killer whale 113 113 Orcinus orca Type A Antarctic killer whale 114 114 Orcinus orca Type B Antarctic killer whale 115 115 Orcinus orca Type C Antarctic killer whale common_name2 common_name3 common_name4 32 slender blackfish 33 37 110 111 112 113 114 115 Note that if species_codes is NULL, as in the examples above, the list of codes used in ABUND9 will be used as a default. "],["backlog.html", " 21 Appendix: Backlog", " 21 Appendix: Backlog Make sure you are using AICc, not AIC Accommodate species belonging to multiple cohorts in GoogleSheet framework Remove random.seed from settings functions. Investigate discrepancies between output from format_distance() and summarize_effort(). Follow-through on converting format_distance() to match the formatting from ABUND9? (It is currently matching the format from ABUND7). Fix polygon area estimation! (Currently does not subtract land area! Ack!) Should LTabundR accept Bft values of NA? Or include an option for interpolating replacement values based on the preceding and proceding values? Or just leave it to analysts to correct the DAS data? Consider making the minimum group size for group size calibration (for unknown observers) a cohort-specific setting that users can define. Update all package and vignette documentation accordingly. Consider making the km gap handling arguments for format_das() (and its subroutine process_km()) into survey settings that users can define. Update all package and vignette documentation accordingly. Add options to das_load() that allows you to determine timezone from date and lat/long, then adjust DateTime accordingly. This would avoid errors in the manual specification of GMT offset within the DAS data. Jay uses the phrase ‘stratify by IO’: add segmentizing feature in which segment is reset if IO status changes. Main Islands map bbox Map bbox for SoCal, Central Cal, and northern CCS Figure out which ETP strata to have as defaults for both survey strata and study area polygons (there are like 100 in the folder Jeff sent us). May be a question for Tim G. Test this code on lots of cruise data files to establish basic functionality for Hawaii, California Current, and ETP. Troubleshoot why longitudes are not displayed on Hawaii base map Accommodate subgroup settings for FKW analyses Method for stratifying by Beaufort. study_area_explore() and study_area_select(). LTabundR is finding more effort in Cruise 1642 than is ABUND. See the slides detailing the ABUND v LTabundR comparison. swfscDAS can accidentally exclude sightings when the GMTOffset field is incorrect; this can be addressed by (i) correcting the DAS file so that GMToffset is correct, (ii) modifying the swfscDAS code, or (iii) building a function for LTabundR that tests and corrects the GMToffset field based upon lat/long and the date. This currently affects only two sightings in Cruise 1004. When observer positions are not entered correctly, sightings will be excluded from analysis. For example, when a sighting is manually entered without correctly establishing observer positions in a previous DAS line, the swfscDAS functions will put NA’s in the ObsL and ObsR data fields. This currently affects only one sighting in Cruise 1607. This can be addressed by (i) leaving as is, (ii) assuming invalid ObsL and ObsR are allowed to be included, or (iii) developing a LTabundR feature in which NA values are replaced based on preceding/subsequent values. When the Bft value is missing, LTabundR does not include the sighting in analysis. This can be addressed by (i) leaving as is (if the DAS data are incorrect, LTabundR will not fix it); (ii) assuming all Bft == NA are valid; or by (iii) developing a LTabundR feature in which NA values are replaced based on preceding/subsequent values. This currently affects two sightings (one in cruise 1607, one in cruise 1621). When sightings occur exactly upon the boundary of a geostratum, LTabundR considers it ‘out’ and excludes it from analysis. ABUND, in contrast, considers it in. This affects one sighting in cruise 1080. We have put an item in the backlog to consider changing this feature. ABUND has some TotSS estimates that are less than 1 (n= about 50). Most of these sightings have ‘raw’ SS estimates of 1 and at least 1 observer(s) who is in the SS calibration DAT file, which means their calibration must have produced an estimate less than 1. It makes sense to me that LTabundR should implement a minimum SS of 1 for each observer’s estimate, which should be applied after calibration. In the ABUND SSCAL routine, when an observer’s coefficients are drawn from the DAT file, the first line of best-high-low weights is used regardless of whether the general model or the year-specific model is used. Is that how it should be? For a few observers (only a few), the second row of weights differs from the first row. The ABUND SSCAL routine assumes Bft == 0 if it is NA. Are we OK with assuming that, or should we bypass calibration in the event of a missing Bft value? There is just 1 sighting with Bft == NA (Cruise 1607, 1997-04-27 07:16:01, SightNo 68, SpCode 037 killer whale). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
