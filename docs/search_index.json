[["index.html", "vignette 1 Status", " vignette 1 Status This status report outlines the code we are developing to process WinCruz data and prepare it for density estimation analysis using distance and related packages in R. Changes New process_sightings() function, with code for calibrating group sizes according to user-provided coefficient tables (mirroring ABUND9 with slight mods, e.g., school size threshold for observers not in the Coeff.DAT file.). (Details: the function process_sightings() relies upon a utility function, group_size(), which in turn relies upon group_size_calibrate().) Added cohort-specific setting, school_size_calibrate, that allows researchers to specify whether a cohort should or should not have school sizes calibrated. Example use case: school_size_calibrate = TRUE for a cohort of dolphin species, but make it FALSE for a cohort of large baleen whales. Basic function for mapping sightings (QA/QC): map_sightings(). Next focus Document vignette Document code Document function manuals Survey summary Review INP shared by Amanda EFFORT and SIGHTINGS outputs Questions for Amanda FKW handling What is the ship list used for? "],["install.html", " 2 Install LTabundR Option 1: Install from GitHub Option 2: Install locally", " 2 Install LTabundR Option 1: Install from GitHub Since the package is currently private, this is more complicated than it will be once the package is released. The first step is creating an R environment variable containing your GitHub personal access token. Complete these instructions only once: # Open your .Renviron file library(usethis) usethis::edit_r_environ() # Add your GitHub personal access token as a variable in `.Renviron` GITHUB_TOKEN &lt;- &#39;your_token_goes_here_as_a_character_string&#39; # Save and close your .Renviron file # Reload .Renviron readRenviron(&#39;~/.Renviron&#39;) Then run this code every time you want to reinstall / update the package: library(devtools) github_token &lt;- Sys.getenv(&quot;GITHUB_TOKEN&quot;) devtools::install_github(repo = &#39;amandalbradford/LTabundR-dev&#39;, subdir=&#39;LTabundR&#39;, auth_token=github_token, force=TRUE) library(LTabundR) Option 2: Install locally For now you can also install or update the package locally, using this code: library(devtools) # Remove the package, if you have an earlier version of it on your machine if(&#39;LTAabundR&#39; %in% installed.packages()){remove.packages(&#39;LTabundR&#39;)} # Specify the path to the package&#39;s project folder. path_to_package &lt;- &#39;../LTabundR&#39; # Update function documentation document(path_to_package) # Install package and any dependencies you need install(path_to_package) library(LTabundR) "],["settings.html", " 3 Settings Survey strata Study area polygon Survey-wide settings Cohort-specific settings Example code", " 3 Settings To customize the way data are processed and included in your analysis, use the load_settings() function. This function emulates and expands upon the settings file, ABUND.INP, that was used to run ABUND7/9 in FORTRAN. This function allows you to use ‘factory defaults’ if you don’t wish to specify anything special such as strata or study area polygons: settings &lt;- load_settings() If you do not want to use all the defaults, you can give load_settings() some custom inputs. The function accepts four arguments: strata: dataframe(s) of coordinates study_area: a dataframe of coordinates survey: settings that will apply universally to the analysis cohorts: settings that are specific to groups of species. By providing cohort-specific settings, the code for a single analysis becomes fully flexible to prepare and more easily reproduced, since the code only needs to be run once without modification. The output of load_settings() is a named list with a slot for each of these arguments: settings %&gt;% names [1] &quot;strata&quot; &quot;study_area&quot; &quot;survey&quot; &quot;cohorts&quot; Survey strata Stratum polygons can be provided as a named list of data.frame objects. Each data.frame must have Lat and Lon as the first two columns, providing coordinates in decimal degrees in which South and West coordinates are negative. Other columns are allowed, but the first two need to be Lon and Lat. The name of the slot holding the data.frame will be used as a reference name for the stratum. If strata is NULL, abundance will not be estimated; only density within the searched area (i.e., the total segment length x effective strip width). While users are welcome to upload polygons of their own, the package comes with “stock” polygons for strata that are commonly used in the main NMFS study regions: the Central North Pacific (CNP, including Hawaii) … data(strata_cnp) names(strata_cnp) [1] &quot;HI_EEZ&quot; &quot;WHICEAS&quot; &quot;OtherCNP&quot; …the California Current System (CCS) … data(strata_ccs) names(strata_ccs) [1] &quot;CCS&quot; &quot;Southern_CA&quot; &quot;Central_CA&quot; &quot;Nothern_CA&quot; &quot;OR_WA&quot; … and the Eastern Tropical Pacific (ETP): data(strata_etp) names(strata_etp) [1] &quot;MOPS_AreaCoreM&quot; &quot;MOPS_AreaIn&quot; &quot;MOPS_AreaIn1&quot; [4] &quot;MOPS_AreaIn2&quot; &quot;MOPS_AREAINS&quot; &quot;MOPS_AREAMID&quot; [7] &quot;MOPS_AreaMid1&quot; &quot;MOPS_AreaMid2&quot; &quot;MOPS_AreaMOPS&quot; [10] &quot;MOPS_AREANORS&quot; &quot;MOPS_AreaOuterM&quot; &quot;MOPS_AreaSou&quot; [13] &quot;MOPS_AREASOUS&quot; &quot;MOPS_AreaSpin&quot; &quot;MOPS_AreaSpinS&quot; [16] &quot;MOPS_AREAWES&quot; &quot;PODS_93STRAT1&quot; &quot;PODS_93STRAT2&quot; [19] &quot;PODS_Area92&quot; &quot;PODS_AREA92RS&quot; &quot;PODS_Area92s&quot; [22] &quot;PODS_AREA93&quot; &quot;PODS_AREA93A&quot; &quot;PODS_AREA93AR&quot; [25] &quot;PODS_AREA93AS&quot; &quot;PODS_AREA93BR&quot; &quot;PODS_AREA93M&quot; [28] &quot;PODS_AREA93MS&quot; &quot;PODS_AREA93R&quot; &quot;PODS_AREA93R1&quot; [31] &quot;PODS_AREA93R2&quot; &quot;PODS_AREA93RS&quot; &quot;PODS_AREA93S&quot; [34] &quot;PODS_AREANCOR&quot; &quot;PODS_GOCpoly&quot; &quot;Pre1986_Area79ES1&quot; [37] &quot;Pre1986_Area79ES1s&quot; &quot;Pre1986_Area79ES2&quot; &quot;Pre1986_Area79ES2s&quot; [40] &quot;Pre1986_Area79NE1&quot; &quot;Pre1986_Area79NE1s&quot; &quot;Pre1986_Area79NE2&quot; [43] &quot;Pre1986_Area79NE2s&quot; &quot;Pre1986_Area79NE3&quot; &quot;Pre1986_Area79NE3s&quot; [46] &quot;Pre1986_AreaCal&quot; &quot;Pre1986_AreaCals&quot; &quot;Pre1986_AreaMid&quot; [49] &quot;Pre1986_AreaMidS&quot; &quot;Pre1986_AreaNorth&quot; &quot;Pre1986_AreaNorthS&quot; [52] &quot;Pre1986_AreaSouth&quot; &quot;Pre1986_AreaSouthS&quot; &quot;STAR_Area98a&quot; [55] &quot;STAR_Area98b&quot; &quot;STAR_AreaCore&quot; &quot;STAR_AreaCore2&quot; [58] &quot;STAR_AreaCoreS&quot; &quot;STAR_AreaNCoast&quot; &quot;STAR_AreaNCstS&quot; [61] &quot;STAR_AreaOuter&quot; &quot;STAR_AreaOuter00&quot; &quot;STAR_AreaSCoast&quot; [64] &quot;STAR_AreaSCstS&quot; &quot;STAR_AreaSPn&quot; &quot;STAR_AreaSPs&quot; [67] &quot;STAR_AreaSTAR&quot; &quot;STAR_AreaSTAR2&quot; &quot;STAR_AreaSTARlite&quot; [70] &quot;STAR_Dcaparea&quot; The package includes functions for visualizing and selecting from these strata. See the Strata Gallery appendix. Study area polygon The study_area argument accepts a single data.frame, formatted the same as those for the strata argument, or a numeric value indicating the area of your study area in square km. If study_area is NULL, abundance will not be estimated; only density. Study area polygons can be provided in the same format as strata: a two-column csv (column names Lon and Lat with decimal-degree coordinates). A stock study area polygon is available for the CNP: data(study_cnp) study_cnp Lon Lat 1 -131 40.00 2 -126 32.05 3 -120 25.00 4 -120 -5.00 5 -185 -5.00 6 -185 40.00 7 -131 40.00 Survey-wide settings Survey-wide settings apply universally to all species in the analysis. Defaults settings$survey $segment_method [1] &quot;day&quot; $segment_target_km [1] 150 $segment_max_km_gap [1] 5 $segment_max_interval [1] 48 $segment_type_handling [1] &quot;separate&quot; &quot;pool&quot; $segment_remainder_handling [1] &quot;append&quot; &quot;segment&quot; $ship_list NULL $species_codes NULL $group_size_coefficients NULL $smear_angles [1] FALSE $random_seed [1] 833171 $verbose [1] TRUE Defaults for the survey argument list are built up efficiently using the function load_survey_settings() (see example code at bottom). Details The survey_settings input accepts a list with any of the following named slots. The first few slots are devoted to controlling how effort will be “segmentized”, or chopped into discrete sections for the purposes of estimating the variabce of the abundance estimate. segment_method: the two method options are \"day\" – all effort within the same Cruise-StudyArea-Stratum-Year-Effort scenario will be binned into segments by calendar date – and \"equallength\" – effort within each unique effort scenario (Cruise-StudyArea-etc.) will be divided into segments of approximately equal length. See the Appendix on segmentizing for details. segment_target_km: if segmenting by \"equallength\", this field allows you to specify what that target length is, in km. segment_max_km_gap: the segmentizing function works by calculating the distance between each row of DAS data. If that distance exceeds the length specified here (e.g., 5 km), the function will assume that there was a break in effort. segment_max_interval: if segmentizing by \"equallength\", this setting allows you to specify the time gaps in effort that are allowed to be contained within a single segment. For example, if your goal is a few large segments of equal length (e.g., 150-km segments, for bootstrap estimation of density variance), you are probably willing for discrete periods of effort to be concatenated into a single segment, even if the gaps between effort are as large as 1 or 2 days, in which case you would set segment_max_interval to 24 or 48 (hours), respectively. However, if your goal is many smaller segments (e.g., 5-km segments, for habitat modeling), you want to ensure that effort is contiguous so that segment locations can be accurately related to environmental variables, in which case you would set segment_max_interval to be very small (e.g., .2 hours, or 12 minutes). Setting this interval to a small number, such as 0.2, also allows the segmentizing function overlook momentary breaks in effort, such as when an unofficial observer logs a sighting. segment_type_handling: if \"pool\", all effort types that qualify as valid according to cohort_settings (see next section) will be pooled together, making it possible for different effort types to occur in the same segment. Specifically, when this setting is \"pool\", the EffType’s “S” and “F” are allowed to occur in the same segment; if this argument is \"separate\", segments will only be allowed to contain a single effort type. segment_remainder_handling: if segmentizing by \"equallength\", periods of effectively-contiguous effort (as specified by segment_max_interval) are unlikely to be perfectly divisible by your segment_target_km; there is going to be a remainder. You can handle this remainder in three ways: (1) \"disperse\" allows the function to adjust segment_target_km so that there is in fact no remainder, effectively dispersing the remainder evenly across all segments within that period of contiguous effort; (2) \"append\" asks the function to append the remainder to a randomly selected segment, such that most segments are the target length with the exception of one longer one; or (3) \"segment\" asks the function to simply place the remainder in its own segment, placed randomly within the period of contiguous effort. This setting also has a second layer of versatility, because it can accept a one- or two-element character vector. If a two-element vector is provided (e.g., c(\"append\",\"segment\")), the first element will be used in the event that the remainder is less than or equal to half your segment_target_km; if the remainder is more than half that target length, the second element will be used. This feature allows for replication of the segmentizing methods in Becker et al. (2010). The remaining slots in survey_settings pertain to various datasets and settings used in data processing: ship_list: A data.frame containing a list of ship names. If not provided the default version, which was current as of the release of ABUND9 in 2020, will be used (data(ships)). Supplied data.frames must match the column naming structure of data(ships). species_codes: A data.frame containing species codes. If not provided the default version, which was current as of the release of ABUND9 in 2020, will be used (data(species_codes)). Supplied data.frames must match the column naming structure of data(species_codes). group_size_coefficients: A data.frame of calibration factors. Find details in the subsection on processing sightings and estimating school size. smear_angles: If TRUE (the default is FALSE), bearing angles to a group of animals will be “smeared” by adding a uniformly distributed random number between -5 and +5 degrees. This has not been used in any recent analyses because observers have not been rounding angles as much as they used to. It was suggested by Buckland as a method for dealing with rounding which is especially influential when rounding to zero places many sightings at zero perpendicular distance. random_seed: a number used as the seed for stages involving random number generation. If not NULL, the results will be exactly reproducible. Default is 833171 (sensu example code in ABUND9). verbose: If TRUE (the default), status updates will be printed to the R console. Cohort-specific settings Cohort-specific settings apply only to a group of species. Since you can add as many cohorts to a settings object as you need, this allows you to stage your entire analysis and run your code once, without modifying code between the analysis of each cohort. Defaults The default is to use a single cohort for all species: settings$cohorts %&gt;% names [1] &quot;default&quot; Default values for the default cohort: settings$cohorts$default $id [1] &quot;default&quot; $species NULL $probable_species [1] FALSE $sighting_method [1] 0 $cue_range [1] 0 1 2 3 4 5 6 7 $school_size_range [1] 0 10000 $school_size_calibrate [1] TRUE $use_low_if_na [1] FALSE $io_sightings [1] 0 $geometric_mean_group [1] TRUE $truncation_km [1] 5.5 $beaufort_range [1] 0 1 2 3 4 5 6 $abeam_sightings [1] FALSE $strata_overlap_handling [1] &quot;smallest&quot; &quot;largest&quot; &quot;each&quot; $density_types [1] &quot;S&quot; &quot;F&quot; $density_modes [1] &quot;P&quot; &quot;C&quot; $density_on_off [1] TRUE $distance_types NULL $distance_modes NULL $distance_on_off [1] TRUE Defaults for the cohorts argument list is built up efficiently using the function load_cohort_settings() (see example code at bottom). Details The cohort_settings input accepts a list of any length. Each slot in that list can contain settings for a different cohort. Each cohort list can have any of the following named slots: id: An informal identifier for this cohort, to help you keep track of which cohort is which. For example, settings for a cohort of large whales species could be named \"big whales\"; settings for small delphinids and phocoenids could be named \"small_odontocetes\"; settings for beaked whales could be named \"beakers\". species: A vector of species codes to include in this cohort. If NULL (the default), all species will be included. probable_species: If TRUE (default is FALSE), the “probable” species identifications will be used in place of the “unidentified” categories. sighting_method: A coded integer which determines which sightings will be included based on how they were first seen. Allowable codes are 0=any method, 1=with 25X only, 2=neither with 25x binoculars nor from the helicopter (i.e., naked eyes and 7x binoculars only). These codes match those used in ABUND7/9. cue_range: Numeric vector of acceptable “observation cues” for sightings used in estimates of abundance. (0=this detail is missing in the data, 1=associated birds, 2=splashes, 3=body of the marine mammal, 4=associated vessel, 5=?, 6=blow / spout, 7=associated helicopter). These codes match those used in ABUND7/9. school_size_range: Minimum and maximum group sizes to be included in estimates of abundance. This is the overall group size, not the number of the given species that are present in a group. school_size_calibrate: A logical (TRUE or FALSE) specifying whether or not to carry out school size adjustments according to the calibration table provided in survey$group_size_coefficients (if that table is provided). This setting allows you to toggle the survey-wide setting for certain cohorts. For example, perhaps you want to carry out calibration for a cohort of dolphin species, but not for a cohort of large whales whose group sizes tend to be smaller and easier to estimate accurately. use_low_if_na: If this setting is TRUE, an observer does not make a best estimate of group size, mean group size will be calculated from “low” estimates. This will be done only if no observer has a “best” estimate. io_sightings: A coded integer which specifies how sightings by the independent observer will be handled. Allowable codes, which are inherited from those used in ABUND7/9, are \"_1\"=include independent observer sightings wih all other sightings, \"0\"=ignore sightings by independent observer, \"1\"=use only sightings made by regular observer team WHEN an independent observer was present, \"2\"=include only sightings made by the independent observer. IO sightings are typically used only for making g(0) estimates, otherwise IO sightings are usually ignored (code = \"0\"). geometric_mean_group: This logical variable specifies whether to use a weighted geometric mean when calculating mean group size. Barlow, Gerrodette, and Perryman (1998) found that this gave slightly better performance than a straight mean group size. Default is TRUE, but it will only be done if group_size_coefficients is not NULL. truncation_km: Specifies the maximum perpendicular distance for groups that are to be included for abundance estimation. Also determines the bins used for grouped perpendicular distances. beaufort_range: Vector of Beaufort sea states (integers) that are acceptable in estimating the detection function and density. Beaufort data with a decimal place will be rounded to the nearest integer to evaluate for inclusion. abeam_sightings: = If TRUE, sightings that occur aft of beam are included in estimating the detectin function and densities. Default is FALSE: all abeam sightings will be ignored. strata_overlap_handling: In the event that survey strata overlap, this setting tells R how to handle it. The options are \"smallest\" (the default), in which effort can belong to only a single stratum, and the smallest of overlapping strata will be used (e.g., an insular polygon nested within a larger EEZ polygon); \"largest\", in which effort can belong to only a single stratum and the largest of overlapping strata will be used (we are not sure what use case this would serve, but we offer it as an option for niche analyses); and \"each\", in which each effort is allowed to belong to two or more strata at once and all analyses will be conducted for each overlapping polygon separately (e.g., this may be appropriate for nested strata in which you want to estimate density in the entirety of the larger stratum in addition to estimating density for the nested stratum). density_types: A character vector of the effort types that will be included in detection function estimation and density estimation, and therefore considered in effort segmentizing. Accepted values are \"S\" (systematic/standard effort), \"F\" (fine-scale effort), and \"N\" (non-systematic/non-standard effort, in which systematic protocols are being used but effort is not occurring along design-based transect routes). The default values are c(\"S\",\"F\"). density_modes: The effort modes that will be included in detection function estimation and density estimation, and therefore considered in effort segmentizing. Accepted values are \"P\" (passing) and \"C\" (closing), and the default values are c(\"P\",\"C\"). density_on_off: The value(s) of OnEffort (On Effort is TRUE, Off Effort is FALSE) that will be included in detection function estimation and density estimation, and therefore considered in effort segmentizing. Default is TRUE only. (We don’t expect FALSE or c(TRUE,FALSE) to be used much, if at all, but we make this option available). distance_types: If effort types are specified in this argument, this selection will be used to filter data for estimation of the detection function. If left NULL (the default), the argument density_types will be used. This option will be rarely used, but it may prove helpful in cases in which sightings for a species are so few that including all effort types (c('S','F','N')) may be justifiable for estimating the detection function, even if only 'S' and 'F' are used for density estimation. distance_modes: Same idea as distance_types above, but for effort modes (Passing / Closing). distance_on_off: Same idea as distance_types above, but for OnEffort status (TRUE or FALSE). Example code Use settings defaults (no strata) settings &lt;- load_settings() Use settings defaults with strata # Load strata dataframes data(strata_cnp) data(study_cnp) settings &lt;- load_settings(strata = strata_cnp, study_area = study_cnp) Customize survey, but not cohorts When a cohort is not specified, the default values will be used. # Load strata dataframes data(strata_cnp) data(study_cnp) # Survey settings survey &lt;- load_survey_settings(segment_method = &#39;equallength&#39;, segment_target_km = 30, segment_max_km_gap = 5, segment_max_interval = 48, segment_type_handling = &#39;separate&#39;, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), verbose = TRUE) # Load settings settings &lt;- load_settings(strata = strata_cnp, study_area = study_cnp, survey) Fully custom: strata, study area, survey &amp; cohorts These are the settings we will use in the remainder of the tutorial: # Load strata data(strata_cnp) data(study_cnp) # Survey settings survey &lt;- load_survey_settings(segment_method = &#39;equallength&#39;, segment_target_km = 30, segment_max_km_gap = 5, segment_max_interval = 48, segment_type_handling = &#39;separate&#39;, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), verbose = TRUE) # Cohort 1 (default) cohort1 &lt;- load_cohort_settings() # Cohort 2 cohort2 &lt;- load_cohort_settings(id=&#39;fkw_insular&#39;, species=33, use_low_if_na = TRUE, truncation_km = 5, strata_overlap_handling = &#39;each&#39;) # Load settings settings &lt;- load_settings(strata = strata_cnp, study_area = study_cnp, survey = survey, cohorts=list(cohort1, cohort2)) "],["processing.html", " 4 Data processing Bring in cruise data Format DAS data Process strata Segmentize the data Process sightings", " 4 Data processing Bring in cruise data Specify the path to your .DAS data file(s): das_file &lt;- &#39;data/surveys/HICEASwinter2020.das&#39; Read in and process this .DAS file using the functions in Sam’s swfscDAS package. To do so quickly, we built a wrapper function that makes this quick and easy: das &lt;- load_das(das_file, perform_checks = TRUE, print_glimpse = TRUE) Cruise numbers: 2001 &lt;NA&gt; 48 0 Rows: 22,486 Columns: 40 $ Event &lt;chr&gt; &quot;*&quot;, &quot;*&quot;, &quot;*&quot;, &quot;*&quot;, &quot;*&quot;, &quot;*&quot;, &quot;*&quot;, &quot;*&quot;, &quot;*&quot;, &quot;*&quot;, &quot;B&quot;, &quot;R&quot;, … $ DateTime &lt;dttm&gt; 2020-01-19 07:11:52, 2020-01-19 07:13:52, 2020-01-19 07:15:… $ Lat &lt;dbl&gt; 21.79983, 21.80517, 21.81050, 21.81583, 21.82133, 21.82667, … $ Lon &lt;dbl&gt; -159.7652, -159.7657, -159.7662, -159.7668, -159.7673, -159.… $ OnEffort &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… $ Cruise &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2001, 2001, 2001, 20… $ Mode &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, … $ OffsetGMT &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, -10, -10, -10, -10, … $ EffType &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;… $ ESWsides &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, 2, 2, 2, 2, 2… $ Course &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 350,… $ SpdKt &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 9.9,… $ Bft &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 4, 4, 4,… $ SwellHght &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 6, 6, 6,… $ WindSpdKt &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 15, 15, … $ RainFog &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ HorizSun &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ VertSun &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ Glare &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ Vis &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ ObsL &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;126&quot;, &quot;126&quot;… $ Rec &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;307&quot;, &quot;307&quot;… $ ObsR &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;238&quot;, &quot;238&quot;… $ ObsInd &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ Data1 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;2001&quot;, &quot;F&quot;, &quot;126&quot;, … $ Data2 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;C&quot;, NA, &quot;307&quot;, &quot;06&quot;… $ Data3 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;-10&quot;, NA, &quot;238&quot;, &quot;1… $ Data4 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;N&quot;, NA, NA, NA, NA,… $ Data5 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;15.0&quot;, … $ Data6 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ Data7 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ Data8 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ Data9 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ Data10 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ Data11 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ Data12 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ EffortDot &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… $ EventNum &lt;chr&gt; &quot;001&quot;, &quot;002&quot;, &quot;003&quot;, &quot;004&quot;, &quot;005&quot;, &quot;006&quot;, &quot;007&quot;, &quot;008&quot;, &quot;009… $ file_das &lt;chr&gt; &quot;HICEASwinter2020.das&quot;, &quot;HICEASwinter2020.das&quot;, &quot;HICEASwinte… $ line_num &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1… Format DAS data Finalize formatting: remove rows with invalid locations and calculate the distance, in km, between each row of data. Other refinements can be added to this function later on. das &lt;- format_das(das, settings) par(mfrow=c(1,2)) hist(das$km_int, xlab=&#39;KM between each DAS row&#39;, main=NULL, breaks=seq(0,max(das$km_int),length=100)) plot(das$km_cum, ylab=&#39;Cumulative KM surveyed&#39;, main=NULL, type=&#39;l&#39;) Process strata Run the following function to add strata and study-area information to each sub-segment of effort: cruz &lt;- process_strata(das, settings) Spherical geometry (s2) switched off This function loops through each stratum data.frame you have provided it in settings$strata, formats the stratum, and asks whether each DAS row occurs within it. For each stratum, a column named stratum_&lt;StratumName&gt; is added to the das object; each row in this column is TRUE (included) or FALSE. A similar procedure is run if a dataframe is provided in settings$study_area. A column named study_area is added to das containing a boolean (TRUE if the sub-segment or sighting occurs within the study area). The function then loops through each species cohort and uses that cohort’s settings to determine a single stratum assignment for each row of DAS data in the event of overlapping strata. The key cohort setting referenced here is stratum_overlap_handling. The cruz object The function process_strata() returns a list, which we have saved in an object named cruz, with several slots: cruz %&gt;% names [1] &quot;das&quot; &quot;settings&quot; &quot;strata&quot; &quot;study_area&quot; &quot;cohorts&quot; The slots strata and study_area provide the area, in square km, of each polygon being used: cruz$strata stratum area 1 HI_EEZ 2491447.3 2 WHICEAS 419789.2 3 OtherCNP 34232739.5 cruz$study_area [1] 34232739 The slot cohorts is itself a list with one slot for each cohort. The slots are named using the id cohort setting. cruz$cohorts %&gt;% names [1] &quot;default&quot; &quot;fkw_insular&quot; Each cohort slot has a copy of the DAS data with a stratum assignment tailored to its cohort-specific settings. For instance, the default cohort, whose stratum_overlap_handling is set to \"smallest\", assigns the smallest stratum in the event of overlapping or nested strata: cruz$cohorts$default$stratum %&gt;% table(useNA=&#39;ifany&#39;) . HI_EEZ WHICEAS 142 22228 The fkw_insular cohort, whose stratum_overlap_handling is set to \"each\" (i.e., effort is allowed to belong to multiple segments, if they overlap, and all analyses will be conducted for each stratum separately), has stratum assignments that look like this; cruz$cohorts$fkw_insular$stratum %&gt;% table(useNA=&#39;ifany&#39;) . HI_EEZ&amp;OtherCNP HI_EEZ&amp;WHICEAS&amp;OtherCNP 142 22228 When a row of DAS effort occurs in two overlapping strata, the stratum assignment for that row is a concatentation of the names of the strata it falls within, with names separated by “&amp;”. This list, with these five primary slots, will be referred to as a cruz object. The remainder of the data processing work flow is focused upon refining the effort and sighting data for each slot in cohorts. The other slots are no longer modified. Segmentize the data To allocate survey data into discrete ‘effort segments’, which are used in variance estimation in subsequent steps, run the function segmentize(). This process is controlled by both survey-wide and cohort-specific settings, which are now carried in a slot within the cruz object. cruz &lt;- segmentize(cruz, verbose=TRUE) This function does not change the high-level structure of the cruz object … cruz %&gt;% names [1] &quot;das&quot; &quot;settings&quot; &quot;strata&quot; &quot;study_area&quot; &quot;cohorts&quot; … or the cohort names in the cohorts slot … cruz$cohorts %&gt;% names [1] &quot;default&quot; &quot;fkw_insular&quot; But it does change the structure of data within each cohort. Each cohort will now have a slot named density … cruz$cohorts$default %&gt;% names [1] &quot;density&quot; And, if your settings specify that settings for density estimation differ from detection function estimation, a cohort will have a second slot named distance. This is the case for the second cohort in our example analysis: fkw_insular. cruz$cohorts$fkw_insular %&gt;% names [1] &quot;density&quot; &quot;distance&quot; Though their data segmentization will differ, the density and distance slots have identical structures: cruz$cohorts$fkw_insular$density %&gt;% names [1] &quot;segments&quot; &quot;effort&quot; &quot;das&quot; cruz$cohorts$fkw_insular$distance %&gt;% names [1] &quot;segments&quot; &quot;effort&quot; &quot;das&quot; The segments slot contains summary data for each effort segment, including start/mid/end coordinates, average conditions, and segment distance: cruz$cohorts$default$density$segments %&gt;% glimpse Rows: 278 Columns: 36 $ Cruise &lt;dbl&gt; 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 200… $ stratum &lt;chr&gt; &quot;HI_EEZ&quot;, &quot;HI_EEZ&quot;, &quot;HI_EEZ&quot;, &quot;HI_EEZ&quot;, &quot;HI_EEZ&quot;, &quot;WHICEA… $ study_area &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRU… $ seg_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… $ yday &lt;dbl&gt; 21, 22, 39, 68, 22, 19, 19, 19, 20, 20, 21, 22, 23, 23, 2… $ dist &lt;dbl&gt; 3.2237470, 0.6118778, 29.7257476, 19.8973176, 1.2231556, … $ lat1 &lt;dbl&gt; 22.33300, 22.68750, 20.79283, 21.65833, 22.68750, 21.8520… $ lon1 &lt;dbl&gt; -161.2520, -161.1208, -153.6117, -161.7810, -161.1208, -1… $ DateTime1 &lt;dttm&gt; 2020-01-21 07:28:48, 2020-01-22 07:42:01, 2020-02-08 06:… $ timestamp1 &lt;dbl&gt; 1579591728, 1579678921, 1581144814, 1583688255, 157967892… $ lat2 &lt;dbl&gt; 22.68750, 20.79217, 21.65683, 21.66717, 22.68617, 22.1771… $ lon2 &lt;dbl&gt; -161.1208, -153.6090, -161.7753, -161.9295, -161.1158, -1… $ DateTime2 &lt;dttm&gt; 2020-01-22 07:42:01, 2020-02-08 06:51:34, 2020-03-08 17:… $ timestamp2 &lt;dbl&gt; 1579678921, 1581144694, 1583688135, 1583692815, 157967903… $ mlat &lt;dbl&gt; 22.69117, 20.79133, 21.62583, 21.68417, 22.68750, 22.2345… $ mlon &lt;dbl&gt; -161.1353, -153.6063, -161.6553, -161.8822, -161.1208, -1… $ mDateTime &lt;dttm&gt; 2020-01-22 07:31:52, 2020-02-08 06:49:34, 2020-03-08 16:… $ mtimestamp &lt;dbl&gt; 1579591728, 1579678921, 1581144814, 1583688255, 157967892… $ use &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FA… $ Mode &lt;chr&gt; NA, &quot;P&quot;, NA, &quot;C&quot;, &quot;P&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, NA, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, … $ EffType &lt;chr&gt; NA, &quot;S&quot;, NA, &quot;S&quot;, &quot;S&quot;, NA, &quot;N&quot;, &quot;F&quot;, NA, &quot;S&quot;, &quot;F&quot;, &quot;S&quot;, &quot;… $ ESWsides &lt;dbl&gt; NA, 2, NA, 2, 2, NA, 2, 2, NA, 2, 2, 2, 2, 2, NA, 2, 2, 2… $ year &lt;dbl&gt; 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 202… $ month &lt;dbl&gt; 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … $ day &lt;int&gt; 21, 22, 8, 8, 22, 19, 19, 19, 20, 20, 21, 22, 23, 23, 24,… $ min_line &lt;int&gt; 1080, 1549, 9811, 20825, 1550, 11, 240, 601, 701, 963, 14… $ max_line &lt;int&gt; 1548, 9810, 20824, 20886, 1554, 239, 499, 700, 962, 1486,… $ n_rows &lt;int&gt; 14, 3, 58, 62, 5, 103, 120, 99, 111, 164, 117, 89, 90, 59… $ avgBft &lt;dbl&gt; NaN, NaN, 7.000000, 6.418047, 2.000000, 5.150894, 4.84389… $ avgSwellHght &lt;dbl&gt; NaN, NaN, 7.000000, 7.000000, 4.000000, 7.433929, 7.20666… $ avgHorizSun &lt;dbl&gt; NaN, NaN, 10.958794, 11.000000, NaN, 6.954993, 6.314060, … $ avgVertSun &lt;dbl&gt; NaN, NaN, 1.058675, 2.418106, NaN, 1.419236, 1.178142, 2.… $ avgGlare &lt;dbl&gt; NaN, NaN, 0.05867486, 1.00000000, NaN, 0.00000000, 0.4734… $ avgVis &lt;dbl&gt; 4.000000, NaN, 5.000000, 4.709023, 6.200000, 5.641518, 6.… $ avgCourse &lt;dbl&gt; NaN, NaN, 287.23315, 269.55972, 105.00000, 247.16155, 166… $ avgSpdKt &lt;dbl&gt; NaN, NaN, 9.968032, 9.315076, 9.000000, 8.964329, 8.18859… # Number of segments cruz$cohorts$default$density$segments %&gt;% nrow [1] 278 # Segment length distribution hist(cruz$cohorts$default$density$segments$dist, breaks = seq(0,60,by=1), xlab=&#39;Segment lengths (km)&#39;, main=paste0(&#39;Target km: &#39;,settings$survey$segment_target_km)) The effort slot is itself a list in which each slot holds the DAS for a single segment. cruz$cohorts$default$density$effort %&gt;% length [1] 278 And the das slot holds the original data.frame of DAS data, modified slightly: the column OnEffort has been modified according to Beaufort range conditions, and the column seg_id indicates which segment the event occurs within cruz$cohorts$default$density$das %&gt;% names [1] &quot;Event&quot; &quot;DateTime&quot; &quot;Lat&quot; &quot;Lon&quot; [5] &quot;OnEffort&quot; &quot;Cruise&quot; &quot;Mode&quot; &quot;OffsetGMT&quot; [9] &quot;EffType&quot; &quot;ESWsides&quot; &quot;Course&quot; &quot;SpdKt&quot; [13] &quot;Bft&quot; &quot;SwellHght&quot; &quot;WindSpdKt&quot; &quot;RainFog&quot; [17] &quot;HorizSun&quot; &quot;VertSun&quot; &quot;Glare&quot; &quot;Vis&quot; [21] &quot;ObsL&quot; &quot;Rec&quot; &quot;ObsR&quot; &quot;ObsInd&quot; [25] &quot;Data1&quot; &quot;Data2&quot; &quot;Data3&quot; &quot;Data4&quot; [29] &quot;Data5&quot; &quot;Data6&quot; &quot;Data7&quot; &quot;Data8&quot; [33] &quot;Data9&quot; &quot;Data10&quot; &quot;Data11&quot; &quot;Data12&quot; [37] &quot;EffortDot&quot; &quot;EventNum&quot; &quot;file_das&quot; &quot;line_num&quot; [41] &quot;year&quot; &quot;month&quot; &quot;day&quot; &quot;yday&quot; [45] &quot;km_int&quot; &quot;km_cum&quot; &quot;stratum_HI_EEZ&quot; &quot;stratum_WHICEAS&quot; [49] &quot;stratum_OtherCNP&quot; &quot;study_area&quot; &quot;stratum&quot; &quot;seg_id&quot; [53] &quot;use&quot; The segmentize() function and its associated settings were designed to give researchers full control over how data are segmented, be it for design-based density analysis (which tend to use long segments of 100 km or more and allow for non-contiguous effort to be included in the same segment) or for habitat modeling (which tend to use short segments of 5 - 10 km and disallow non-contiguous effort to be pooled into the same segment). To demonstrate that versatility, checkout the appendix on segmentizing. Process sightings To process sightings for each cohort of species, use the function process_sightings(). This function has three basic steps: for each cohort, the function (1) prepares a sightings table using the function das_sight() from swfscDAS; (2) filters those sightings to species codes specified for the cohort in your settings input; and (3) evaluates each of those sightings, asking if each should be included in the analysis according to your settings. cruz &lt;- process_sightings(cruz) The function produces a formatted dataset and adds it to a new sightings slot. It does this for each analysis (density and, if specified, distance) in each cohort. cruz$cohorts$default$density %&gt;% names [1] &quot;segments&quot; &quot;effort&quot; &quot;das&quot; &quot;sightings&quot; cruz$cohorts$fkw_insular$density %&gt;% names [1] &quot;segments&quot; &quot;effort&quot; &quot;das&quot; &quot;sightings&quot; cruz$cohorts$fkw_insular$distance %&gt;% names [1] &quot;segments&quot; &quot;effort&quot; &quot;das&quot; &quot;sightings&quot; Note that the sightings table has a column named included (TRUE = yes, use it in the analysis). Any sightings that do not meet the inclusion criteria as specified in your settings will be included = FALSE, but they won’t be removed from the data. Since the sightings in each cohort are processed slightly differently, and since each cohort is species-group specific, you should expect different numbers of included/excluded sightings in each cohort-analysis dataset: cruz$cohorts$default$density$sightings$included %&gt;% table . FALSE TRUE 143 266 cruz$cohorts$fkw_insular$density$sightings$included %&gt;% table . FALSE TRUE 1 4 cruz$cohorts$fkw_insular$distance$sightings$included %&gt;% table . TRUE 5 When this function’s verbose argument is TRUE (the default), a message is printed each time a sighting does not meet the inclusion criteria (see above). Sightings data structure The sightings table has many other variables: cruz$cohorts$default$density$sightings %&gt;% names [1] &quot;Event&quot; &quot;DateTime&quot; &quot;Lat&quot; &quot;Lon&quot; [5] &quot;OnEffort&quot; &quot;Cruise&quot; &quot;Mode&quot; &quot;OffsetGMT&quot; [9] &quot;EffType&quot; &quot;ESWsides&quot; &quot;Course&quot; &quot;SpdKt&quot; [13] &quot;Bft&quot; &quot;SwellHght&quot; &quot;WindSpdKt&quot; &quot;RainFog&quot; [17] &quot;HorizSun&quot; &quot;VertSun&quot; &quot;Glare&quot; &quot;Vis&quot; [21] &quot;ObsL&quot; &quot;Rec&quot; &quot;ObsR&quot; &quot;ObsInd&quot; [25] &quot;EffortDot&quot; &quot;EventNum&quot; &quot;file_das&quot; &quot;line_num&quot; [29] &quot;year&quot; &quot;month&quot; &quot;day&quot; &quot;yday&quot; [33] &quot;km_int&quot; &quot;km_cum&quot; &quot;stratum_HI_EEZ&quot; &quot;stratum_WHICEAS&quot; [37] &quot;stratum_OtherCNP&quot; &quot;study_area&quot; &quot;stratum&quot; &quot;seg_id&quot; [41] &quot;use&quot; &quot;SightNo&quot; &quot;Subgroup&quot; &quot;SightNoDaily&quot; [45] &quot;Obs&quot; &quot;ObsStd&quot; &quot;Bearing&quot; &quot;Reticle&quot; [49] &quot;DistNm&quot; &quot;Cue&quot; &quot;Method&quot; &quot;Photos&quot; [53] &quot;Birds&quot; &quot;CalibSchool&quot; &quot;PhotosAerial&quot; &quot;Biopsy&quot; [57] &quot;CourseSchool&quot; &quot;TurtleSp&quot; &quot;TurtleGs&quot; &quot;TurtleJFR&quot; [61] &quot;TurtleAge&quot; &quot;TurtleCapt&quot; &quot;PinnipedSp&quot; &quot;PinnipedGs&quot; [65] &quot;BoatType&quot; &quot;BoatGs&quot; &quot;PerpDistKm&quot; &quot;species&quot; [69] &quot;best&quot; &quot;low&quot; &quot;high&quot; &quot;prob&quot; [73] &quot;mixed&quot; &quot;n_sp&quot; &quot;n_obs&quot; &quot;n_best&quot; [77] &quot;n_low&quot; &quot;n_high&quot; &quot;calibr&quot; &quot;included&quot; Here is a glimpse of the data: cruz$cohorts$fkw_insular$distance$sightings %&gt;% glimpse Rows: 5 Columns: 80 $ Event &lt;chr&gt; &quot;S&quot;, &quot;s&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot; $ DateTime &lt;dttm&gt; 2020-01-28 16:35:09, 2020-01-28 16:41:59, 2020-02-27 … $ Lat &lt;dbl&gt; 20.80800, 20.79717, 18.83133, 19.13683, 18.20767 $ Lon &lt;dbl&gt; -158.3488, -158.3442, -157.1577, -156.3863, -154.7485 $ OnEffort &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE $ Cruise &lt;dbl&gt; 2001, 2001, 2001, 2001, 2001 $ Mode &lt;chr&gt; &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot; $ OffsetGMT &lt;int&gt; -10, -10, -10, -10, -10 $ EffType &lt;chr&gt; &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;N&quot;, &quot;S&quot; $ ESWsides &lt;dbl&gt; 2, 2, 2, 2, 2 $ Course &lt;dbl&gt; 122, 122, 285, 90, 107 $ SpdKt &lt;dbl&gt; 9.6, 9.6, 9.4, 8.6, 7.9 $ Bft &lt;dbl&gt; 3, 3, 5, 5, 6 $ SwellHght &lt;dbl&gt; 5, 5, 8, 7, 11 $ WindSpdKt &lt;dbl&gt; 10, 10, 17, 17, 24 $ RainFog &lt;dbl&gt; 5, 5, 5, 5, 5 $ HorizSun &lt;dbl&gt; 4, 4, 7, 3, 4 $ VertSun &lt;dbl&gt; 2, 2, 1, 1, 1 $ Glare &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE $ Vis &lt;dbl&gt; 6.0, 6.0, 5.5, 5.0, 4.0 $ ObsL &lt;chr&gt; &quot;238&quot;, &quot;238&quot;, &quot;238&quot;, &quot;197&quot;, &quot;125&quot; $ Rec &lt;chr&gt; &quot;125&quot;, &quot;125&quot;, &quot;125&quot;, &quot;227&quot;, &quot;197&quot; $ ObsR &lt;chr&gt; &quot;197&quot;, &quot;197&quot;, &quot;197&quot;, &quot;126&quot;, &quot;227&quot; $ ObsInd &lt;chr&gt; NA, NA, NA, NA, NA $ EffortDot &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE $ EventNum &lt;chr&gt; &quot;422&quot;, &quot;429&quot;, &quot;213&quot;, &quot;238&quot;, &quot;371&quot; $ file_das &lt;chr&gt; &quot;HICEASwinter2020.das&quot;, &quot;HICEASwinter2020.das&quot;, &quot;HIC… $ line_num &lt;int&gt; 5016, 5030, 15756, 16275, 18395 $ year &lt;dbl&gt; 2020, 2020, 2020, 2020, 2020 $ month &lt;dbl&gt; 1, 1, 2, 2, 3 $ day &lt;int&gt; 28, 28, 27, 28, 3 $ yday &lt;dbl&gt; 28, 28, 58, 59, 63 $ km_int &lt;dbl&gt; 0.0000000000, 0.5481950970, 0.0000000000, 0.000000000… $ km_cum &lt;dbl&gt; 2437.120, 2439.638, 8902.680, 9186.375, 10120.889 $ stratum_HI_EEZ &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE $ stratum_WHICEAS &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE $ stratum_OtherCNP &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE $ study_area &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE $ stratum &lt;chr&gt; &quot;HI_EEZ&amp;WHICEAS&amp;OtherCNP&quot;, &quot;HI_EEZ&amp;WHICEAS&amp;OtherCNP&quot;,… $ seg_id &lt;int&gt; 165, 21, 241, 116, 252 $ use &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE $ SightNo &lt;chr&gt; &quot;131&quot;, &quot;131&quot;, &quot;255&quot;, &quot;258&quot;, &quot;285&quot; $ Subgroup &lt;chr&gt; NA, NA, NA, NA, NA $ SightNoDaily &lt;chr&gt; &quot;20200128_8&quot;, NA, &quot;20200227_19&quot;, &quot;20200228_18&quot;, &quot;2020… $ Obs &lt;chr&gt; &quot;197&quot;, NA, &quot;125&quot;, &quot;126&quot;, &quot;125&quot; $ ObsStd &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE $ Bearing &lt;dbl&gt; 79, 79, 0, 15, 314 $ Reticle &lt;dbl&gt; 1.8, 4.5, NA, 1.5, 5.0 $ DistNm &lt;dbl&gt; 1.88, 1.00, 0.20, 2.09, 0.92 $ Cue &lt;dbl&gt; 3, NA, 2, 3, 3 $ Method &lt;dbl&gt; 4, NA, 1, 4, 4 $ Photos &lt;chr&gt; &quot;Y&quot;, NA, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot; $ Birds &lt;chr&gt; &quot;N&quot;, NA, &quot;N&quot;, &quot;N&quot;, &quot;N&quot; $ CalibSchool &lt;chr&gt; &quot;N&quot;, NA, &quot;N&quot;, &quot;N&quot;, &quot;N&quot; $ PhotosAerial &lt;chr&gt; &quot;N&quot;, NA, &quot;N&quot;, &quot;N&quot;, &quot;N&quot; $ Biopsy &lt;chr&gt; &quot;N&quot;, NA, &quot;N&quot;, &quot;N&quot;, &quot;N&quot; $ CourseSchool &lt;dbl&gt; NA, NA, NA, NA, NA $ TurtleSp &lt;chr&gt; NA, NA, NA, NA, NA $ TurtleGs &lt;dbl&gt; NA, NA, NA, NA, NA $ TurtleJFR &lt;chr&gt; NA, NA, NA, NA, NA $ TurtleAge &lt;chr&gt; NA, NA, NA, NA, NA $ TurtleCapt &lt;chr&gt; NA, NA, NA, NA, NA $ PinnipedSp &lt;chr&gt; NA, NA, NA, NA, NA $ PinnipedGs &lt;dbl&gt; NA, NA, NA, NA, NA $ BoatType &lt;chr&gt; NA, NA, NA, NA, NA $ BoatGs &lt;dbl&gt; NA, NA, NA, NA, NA $ PerpDistKm &lt;dbl&gt; 3.417790, 3.417790, 0.000000, 1.001806, 1.225640 $ species &lt;chr&gt; &quot;033&quot;, &quot;033&quot;, &quot;033&quot;, &quot;033&quot;, &quot;033&quot; $ best &lt;dbl&gt; 32.800869, 32.800869, NaN, 13.913043, 6.956522 $ low &lt;dbl&gt; 19.672365, 19.672365, NA, 9.165151, 5.000000 $ high &lt;dbl&gt; 43.42268, 43.42268, NA, 16.58312, 20.00000 $ prob &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE $ mixed &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE $ n_sp &lt;dbl&gt; 1, 1, 1, 1, 1 $ n_obs &lt;int&gt; 3, 3, 1, 2, 1 $ n_best &lt;int&gt; 3, 3, 0, 2, 1 $ n_low &lt;int&gt; 3, 3, 0, 2, 1 $ n_high &lt;int&gt; 3, 3, 0, 2, 1 $ calibr &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE $ included &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE Note that the process_sightings() function draws upon cruz$settings for inclusion criteria, but some of those settings can be overridden with the function’s manual inputs if you want to explore your options. School size estimates In the settings we are using in this tutorial, school size estimates are adjusted using the calibration models from Barlow, Gerrodette, and Perryman (1998) (their analysis is refined slightly and further explained in Gerrodette, Perryman and Barlow, 2002). These calibration corrections are observer-specific. Some observers tend to underestimate school size and their estimates are adjusted up; others tend to overestimate and their estimates are adjusted down. Some observers do not have calibration coefficients, and for them a generic adjustment (upwards, by dividing estimates by 0.8625) is used. Each observer’s estimate is calibrated, then all observer estimates are averaged. To do that averaging, our settings specify that we shall use a geometric weighted mean, instead of an arithmetic mean, that weights school size estimates from multiple observers according to the variance of their calibration coefficients. Here are our current best estimates of school size: cruz$cohorts$default$density$sightings$best %&gt;% head(20) [1] 1.000000 18.457718 18.457718 3.342736 3.342736 1.000000 2.000000 [8] 2.000000 2.000000 3.000000 2.000000 2.000000 1.000000 1.000000 [15] 1.000000 69.279021 69.279021 1.000000 1.000000 4.637681 Let’s compare those estimates to unadjusted ones, in which calibration (and therefore weighted geometric mean) is turned off: cruz_demo &lt;- process_sightings(cruz, calibrate = FALSE, verbose = FALSE) cruz_demo$cohorts$default$density$sightings$best %&gt;% head(20) [1] 1.000000 19.191111 19.191111 3.475556 3.475556 1.000000 2.000000 [8] 2.000000 2.000000 3.000000 2.000000 2.000000 1.000000 1.000000 [15] 1.000000 78.333333 78.333333 1.000000 1.000000 4.000000 Since calibration is only used for schools above a certain size, the difference between calibration and non-calibrated estimates becomes clearer in larger groups. You can also carry out calibration corrections without using a geometric weighted mean (the arithmetic mean will be used instead): cruz_demo &lt;- process_sightings(cruz, calibrate = TRUE, geometric_mean = FALSE, verbose = FALSE) cruz_demo$cohorts$default$density$sightings$best %&gt;% head(20) [1] 1.000000 20.945347 20.945347 3.793252 3.793252 1.000000 2.000000 [8] 2.000000 2.000000 3.000000 2.000000 2.000000 1.000000 1.000000 [15] 1.000000 86.815274 86.815274 1.000000 1.000000 4.637681 Note that school size calibration is only carried out if settings$group_size_calibration is not NULL. However, even when calibration coefficients are provided, calibration is only carried out for raw estimates above a minimum threshold, since observers are unlikely to mis-estimate the school size of a lone whale or pair. For observers who have calibration coefficients in the settings$group_size_coefficients table, that minimum is specified for each observer individually. For observers not in that table, calibration will only be applied to raw school size estimates of 4 or above. "],["maps.html", " 5 Maps Base maps Add strata Add survey tracks Add sightings", " 5 Maps To build a flexible system for mapping cruise data, we have the following functions: Base maps Begin with a basic map, including EEZ borders: m &lt;- map_base(region=&#39;cnp&#39;) m We also have a base map for the California Current … m &lt;- map_base(region=&#39;ccs&#39;) m And the ETP: m &lt;- map_base(region=&#39;etp&#39;) m Add strata Add your research strata to your map: m &lt;- map_base(region=&#39;cnp&#39;) m &lt;- map_strata(m, cruz$settings, region=&#39;cnp&#39;) m Add survey tracks m1 &lt;- map_effort(m, cruz) m1 The defaults of map_effort() assume, for simplicity, that you want to see the segments to be included in density estimation for the first cohort specified in your settings. You can adjust this and other defaults using the function arguments. Customizing effort Inputs To demonstrate some of the customization options, consider this map that shows segments to be excluded from the \"distance\" (detection function) analysis for our second cohort (fkw_insular). map_effort(m, cruz, cohort = 2, analysis = &#39;distance&#39;, use_type = c(FALSE), effort_color=&#39;firebrick&#39;, effort_stroke=2.5, effort_linetype=1,) Color-code conditions Your second customization option is to add format variables to the segments slot of the cohort of interest in the cruz object. This gives you full control of line color, thickness, and line-type according to whatever specifications you wish to set, e.g., color-coding by effort type or Beaufort sea state. This is possible because the function map_effort() looks for the variables col (line color), lwd (line thickness or stroke), and lty (line type) in the columns of cruz$segments. If these columns exist, the values therein will be used instead of the function defaults. For example, color-code by Beaufort scale: # Save copy of cruz object data to modify cruz2 &lt;- cruz segments &lt;- cruz2$cohorts$default$density$segments # Add column `col`: color code by BFT sea state bft_colors &lt;- c(&#39;steelblue4&#39;,&#39;steelblue2&#39;,&#39;cadetblue1&#39;,&#39;grey&#39;) segments$col &lt;- bft_colors[4] segments$col[ segments$avgBft &lt;= 7 ] &lt;- bft_colors[3] # bft 5 + segments$col[ segments$avgBft &lt;= 4 ] &lt;- bft_colors[2] # bft 3 - 4 segments$col[ segments$avgBft &lt;= 2 ] &lt;- bft_colors[1] # bft 0 -2 # Update sub_segments slot in `cruz` object cruz2$cohorts$default$density$segments &lt;- segments # Update map m_custom2 &lt;- map_effort(m, cruz2) # Add legend using native functions from mapping package `tmap` m_custom2 &lt;- m_custom2 + tmap::tm_add_legend(&#39;line&#39;, col = bft_colors, lwd = 3, labels = c(&#39; 0 - 2&#39;, &#39; 3 - 4&#39;, &#39; 5 +&#39;, &#39; no data&#39;), title=&quot;Beaufort sea state&quot;) + tmap::tm_layout(legend.position=c(&#39;left&#39;,&#39;bottom&#39;)) # Show map m_custom2 Add sightings Use the function map_sightings() to add sightings to your map: map_sightings(m, cruz) Customizing sightings To demonstrate some of the customization options, consider this map that shows sightings of false killer whales with custom dot color, shape, and size: map_sightings(m, cruz, include_species = &#39;033&#39;, color_base = &#39;purple&#39;, shape_base = 18, size_base = 1) Next is a map of humpback whales and sperm whales, color-coded by species and shape-coded by whether or not the sighting will be included in the analysis: map_sightings(m, cruz, include_species = c(&#39;076&#39;,&#39;046&#39;), color_code = TRUE, shape_code = TRUE) Here is an overview of the steps needed to map strata, survey tracks, and sightings all together: m &lt;- map_base(&#39;cnp&#39;) m &lt;- map_strata(m, cruz$settings) m &lt;- map_effort(m, cruz) m &lt;- map_sightings(m, cruz, size_base=.4) m "],["summarize.html", " 6 Summarize survey", " 6 Summarize survey Coming soon! "],["df.html", " 7 Detection functions", " 7 Detection functions Coming soon! "],["abundance.html", " 8 Abundance estimation", " 8 Abundance estimation Coming soon! "],["casestudies.html", " 9 Case studies Central North Pacific California Current System Eastern Tropical Pacific", " 9 Case studies To demonstrate what we are building, below is template for quickly running all of the steps detailed in subsequent chapters. Central North Pacific This template use the same data and settings used throughout the vignette: library(LTabundR) # Settings ===================================================================== # Load strata &amp; study area data(strata_cnp) data(study_cnp) # Survey-wide survey &lt;- load_survey_settings(segment_method = &#39;equallength&#39;, segment_target_km = 30, segment_max_km_gap = 5, segment_max_interval = 48, segment_type_handling = &#39;separate&#39;, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), verbose = TRUE) # Cohort 1 (default) cohort1 &lt;- load_cohort_settings() # Cohort 2 cohort2 &lt;- load_cohort_settings(id=&#39;fkw_insular&#39;, species=33, use_low_if_na = TRUE, truncation_km = 5, strata_overlap_handling = &#39;each&#39;) # Finalize settings settings &lt;- load_settings(strata = strata_cnp, study_area = study_cnp, survey = survey, cohorts=list(cohort1, cohort2)) # Processing =================================================================== # Load &amp; format DAS das_file &lt;- &#39;data/surveys/HICEASwinter2020.das&#39; das &lt;- load_das(das_file, perform_checks = TRUE, print_glimpse = TRUE) das &lt;- format_das(das, settings) # Process strata cruz &lt;- process_strata(das, settings) # Sanity-check stratum assignments cruz$cohorts$default$stratum %&gt;% table(useNA=&#39;ifany&#39;) cruz$cohorts$fkw_insular$stratum %&gt;% table(useNA=&#39;ifany&#39;) # Segmentize cruz &lt;- segmentize(cruz, verbose=TRUE) # Review: Number of segments cruz$cohorts$default$density$segments %&gt;% nrow # Review: segment length distribution hist(cruz$cohorts$default$density$segments$dist, breaks = seq(0,60,by=1), xlab=&#39;Segment lengths (km)&#39;, main=paste0(&#39;Target km: &#39;,settings$survey$segment_target_km)) California Current System library(LTabundR) # Read in DAS data das_file &lt;- &#39;data/surveys/CAORWA_91-09.das&#39; das &lt;- load_das(das_file) # Subset to a single cruise das &lt;- das %&gt;% filter(Cruise == 1635) Eastern Tropical Pacific "],["practicalities.html", " 10 Practicalities", " 10 Practicalities Reproducibility &amp; project management One of the features of the ABUND framework we need to retain is that each analysis is self-contained and reproducible. The folder for an analysis must have all the files and data needed for someone else to be able to replicate it. In the R package framework we are developing, the contents of a project folder will be straightforward (once the code base is bundled into a package). Contents of a project folder: DAS file(s) with survey data Stratum and study area polygon(s), as csv(s) (optional) Group size calibration files (optional) An analysis.R file (or perhaps a .Rmd), containing an adaptation of the code provided in the template above. This script will contain project-specific settings (e.g., the load_settings() call), which allow for the script to be reproducible. "],["stratagallery.html", " 11 Appendix: Strata gallery Central North Pacific California Current ETP", " 11 Appendix: Strata gallery This packages comes with several built-in datasets of geographic strata that are commonly used in NOAA/NMFS surveys. The functions strata_explore() and strata_select() were developed to help you explore those built-in options. Central North Pacific strata_explore(&#39;cnp&#39;) To acquire the filepath to one of these strata, pass the index (or indices) printed in the map titles above to the function strata_select(): strata &lt;- strata_select(selections = c(1,3), region = &#39;cnp&#39;) This function returns a named list that can be passed directly to the strata argument in load_settings(). strata %&gt;% names [1] &quot;HI_EEZ&quot; &quot;OtherCNP&quot; The second slot, $paths, contains the filepaths you would need to pass to the load_settings() function. California Current strata_explore(&#39;ccs&#39;) ETP strata_explore(&#39;etp&#39;) "],["segmentizing.html", " 12 Appendix: Segmentizing Approach Defaults Day vs Equal Length Contiguous vs. non-contiguous effort Segment remainder handling Typical settings", " 12 Appendix: Segmentizing The package’s segmentize() function and its associated settings were designed to give researchers full control over how data are segmented, be it for design-based density analysis (which tend to use long segments of 100 km or more and allow for non-contiguous effort to be included in the same segment) or for habitat modeling (which tend to use short segments of 5 - 10 km and disallow non-contiguous effort to be pooled into the same segment). Approach Segments are built and stored separately for each cohort of species, since each cohort has specific settings for segmentizing. Within each cohort, two different sets of segments may be built, in the event that any of the cohort-specific settings for detection function estimation (distance_types, distance_modes, and distance_on_off) are different from the settings for density estimation (density_types, density_modes, and density_on_off). When that happens, each cohort will have two lists, one for each analysis: a slot for density segments and their derivated datasets, and a slot for distance segments and derivative data. Within each cohort-analysis, the survey is first grouped into “blocs” of data that all share the same “effort scenario”, i.e., all rows share the same Cruise number, study area status (in or out), geographic stratum, and year. Since a survey may leave a stratum then return to it many days hence, it is normal for these blocs to contain non-contiguous data with large spatial gaps. These gaps will be addressed a few steps down. The blocs are split further according to survey-wide and cohort-specific settings; for example, if segment_type_handling input is \"separate\", then data with EffType = \"S\" will all belong in its own block, and data with EffType = \"N\" will belong in its own bloc, etc. The blocs are split a final time according to whether the effort scenario meets inclusion criteria for the analysis. These inclusion criteria are controlled by the cohort-specific settings such as density_types, density_modes, and density_in_off. Rows of data that meet the inclusion criteria are relegated to their own data bloc, and given a new column, use, with the value TRUE. Data that do not meet the criteria are relegated to their own bloc as well (column use is FALSE). This means that, at the end of this process, we will have segments that will be used in the density/detection function analysis, and segments that will not. (The excluded segments are not deleted or transformed in any other way; they can still be used in summarizing detections, etc.) Next, the segmentize() function loops through each of these blocs of effort and parses its data into segments according to the segment_method. If segmentizing by \"day\", this is straightforward: all data occurring on a unique date are assigned to its own segment. Segmentizing by \"equallength\" is a bit more complicated in terms of coding: segments are built up one line of data at a time; if the segment_target_km is reached or the segment_max_interval is exceeded, a new segment is begun. At the end of this process, you have lists of data sorted into their segments, each with a unique seg_id, as well as a summary dataframe that provides the distance (km); time and coordinates for the beginning, middle, and end of the segment; and the weighted averages of sighting conditions and weather data contained in the segment. Setting up this demo The demonstration of segmentize() on in Processing chapter relies on the settings list that is attached as a slot in the cruz object. But you can override those settings with direct function inputs in segmentize(), which gives us a chance to explore segmentization options. First we load the demo data and carry out initial processing: data(settings) das_file &lt;- &#39;data/surveys/HICEASwinter2020.das&#39; das &lt;- load_das(das_file, perform_checks = FALSE, print_glimpse = FALSE) das &lt;- format_das(das, settings) cruz &lt;- process_strata(das, settings, verbose=FALSE) And this is the histogram function we will be using to display the results of each run of segmentize(): segment_histogram &lt;- function(cohort_effort, by_day=FALSE){ segs &lt;- cohort_effort$segments segmax &lt;- max(segs$dist,na.rm=TRUE)*1.1 if(by_day){ main_use &lt;- paste0(&#39;Segments (use) | by day&#39;) main_exclude &lt;- paste0(&#39;Segments (exclude) | by day&#39;) }else{ main_use &lt;- paste0(&#39;Segments (use) | target: &#39;,settings$survey$segment_target_km,&#39; km&#39;) main_exclude &lt;- paste0(&#39;Segments (exclude) | target: &#39;,settings$survey$segment_target_km,&#39; km&#39;) } par(mfrow=c(1,2)) par(mar=c(4.2,4.2,2.5,.5)) hist(segs$dist[segs$use], breaks = seq(0,segmax,by=segmax/40), xlab=&#39;Segment lengths (km)&#39;, main=main_use, cex.main = .8, cex.axis = .8, cex.lab = .8) hist(segs$dist[!segs$use], breaks = seq(0,segmax,by=segmax/40), xlab=&#39;Segment lengths (km)&#39;, main=main_exclude, cex.main = .8, cex.axis = .8, cex.lab = .8) par(mfrow=c(1,1)) } Defaults Here is the segmentize() function parameterized with the “factory default” settings from load_settings(). cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 30, segment_max_interval = 48, segment_type_handling = &#39;separate&#39;, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), density_types = c(&#39;S&#39;,&#39;F&#39;), density_modes = c(&#39;P&#39;,&#39;C&#39;), density_on_off = c(TRUE), distance_types = NULL, distance_modes = NULL, distance_on_off = NULL, random_seed = NULL) # Number of segments cruz_demo$cohorts$default$density$segments %&gt;% nrow [1] 278 # Plot segment_histogram(cruz_demo$cohorts$default$density) Day vs Equal Length By day cruz_demo &lt;- segmentize(cruz, segment_method = &#39;day&#39;, segment_target_km = 30, segment_max_interval = 48, segment_type_handling = &#39;separate&#39;, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), density_types = c(&#39;S&#39;,&#39;F&#39;), density_modes = c(&#39;P&#39;,&#39;C&#39;), density_on_off = c(TRUE), distance_types = NULL, distance_modes = NULL, distance_on_off = NULL, random_seed = NULL) # Number of segments cruz_demo$cohorts$default$density$segments %&gt;% nrow [1] 96 # Plot segment_histogram(cruz_demo$cohorts$default$density, by_day = TRUE) By target length of 150 km cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 150, segment_max_interval = 48, segment_type_handling = &#39;separate&#39;, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), density_types = c(&#39;S&#39;,&#39;F&#39;), density_modes = c(&#39;P&#39;,&#39;C&#39;), density_on_off = c(TRUE), distance_types = NULL, distance_modes = NULL, distance_on_off = NULL, random_seed = NULL) # Number of segments cruz_demo$cohorts$default$density$segments %&gt;% nrow [1] 61 # Plot segment_histogram(cruz_demo$cohorts$default$density, by_day = TRUE) Contiguous vs. non-contiguous effort The example above allows for non-contiguous effort; a segment is allowed to contain effort separated by gaps as large as 24 hours (settings$max_interval). To coerce segments to represent only contiguous effort, make that setting very small: cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 30, segment_max_interval = .1, segment_type_handling = &#39;separate&#39;, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), density_types = c(&#39;S&#39;,&#39;F&#39;), density_modes = c(&#39;P&#39;,&#39;C&#39;), density_on_off = c(TRUE), distance_types = NULL, distance_modes = NULL, distance_on_off = NULL, random_seed = NULL) # Number of segments cruz_demo$cohorts$default$density$segments %&gt;% nrow [1] 493 # Plot segment_histogram(cruz_demo$cohorts$default$density, by_day = TRUE) You can see that many contiguous periods of effort were much shorter than the target length of 30 km. This is why allowing for non-contiguous effort can be advantageous for target segment lengths larger than 5 - 10 km. Segment remainder handling The default setting for segment_remainder_handling, c('append','segment'), means that remainders less than half the target length will be randomly appended to another segment, while remainders more than half will be treated as their own segment (and will be placed randomly along the trackline). If you don’t want that level of complexity, you can simply assign a single setting: 'append' will append the remainder in all cases, regardless of remainder length relative to the target length. The same idea goes for 'segment'. The other possible setting is 'disperse', which disperses the remainder evenly across all segments. To demonstrate, let’s use a target length of 100 km. cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 100, segment_max_interval = 48, segment_type_handling = &#39;separate&#39;, segment_remainder_handling = c(&#39;disperse&#39;), density_types = c(&#39;S&#39;,&#39;F&#39;), density_modes = c(&#39;P&#39;,&#39;C&#39;), density_on_off = c(TRUE), distance_types = NULL, distance_modes = NULL, distance_on_off = NULL, random_seed = NULL) # Number of segments cruz_demo$cohorts$default$density$segments %&gt;% nrow [1] 80 # Plot segment_histogram(cruz_demo$cohorts$default$density, by_day = TRUE) Note that most segments are longer than the target length, due to the impact of dispersing the remainder. If you wanted, you could combat this by making the target length slightly smaller: cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 90, segment_max_interval = 48, segment_type_handling = &#39;separate&#39;, segment_remainder_handling = c(&#39;disperse&#39;), density_types = c(&#39;S&#39;,&#39;F&#39;), density_modes = c(&#39;P&#39;,&#39;C&#39;), density_on_off = c(TRUE), distance_types = NULL, distance_modes = NULL, distance_on_off = NULL, random_seed = NULL) # Number of segments cruz_demo$cohorts$default$density$segments %&gt;% nrow [1] 89 # Plot segment_histogram(cruz_demo$cohorts$default$density, by_day = TRUE) But in general, the disperse option may be more appropriate for shorter segment lengths. Typical settings Design-based line transect analysis To replicate methods used in density estimation analyses, use large segment lengths (100 km or more) or simply segmentize by day, with S and F effort types kept separate. (See the examples above.) Remember that long segment lengths won’t work well unless you allow for non-contiguous effort. Habitat modeling To replicate the methods used in typical habitat modeling studies, use smaller segment lengths of contiguous effort. In these studies it may be acceptable to pool various effort types into the same segments. cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 5, segment_max_interval = .1, segment_type_handling = &#39;pool&#39;, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), density_types = c(&#39;S&#39;,&#39;F&#39;), density_modes = c(&#39;P&#39;,&#39;C&#39;), density_on_off = c(TRUE), distance_types = NULL, distance_modes = NULL, distance_on_off = NULL, random_seed = NULL) # Number of segments cruz_demo$cohorts$default$density$segments %&gt;% nrow [1] 1729 # Plot segment_histogram(cruz_demo$cohorts$default$density, by_day = TRUE) "],["backlog.html", " 13 Appendix: Backlog", " 13 Appendix: Backlog Handle probable species in process_sightings(). Probable species currently are not handled in process_sightings(): I need a dataset that has a probable event in order to code. Look for it in the CCS data. Double check use low if best NA (you might be using it for every observer, but it should only be used if NO observer has a best) Main Islands map bbox Map bbox for SoCal, Central Cal, and northern CCS Appendix for helper functions, incl species_translator(). Method for stratifying by Beaufort. Add Survey summary function / section to this Rmd study_area_explore() and study_area_select(). Figure out which ETP strata to have as defaults for both survey strata and study area polygons (there are like 100 in the folder Jeff sent us). May be a question for Tim G. Test this code on lots of cruise data files to establish basic functionality for Hawaii, California Current, and ETP. Plotting functions for species (or subsets of species) Troubleshoot why longitudes are not displayed on Hawaii base map Accommodate subgroup settings for FKW analyses Close comparison to FORTRAN outputs Onward into next stages of Data Processing milestones "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
